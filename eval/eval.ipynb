{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ql261/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import six\n",
    "import sys\n",
    "import os\n",
    "import traceback\n",
    "import re\n",
    "import pickle\n",
    "from copy import deepcopy\n",
    "\n",
    "from chainer import cuda\n",
    "from context2vec.common.context_models import Toks\n",
    "from context2vec.common.model_reader import ModelReader\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import logging\n",
    "from scipy.stats import spearmanr\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import gensim\n",
    "import math\n",
    "import collections\n",
    "import argparse\n",
    "import h5py\n",
    "from collections import defaultdict\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_vectors(vectorfile, dim=300, skip=False):\n",
    "#   '''loads word vectors from a text file\n",
    "#   Args:\n",
    "#     vectorfile: string; vector file name\n",
    "#     dim: int; dimensions of the vectors\n",
    "#     skip: boolean; whether or not to skip the first line (for word2vec)\n",
    "#   Returns:\n",
    "#     generator of (string, numpy array); word and its embedding\n",
    "#   '''\n",
    "#   with open(vectorfile, 'r') as f:\n",
    "#     for line in f:\n",
    "#       if skip:\n",
    "#         skip = False\n",
    "#       else:\n",
    "#         index = line.index(' ')\n",
    "#         word = line[:index]\n",
    "#         yield word, np.array([FLOAT(entry) for entry in line[index+1:].split()[:dim]])\n",
    "\n",
    "def produce_top_n_simwords(w_filter,context_embed,n_result,index2word,debug=False):\n",
    "        #assume that w_filter is already normalized\n",
    "        context_embed = context_embed / xp.sqrt((context_embed * context_embed).sum())\n",
    "        similarity_scores=[]\n",
    "#         print('producing top {0} simwords'.format(n_result))\n",
    "        similarity = (w_filter.dot(context_embed)+1.0)/2\n",
    "        top_words_i=[]\n",
    "        top_words=[]\n",
    "        count = 0\n",
    "        for i in (-similarity).argsort():\n",
    "                    if xp.isnan(similarity[i]):\n",
    "                        continue\n",
    "                    if debug==True:\n",
    "                        print('{0}: {1}'.format(str(index2word[int(i)]), str(similarity[int(i)])))\n",
    "                    count += 1\n",
    "                    top_words_i.append(int(i))\n",
    "                    top_words.append(index2word[int(i)])\n",
    "                    similarity_scores.append(float(similarity[int(i)]))\n",
    "                    if count == n_result:\n",
    "                        break\n",
    "\n",
    "        top_vec=w_filter[top_words_i,:]\n",
    "        return top_vec,xp.array(similarity_scores),top_words\n",
    "\n",
    "def top_cluster_density(top_vec,similarity_scores):\n",
    "    #normalize the top_vec\n",
    "    s = xp.sqrt((top_vec * top_vec).sum(1))\n",
    "    s[s==0.] = 1.\n",
    "    top_vec = top_vec/ s.reshape((s.shape[0], 1))\n",
    "    \n",
    "    #perform the centroid\n",
    "    max_score=similarity_scores[0]\n",
    "    similarity_scores=xp.array(similarity_scores).reshape(len(similarity_scores),1)/sum(similarity_scores)\n",
    "    centroid_vector=sum(top_vec*similarity_scores)\n",
    "    # average of cosine distance to the centroid,weighted by max scores\n",
    "    inf_score=float(sum(top_vec.dot(centroid_vector))/len(top_vec)*max_score)\n",
    "    return inf_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_w2salience(model_w2v,w2salience_f,weight_type):\n",
    "    w2salience={}\n",
    "    with open(w2salience_f) as f:\n",
    "        for line in f:\n",
    "            line=line.strip()\n",
    "            if line=='':\n",
    "                continue\n",
    "            if line.startswith('sentence total'):\n",
    "                sent_total=int(line.split(':')[1])\n",
    "                continue\n",
    "            w,w_count,s_count=line.split('\\t')\n",
    "            if model_w2v.wv.__contains__(w):\n",
    "                if weight_type==INVERSE_W_FREQ:\n",
    "                    w2salience[w]=1/float(w_count)\n",
    "                elif weight_type==INVERSE_S_FREQ:\n",
    "                    w2salience[w]=math.log(1+sent_total/float(s_count))\n",
    "    #                 w2salience[w]=math.log(1+84755431/float(s_count))\n",
    "    return w2salience\n",
    "\n",
    "def skipgram_context(model,words,pos,weight=None,w2entropy=None):\n",
    "    context_wvs=[]\n",
    "    weights=[]\n",
    "    for i,word in enumerate(words):\n",
    "        if i != pos: #surroudn context words\n",
    "            try:\n",
    "                if weight ==LDA:\n",
    "                    if word in w2entropy and word in model:\n",
    "#                         print (word,w2entropy[word])\n",
    "                        weights.append(1/(w2entropy[word]+1.0))\n",
    "                        context_wvs.append(model[word])\n",
    "        \n",
    "                elif weight in [INVERSE_W_FREQ,INVERSE_S_FREQ]:\n",
    "                    if word in w2entropy and word in model:\n",
    "#                         print (word,w2entropy[word])\n",
    "                        weights.append(w2entropy[word])\n",
    "                        context_wvs.append(model[word])\n",
    "                else:\n",
    "                    #equal weights per word\n",
    "                    context_wvs.append(model[word])\n",
    "#                     print ('skipgram context',model[word][:10])\n",
    "                    weights.append(1.0)\n",
    "            except KeyError as e:\n",
    "                print ('==warning==: key error in context {0}'.format(e))\n",
    "#     print ('per word weights',weights)\n",
    "    context_embed=sum(np.array(context_wvs)*np.array(weights).reshape(len(weights),1))#/sum(weights)\n",
    "#     print ('skipgram context sum:', context_embed[:10])\n",
    "    return sum(weights),context_embed #  will be normalized later\n",
    "\n",
    "def lg_model_out_w2v(top_words,w_target,word2index_target):\n",
    "        # lg model substitutes in skipgram embedding\n",
    "        top_vec=[]\n",
    "        index_list=[]\n",
    "        for i,word in enumerate(top_words):\n",
    "            try :\n",
    "                top_vec.append(w_target[word2index_target[word]])\n",
    "                index_list.append(i)\n",
    "            except KeyError as e:\n",
    "                print ('lg subs {0} not in w2v'.format(e))\n",
    "        if top_vec==[]:\n",
    "            print ('no lg subs in w2v space')\n",
    "            return xp.array([]),[]\n",
    "        else:\n",
    "            return xp.stack(top_vec),index_list\n",
    "    \n",
    "def context_inform(test_s,test_w, model,model_type,n_result,w_filter,index2word,weight,w2entropy=None,w_target=None,word2index_target=None,index2word_target=None):\n",
    "    #produce context representation and infromative score for each context\n",
    "    test_s=test_s.replace(test_w, ' '+test_w+' ')\n",
    "#     print(test_s)\n",
    "    words=test_s.split()\n",
    "    pos=words.index(test_w)\n",
    "\n",
    "    score=1.0 #default score\n",
    "    \n",
    "    # Decide on the model\n",
    "    if model_type=='context2vec':\n",
    "            context_embed= model.context2vec(words, pos)\n",
    "\n",
    "    elif model_type=='skipgram':\n",
    "        score,context_embed=skipgram_context(model,words,pos,weight,w2entropy)\n",
    "        context_embed_out=xp.array(context_embed)\n",
    "        \n",
    "    elif model_type=='context2vec-skipgram':\n",
    "        context_embed= model.context2vec(words, pos)\n",
    "\n",
    "        top_vec,sim_scores,top_words=produce_top_n_simwords(w_filter,context_embed,n_result,index2word)\n",
    "        top_vec,index_list=lg_model_out_w2v(top_words,w_target,word2index_target) \n",
    "        sim_scores=sim_scores[index_list] #weighted by substitute probability\n",
    "        if weight==SUBSTITUTE_PROB:\n",
    "            context_embed_out=xp.array(sum(top_vec*sim_scores.reshape(len(sim_scores),1)))\n",
    "        else:\n",
    "            context_embed_out=xp.array(sum(top_vec*((sim_scores/sum(sim_scores)).reshape(len(sim_scores),1))))\n",
    "        \n",
    "    else:\n",
    "        print ('model type {0} not recognized'.format(model_type))\n",
    "        sys.exit(1)\n",
    "        \n",
    "    #decide on weight per sentence\n",
    "    if weight==TOP_MUTUAL_SIM:\n",
    "#         if word2index_target==None: #not context2vec-skipgram\n",
    "#             context2vec word embedding space neighbours\n",
    "        top_vec,sim_scores,top_words=produce_top_n_simwords(w_filter,context_embed,n_result,index2word)\n",
    "        #skipgram word embedding space neighbours when context2vec-skipgram\n",
    "        score=top_mutual_sim(top_vec,sim_scores)\n",
    "    elif weight==TOP_CLUSTER_DENSITY:\n",
    "        top_vec,sim_scores,top_words=produce_top_n_simwords(w_filter,context_embed,n_result,index2word)\n",
    "        score=top_cluster_density(top_vec,sim_scores)\n",
    "    elif weight==SUBSTITUTE_PROB:\n",
    "        score=sum(sim_scores)\n",
    "        print ('substitute prob score',score)\n",
    "    elif weight=='learned':\n",
    "        print ('learned not implemented')\n",
    "    elif weight=='gaussian':\n",
    "        print ('gaussian not implemented')\n",
    "    elif weight ==False or weight in [LDA,INVERSE_S_FREQ,INVERSE_W_FREQ]:\n",
    "        score=score\n",
    "    else:\n",
    "        print ('weight mode {0} not recognized'.format(weight))\n",
    "    return float(score),context_embed_out\n",
    "\n",
    "def additive_model(test_ss,test_w, model_type,model,n_result,w_filter,index2word,weight=False,w2entropy=None,w_target=None,word2index_target=None,index2word_target=None,f_w=None,context2vec_preembeds=None,scores=None):\n",
    "    #produce context representation across contexts using weighted average\n",
    "    print ('model type is :{0}'.format(model_type))\n",
    "    context_out=[]\n",
    "    context_weights=[]\n",
    "    for test_id in range(len(test_ss)):\n",
    "        test_s=test_ss[test_id]\n",
    "        test_s=test_s.lower().strip()\n",
    "        \n",
    "        #produce context representation with scores\n",
    "        if type(context2vec_preembeds)!=type(None):\n",
    "            context_embed=xp.array(context2vec_preembeds[test_id])\n",
    "            score=float(scores[test_id])\n",
    "        else:\n",
    "            score,context_embed=context_inform(test_s,test_w, model,model_type,n_result,w_filter,index2word,weight,w2entropy,w_target,word2index_target,index2word_target)\n",
    "        \n",
    "        \n",
    "        if score==0 or context_embed.all()==0:\n",
    "            print ('empty context vector')\n",
    "           \n",
    "        else:\n",
    "            context_out.append(context_embed)\n",
    "            context_weights.append(score)\n",
    "    \n",
    "#     print ('context_weights',context_weights)\n",
    "    #sum representation across contexts\n",
    "    if context_out==[]:\n",
    "        return None\n",
    "    else:\n",
    "        context_out=xp.stack(context_out)\n",
    "    \n",
    "    if model_type=='skipgram' or weight==SUBSTITUTE_PROB:\n",
    "        # context representation by weighted sum of all context words in all contexts\n",
    "#         print ('context out: ', context_out[:10])\n",
    "#         print ('context_weights',context_weights)\n",
    "        context_avg=sum(context_out)/sum(context_weights)\n",
    "    else:\n",
    "        norm_weights=xp.array(context_weights).reshape(len(context_weights),1)/float(sum(context_weights))\n",
    "        if f_w!=None:\n",
    "            f_w.write(','.join([str(i[0]) for i in norm_weights])+'\\n')\n",
    "        # context represenatation by weighted sum of contexts\n",
    "        \n",
    "        context_avg=sum(norm_weights*context_out)\n",
    "    \n",
    "    \n",
    "    # check new embedding neighbours\n",
    "\n",
    "#     print('producing top {0} words for new embedding'.format(n_result))\n",
    "#     if index2word_target==None:\n",
    "#         top_vec,scores,top_words=produce_top_n_simwords(w_filter,context_avg,n_result,index2word,debug=False)\n",
    "#     else:\n",
    "#         #print the target space neighbours for context2vec-skipgram\n",
    "#         top_vec,scores,top_words=produce_top_n_simwords(w_target,context_avg,n_result,index2word_target,debug=False)\n",
    "    return context_avg\n",
    "\n",
    "\n",
    "def contexts_per_tgw(sents,model_type,context_model,n_result,w,index2word,weight,w2entropy,w_target,word2index_target,index2word_target,context2vec_preembeds=None,scores=None):\n",
    "    \n",
    "    if model_type=='context2vec-skipgram?skipgram':\n",
    "#             \n",
    "            #context2vevc      \n",
    "            context_avg_1=additive_model(sents,'___', model_type.split('?')[0],context_model[0],n_result,w[0],index2word[0],weight[0],w2entropy[0],w_target[0],word2index_target[0],index2word_target[0],context2vec_preembeds=context2vec_preembeds,scores=scores)\n",
    "            #skipgram\n",
    "\n",
    "            context_avg_2=additive_model(sents,'___', model_type.split('?')[1],context_model[1],n_result,w[1],index2word[1],weight[1],w2entropy[1],w_target[1],word2index_target[1],index2word_target[1])\n",
    "\n",
    "            if type(context_avg_1)!=type(None) and type(context_avg_2)!=type(None):\n",
    "                context_avg=(context_avg_1+context_avg_2)/2\n",
    "#                 print ('context2vec avg embed',context_avg_1[:10])\n",
    "#                 print ('skipgram avg embed', context_avg_2[:10])\n",
    "#                 print ('context avg out', context_avg[:10])\n",
    "\n",
    "            elif type(context_avg_1)!=type(None):\n",
    "                context_avg=context_avg_1\n",
    "\n",
    "            elif type(context_avg_2)!=type(None):\n",
    "                context_avg=context_avg_2\n",
    "            else:\n",
    "                context_avg=None\n",
    "            \n",
    "    else:\n",
    "\n",
    "            context_avg=additive_model(sents,'___', model_type,context_model,n_result,w,index2word,weight[0],w2entropy,w_target,word2index_target,index2word_target,context2vec_preembeds=context2vec_preembeds,scores=scores)\n",
    "#             print ('context avg out', context_avg[:10])\n",
    "    return context_avg\n",
    "  \n",
    "def output_embedding(w,w_target,word2index,word2index_target):\n",
    "    if model_type=='context2vec-skipgram?skipgram':\n",
    "        #compute probe embeddings in skipgram space\n",
    "            w_out=w[1]\n",
    "            w_target_out=w_target[1]\n",
    "            word2index_out=word2index[1]\n",
    "            word2index_target_out=word2index_target[1]\n",
    "    else:\n",
    "            w_out=w\n",
    "            w_target_out=w_target\n",
    "            word2index_out=word2index\n",
    "            word2index_target_out=word2index_target\n",
    "    if word2index_target_out==None:\n",
    "        return w_out,word2index_out\n",
    "    else:\n",
    "        return w_target_out,word2index_target_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_w(w,word2index,index2word):\n",
    "    #filter out words with no letters in, and stopwords\n",
    "    stopw=stopwords.words('english')\n",
    "    stopw=[word.encode('utf-8') for word in stopw]\n",
    "    index2word_filter={}\n",
    "    word2index_filter={}\n",
    "    index_filter2index=[]\n",
    "    counter=0\n",
    "    for word in word2index:\n",
    "            if word not in stopw: #and re.search(r'[a-zA-Z]',word)!=None:\n",
    "                    index_filter2index.append(word2index[word])\n",
    "                    word2index_filter[word]=counter\n",
    "                    index2word_filter[counter]=word\n",
    "                    counter+=1\n",
    "    w_filter= w[index_filter2index,:]\n",
    "    return w_filter,word2index_filter,index2word_filter\n",
    "\n",
    "def rm_stopw_context(model):\n",
    "    stopw=stopwords.words('english')\n",
    "    stopw=[word.encode('utf-8') for word in stopw]\n",
    "    \n",
    "    model2={word:model.wv.__getitem__(word) for word in model.wv.vocab if word not in stopw}\n",
    "    return model2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine(context_avg,probe_w_vec):\n",
    "    context_avg = context_avg / xp.sqrt((context_avg * context_avg).sum())\n",
    "    probe_w_vec=probe_w_vec/xp.sqrt((probe_w_vec*probe_w_vec).sum())\n",
    "    cos=float(probe_w_vec.dot(context_avg))\n",
    "    print (cos)\n",
    "    if np.isnan(cos):\n",
    "        print ('Warning: cos is nan')\n",
    "        sys.exit(1)\n",
    "    return cos\n",
    "\n",
    "def preprocess_nonce(sent,contexts):\n",
    "    \n",
    "    sents_out=[]\n",
    "    \n",
    "    sent=sent.lower()\n",
    "    results=re.finditer('___ ',sent)\n",
    "    matches=[m for m in results]\n",
    "    for i in range(len(matches)):\n",
    "        sent_masked=sent\n",
    "        matches_mask=[(m2.start(0),m2.end(0)) for i2,m2 in enumerate(matches) if i2!=i]\n",
    "        matches_mask=sorted(matches_mask, key=lambda x:x[0],reverse=True)\n",
    "        for m in matches_mask:\n",
    "            sent_masked=sent_masked[:m[0]]+sent_masked[m[1]:]\n",
    "        sents_out.append(sent_masked+' .')\n",
    "    return sents_out\n",
    "\n",
    "def update_mrr(nns,nonce,mrr,ranks,c):\n",
    "    rr = 0\n",
    "    n = 1\n",
    "    for nn in nns:\n",
    "        word = nn[0]\n",
    "        if word == nonce:\n",
    "            print (word)\n",
    "            rr = n\n",
    "            ranks.append(rr)\n",
    "        else:\n",
    "            n+=1\n",
    "\n",
    "    if rr != 0:\n",
    "        mrr+=float(1)/float(rr)\t\n",
    "    print rr,mrr\n",
    "    c+=1\n",
    "    return mrr,ranks,c\n",
    "def eval_nonce(nonce_data_f,context_model,model_w2v,model_type,n_result,w,index2word,word2index,weight=False,w2entropy=None,w_target=None,word2index_target=None,index2word_target=None,contexts=None):\n",
    "        ranks = []\n",
    "        mrr = 0.0\n",
    "        data=pd.read_csv(os.path.join(nonce_data_f),delimiter='\\t',header=None,comment='#')\n",
    "        c = 0\n",
    "        for index, row in data.iterrows():\n",
    "            if index>100 and index%100==0:\n",
    "                print (index)\n",
    "            sents=preprocess_nonce(row[1],contexts)\n",
    "            nonce=row[0]\n",
    "            if nonce not in model_w2v:\n",
    "                print ('{0} not known'.format(nonce))\n",
    "                continue\n",
    "            context_avg=contexts_per_tgw(sents,model_type,context_model,n_result,w,index2word,weight,w2entropy,w_target,word2index_target,index2word_target)\n",
    "            if xp==cuda.cupy:\n",
    "                context_avg=xp.asnumpy(context_avg)\n",
    "                \n",
    "            # MRR Rank calculation\n",
    "            nns=model_w2v.similar_by_vector(context_avg,topn=len(model_w2v.wv.vocab))\n",
    "            mrr,ranks,c=update_mrr(nns,nonce,mrr,ranks,c)\n",
    "#             rr = 0\n",
    "#             n = 1\n",
    "#             for nn in nns:\n",
    "#                 word = nn[0]\n",
    "#                 if word == nonce:\n",
    "#                     print (word)\n",
    "#                     rr = n\n",
    "#                     ranks.append(rr)\n",
    "#                 else:\n",
    "#                     n+=1\n",
    "\n",
    "#             if rr != 0:\n",
    "#                 mrr+=float(1)/float(rr)\t\n",
    "#             print rr,mrr\n",
    "#             c+=1\n",
    "        print (\"Final MRR: \",mrr,c,float(mrr)/float(c))\n",
    "\n",
    "        print ('mediam : {0}'.format(np.median(ranks)))\n",
    "        return ranks\n",
    "            \n",
    "\n",
    "\n",
    "def eval_chimera(chimeras_data_f,context_model,model_type,n_result,w,index2word,word2index,weight=False,w2entropy=None,w_target=None,word2index_target=None,index2word_target=None):\n",
    "    chimeras_data_dir=os.path.dirname(chimeras_data_f)\n",
    "    num_sent=os.path.basename(chimeras_data_f).split('.')[1][1]\n",
    "    print (chimeras_data_dir)\n",
    "    print (num_sent)\n",
    "    with open(chimeras_data_dir+'/weights_{0}_{1}_{2}'.format(num_sent,model_type,str(weight)),'w') as f_w:\n",
    "        spearmans=[]\n",
    "        data=pd.read_csv(os.path.join(chimeras_data_f),delimiter='\\t',header=None)\n",
    "        w_target_out,word2index_target_out=output_embedding(w,w_target,word2index,word2index_target)\n",
    "        for index, row in data.iterrows():\n",
    "            if index>100 and index%100==0:\n",
    "                print (index)\n",
    "            golds=[]\n",
    "            model_predict=[]\n",
    "            probes=[]\n",
    "            sents=row[1].lower().split('@@')\n",
    "            #compute context representation\n",
    "            context_avg=contexts_per_tgw(sents,model_type,context_model,n_result,w,index2word,weight,w2entropy,w_target,word2index_target,index2word_target)\n",
    "            #cosine similarity with probe embedding\n",
    "            for gold,probe in zip(row[3].split(','),row[2].split(',')):\n",
    "                try:\n",
    "                    probe_w_vec=w_target_out[word2index_target_out[probe]]\n",
    "                    cos=cosine(context_avg,probe_w_vec)\n",
    "                    model_predict.append(cos)\n",
    "                    golds.append(gold)\n",
    "                    probes.append(probe)\n",
    "                except KeyError as e:\n",
    "                    print (\"====warning key error for probe=====: {0}\".format(e))\n",
    "            print ('probes',probes)\n",
    "            print ('gold',golds)\n",
    "            print ('model_predict',model_predict)\n",
    "            sp=spearmanr(golds,model_predict)[0]\n",
    "            print ('spearman correlation is {0}'.format(sp))\n",
    "            if not math.isnan(sp):\n",
    "                spearmans.append(sp)\n",
    "        print (\"AVERAGE RHO:\",float(sum(spearmans))/float(len(spearmans)))\n",
    "\n",
    "\n",
    "def load_contexts(rw,sents,model,model_type,n_result,w_filter,index2word,weight,w2entropy,w_target,word2index_target,index2word_target):          \n",
    "    print ('loading contexts..')\n",
    "    contexts=[]\n",
    "    scores=[]\n",
    "    for i,test_s in enumerate(sents):\n",
    "        test_s=sents[i]\n",
    "        test_s=test_s.replace(TARGET_W, ' '+TARGET_W+' ')\n",
    "        print(i),\n",
    "        words=test_s.split()\n",
    "        pos=words.index(TARGET_W)\n",
    "        score,context_embed=context_inform(test_s,TARGET_W, model,model_type,n_result,w_filter,index2word,weight,w2entropy,w_target,word2index_target,index2word_target)\n",
    "        if context_embed.all()==0:\n",
    "            context_embed=xp.zeros(w_target.shape[-1])\n",
    "            score=0\n",
    "        contexts.append(context_embed)\n",
    "        scores.append(score)\n",
    "    contexts=xp.stack(contexts,axis=0)\n",
    "    scores=np.array(scores)\n",
    "    print ('\\ncontexts for {0} completed'.format(rw))\n",
    "    return contexts,scores\n",
    "\n",
    "def load_sents(contexts_f,rw):\n",
    "    sents_all=[]\n",
    "    with open (os.path.join(contexts_f,rw+'.txt')) as f:\n",
    "        for line in f:\n",
    "            line=line.replace(rw,TARGET_W).strip().lower()\n",
    "            if not line.endswith('.'):\n",
    "                line=line+' .'\n",
    "            sents_all.append(line)\n",
    "    sents_all=np.array(sents_all)\n",
    "    return sents_all\n",
    "\n",
    "def produce_contexts_per_trial(trials,freq,scores_all,context2vec_preembeds_all,orders_inf,sents_all):\n",
    "    if trials==1 and type(scores_all)!=type(None):\n",
    "        context_inds=orders_inf[:freq]\n",
    "    else:\n",
    "        context_inds=perm[np.array(range(freq-1, 2*freq-1)),]\n",
    "\n",
    "    sents=sents_all[np.array(context_inds),]\n",
    "    print (context_inds)\n",
    "    if type(context2vec_preembeds_all)!=type(None):\n",
    "        context2vec_preembeds=context2vec_preembeds_all[context_inds,]\n",
    "        scores=scores_all[context_inds,]\n",
    "    return context2vec_preembeds, scores,sents\n",
    "\n",
    "def eval_crw_stf(crw_stf_f,model_param_f,context_model,model_type,n_result,w,index2word,word2index,weight=False,w2entropy=None,w_target=None,word2index_target=None,index2word_target=None,trials=100):\n",
    "    data=pd.read_csv(os.path.join(crw_stf_f),delimiter='\\t',header=None,comment='#')\n",
    "    model_predicts=defaultdict(lambda: defaultdict(list))\n",
    "    golds=defaultdict(lambda: defaultdict(list))\n",
    "    context2vec_preembeds_all=None\n",
    "    context2vec_preembeds=None\n",
    "    scores_all=None\n",
    "    orders_inf=None\n",
    "    scores=None\n",
    "    rw_prev=''\n",
    "#     start evaluation\n",
    "    w_target_out,word2index_target_out=output_embedding(w,w_target,word2index,word2index_target)\n",
    "    contexts_f=os.path.join(os.path.dirname(crw_stf_f),'context')\n",
    "    \n",
    "    for index, row in data.iterrows():\n",
    "        probe_w=row[0]\n",
    "        rw=row[1]\n",
    "        if probe_w not in word2index_target_out: \n",
    "            continue\n",
    "        gold=row[2]\n",
    "        print ('\\n==========processing rareword {0}'.format(rw))\n",
    "\n",
    "        #load sentences\n",
    "        sents_all=load_sents(contexts_f,rw)\n",
    "        #load contexts\n",
    "        if 'context2vec' in model_type.split('?')[0]:\n",
    "            if rw_prev!=rw:\n",
    "                rw_prev=rw\n",
    "                if model_type=='context2vec-skipgram?skipgram':\n",
    "                    context2vec_preembeds_all,scores_all=load_contexts(rw,sents_all,context_model[0],model_type.split('?')[0],n_result,w[0],index2word[0],weight[0],w2entropy[0],w_target[0],word2index_target[0],index2word_target[0])\n",
    "                elif model_type=='context2vec-skipgram':\n",
    "                    context2vec_preembeds_all,scores_all=load_contexts(rw,sents_all,context_model,model_type.split('?')[0],n_result,w,index2word,weight[0],w2entropy,w_target,word2index_target,index2word_target)\n",
    "                orders_inf=(-scores_all).argsort()\n",
    "        #do trials\n",
    "        for trial in range(trials):\n",
    "            print ('\\n=====Trial no. {0}'.format(trial))\n",
    "            perm = np.random.permutation(255)\n",
    "            for logfreq in range(8):\n",
    "                freq = 2**logfreq\n",
    "                print ('\\ncontext num is {0}'.format(freq))\n",
    "                context2vec_preembeds, scores,sents=produce_contexts_per_trial(trials,freq,scores_all,context2vec_preembeds_all,orders_inf,sents_all)\n",
    "                context_avg=contexts_per_tgw(sents,model_type,context_model,n_result,w,index2word,weight,w2entropy,w_target,word2index_target,index2word_target,context2vec_preembeds,scores)\n",
    "                \n",
    "                if type(context_avg)!=type(None):\n",
    "                    #cosine similarity\n",
    "                    probe_w_vec=w_target_out[word2index_target_out[probe_w]]\n",
    "                    cos=cosine(context_avg,probe_w_vec)\n",
    "                    model_predicts[trial][freq].append(cos)\n",
    "                    golds[trial][freq].append(gold)\n",
    "     \n",
    "    #\n",
    "    sps=defaultdict(list)\n",
    "    for trial in model_predicts:\n",
    "        for freq in model_predicts[trial]:\n",
    "            sp=spearmanr(golds[trial][freq],model_predicts[trial][freq])[0]\n",
    "            sps[freq].append(sp)\n",
    "    print ('{0}\\t{1}\\t{2}'.format('freq','SPEARMAN RANKS MEAN','SPEARMAN RANKS STD'))\n",
    "    for freq in sorted(sps.keys()):\n",
    "        print ('{0}\\t{1}\\t{2}: '.format(freq,np.mean(np.array(sps[freq])),np.std(np.array(sps[freq]))))\n",
    "    return model_predicts,sps\n",
    "\n",
    "            \n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read model....\n",
      "Reading config file: ../models/context2vec/model_dir/MODEL-wiki.params.14\n",
      "Config:  {'config_path': '../models/context2vec/model_dir/', 'model_file': 'MODEL-wiki.14', 'deep': 'yes', 'drop_ratio': '0.0', 'words_file': 'WORDS-wiki.targets.14', 'unit': '400'}\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "out of memory to allocate 415001600 bytes (total 4030248448 bytes)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-5afa3fd5032c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mw_skipgram\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_skipgram\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;31m#vector normalize for probe w embedding for calculating top vector similarity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_skipgram\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mw_skipgram\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0mw_skipgram\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mcupy/core/core.pyx\u001b[0m in \u001b[0;36mcupy.core.core.ndarray.__mul__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mcupy/core/elementwise.pxi\u001b[0m in \u001b[0;36mcupy.core.core.ufunc.__call__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mcupy/core/elementwise.pxi\u001b[0m in \u001b[0;36mcupy.core.core._get_out_args\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mcupy/core/core.pyx\u001b[0m in \u001b[0;36mcupy.core.core.ndarray.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mcupy/cuda/memory.pyx\u001b[0m in \u001b[0;36mcupy.cuda.memory.alloc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mcupy/cuda/memory.pyx\u001b[0m in \u001b[0;36mcupy.cuda.memory.MemoryPool.malloc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mcupy/cuda/memory.pyx\u001b[0m in \u001b[0;36mcupy.cuda.memory.MemoryPool.malloc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mcupy/cuda/memory.pyx\u001b[0m in \u001b[0;36mcupy.cuda.memory.SingleDeviceMemoryPool.malloc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mcupy/cuda/memory.pyx\u001b[0m in \u001b[0;36mcupy.cuda.memory.SingleDeviceMemoryPool._malloc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: out of memory to allocate 415001600 bytes (total 4030248448 bytes)"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    \n",
    "    TOP_MUTUAL_SIM='top_mutual_sim'\n",
    "    TOP_CLUSTER_DENSITY='top_cluster_density'\n",
    "    LDA='lda'\n",
    "    TARGET_W='___'\n",
    "    INVERSE_S_FREQ='inverse_s_freq'\n",
    "    INVERSE_W_FREQ='inverse_w_q'\n",
    "    SUBSTITUTE_PROB='substitute_prob'\n",
    "    WEIGHT_DICT={0:False,1:TOP_MUTUAL_SIM,2:LDA,3:INVERSE_S_FREQ,4:INVERSE_W_FREQ,5:TOP_CLUSTER_DENSITY, 6:SUBSTITUTE_PROB}\n",
    "\n",
    "    ##### 1. params read in\n",
    "    if sys.argv[0]=='/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py':\n",
    "        \n",
    "        ###data:\n",
    "#         data='./eval_data/data-chimeras/dataset_alacarte.l2.fixed.test.txt.punct'\n",
    "#         data='./eval_data/data-nonces/n2v.definitional.dataset.test.txt'\n",
    "        data='./eval_data/CRW/CRW-562.txt'\n",
    "        weights=[WEIGHT_DICT[5],WEIGHT_DICT[3]]\n",
    "#         weights=[WEIGHT_DICT[0]]\n",
    "        gpu=0\n",
    "        model_type='context2vec-skipgram?skipgram'\n",
    "        w2salience_f=None\n",
    "        n_result=20\n",
    "        trials=1\n",
    "#         skipgram_model_f='./eval_data/CRW/vectors.txt'\n",
    "#         skipgram_model_f='../models/wiki_all.model/wiki_all.sent.split.model'\n",
    "        context2vec_model_f='../models/context2vec/model_dir/MODEL-wiki.params.14'\n",
    "        ######w2salience_f\n",
    "#         w2salience_f='../corpora/corpora/wiki.all.utf8.sent.split.tokenized.vocab'\n",
    "        w2salience_f='../corpora/corpora/WWC_norarew.txt.tokenized.vocab'\n",
    "#         w2salience_f='../models/lda/w2entropy'\n",
    "\n",
    "\n",
    "        if model_type=='skipgram':\n",
    "            model_param_file=skipgram_model_f\n",
    "        elif model_type=='context2vec-skipgram':\n",
    "            model_param_file=context2vec_model_f+'?'+skipgram_model_f\n",
    "        elif model_type=='context2vec-skipgram?skipgram':\n",
    "            model_param_file=context2vec_model_f+'?'+skipgram_model_f\n",
    "\n",
    "    \n",
    "    else:\n",
    "        \n",
    "\n",
    "        parser = argparse.ArgumentParser(description='Process some integers.')\n",
    "        parser.add_argument('--f',  type=str,\n",
    "                            help='model_param_file',dest='model_param_file')\n",
    "        parser.add_argument('--m', dest='model_type', type=str,\n",
    "                            help='<model_type: context2vec; context2vec-skipgram (context2vec substitutes in skipgram space); context2vec-skipgram?skipgram (context2vec substitutes in skipgram space plus skipgram context words)>')\n",
    "        parser.add_argument('--w', dest='weights', type=int, nargs='+',help='<weight:{0}>'.format (sys.argv[0],WEIGHT_DICT.items()))       \n",
    "        parser.add_argument('--d', dest='data', type=str, help='data file')\n",
    "        parser.add_argument('--g', dest='gpu',type=int, default=-1,help='gpu, default is -1')\n",
    "        parser.add_argument('--ws', dest='w2salience_f',type=str, default=None,help='word2salience file, optional')\n",
    "        parser.add_argument('--n_result',default=20,dest='n_result',type=int,help='top n result for language model substitutes')\n",
    "        parser.add_argument('--t', dest='trials',type=int, default=100, help='trial number. When trial==1, only test on the most infortive contexts')\n",
    "\n",
    "        args = parser.parse_args()\n",
    "        model_param_file = args.model_param_file\n",
    "        model_type=args.model_type\n",
    "        n_result=args.n_result\n",
    "        weights=[WEIGHT_DICT[w_i] for w_i in args.weights]\n",
    "        trials=args.trials \n",
    "        data =args.data\n",
    "        gpu=args.gpu\n",
    "        w2salience_f=args.w2salience_f\n",
    "           \n",
    "    \n",
    "    #### 2. gpu setup \n",
    "   \n",
    "    if gpu >= 0:\n",
    "        cuda.check_cuda_available()\n",
    "        cuda.get_device(gpu).use()    \n",
    "    xp = cuda.cupy if gpu >= 0 else np\n",
    "    \n",
    "    \n",
    "    #### 3. initialize according to model types\n",
    "    print ('read model....')\n",
    "    if model_type=='context2vec':\n",
    "        #read in model\n",
    "        model_reader = ModelReader(model_param_file,gpu)\n",
    "        w = xp.array(model_reader.w)\n",
    "        index2word = model_reader.index2word\n",
    "        word2index=model_reader.word2index\n",
    "        model = model_reader.model\n",
    "        w_target=None\n",
    "        word2index_target=None\n",
    "        index2word_target=None\n",
    "        \n",
    "    elif model_type=='skipgram':\n",
    "        if not model_param_file.endswith('txt'):\n",
    "            model_w2v = gensim.models.Word2Vec.load(model_param_file)\n",
    "        else:\n",
    "            model_w2v = KeyedVectors.load_word2vec_format(model_param_file)\n",
    "            \n",
    "        w=xp.array(deepcopy(model_w2v.wv.vectors))\n",
    "        #vector normalize for target w embedding, consistent with context2vec w and convenient for cosine computation among substitutes\n",
    "        s = xp.sqrt((w * w).sum(1))\n",
    "        s[s==0.] = 1.\n",
    "        w /= s.reshape((s.shape[0], 1))\n",
    "        \n",
    "        index2word=model_w2v.wv.index2word\n",
    "        word2index={key: model_w2v.wv.vocab[key].index for key in model_w2v.wv.vocab}\n",
    "        w_target=None\n",
    "        word2index_target=None\n",
    "        index2word_target=None\n",
    "        print ('filter words for context....')\n",
    "        model=rm_stopw_context(model_w2v)\n",
    "        \n",
    "    elif model_type=='context2vec-skipgram':\n",
    "        model_param_context,model_param_w2v=model_param_file.split('?')\n",
    "        model_reader = ModelReader(model_param_context,gpu)\n",
    "        w = xp.array(model_reader.w)\n",
    "        index2word = model_reader.index2word\n",
    "        word2index=model_reader.word2index\n",
    "        model = model_reader.model\n",
    "        \n",
    "        if not model_param_file.endswith('txt'):\n",
    "            model_w2v = gensim.models.Word2Vec.load(model_param_w2v)\n",
    "        else:\n",
    "            model_w2v = KeyedVectors.load_word2vec_format(model_param_w2v)\n",
    "        w_target=xp.array(model_w2v.wv.vectors)\n",
    "        index2word_target=model_w2v.wv.index2word\n",
    "        word2index_target={key: model_w2v.wv.vocab[key].index for key in model_w2v.wv.vocab}\n",
    "    \n",
    "    elif model_type=='context2vec-skipgram?skipgram':\n",
    "        model_param_context,model_param_w2v=model_param_file.split('?')\n",
    "        #context2vec-skipgram\n",
    "        model_reader = ModelReader(model_param_context,gpu)\n",
    "        w = xp.array(model_reader.w)\n",
    "        index2word = model_reader.index2word\n",
    "        word2index =model_reader.word2index\n",
    "        model = model_reader.model\n",
    "        \n",
    "        if not model_param_file.endswith('txt'):\n",
    "            model_w2v = gensim.models.Word2Vec.load(model_param_w2v)\n",
    "        else:\n",
    "            model_w2v = KeyedVectors.load_word2vec_format(model_param_w2v)\n",
    "        w_target=xp.array(model_w2v.wv.vectors)\n",
    "        index2word_target=model_w2v.wv.index2word\n",
    "        word2index_target={key: model_w2v.wv.vocab[key].index for key in model_w2v.wv.vocab}\n",
    "    \n",
    "        # skigpram\n",
    "        model_skipgram = model_w2v\n",
    "        w_skipgram=xp.array(deepcopy(model_skipgram.wv.vectors))\n",
    "        #vector normalize for probe w embedding for calculating top vector similarity\n",
    "        s = xp.sqrt((w_skipgram * w_skipgram).sum(1))\n",
    "        s[s==0.] = 1.\n",
    "        w_skipgram /= s.reshape((s.shape[0], 1))\n",
    "        \n",
    "        index2word_skipgram=model_skipgram.wv.index2word\n",
    "        word2index_skipgram={key: model_skipgram.wv.vocab[key].index for key in model_skipgram.wv.vocab}\n",
    "        w_target_skipgram=None\n",
    "        word2index_target_skipgram=None\n",
    "        index2word_target_skipgram=None\n",
    "        \n",
    "        print ('filter words for context....')\n",
    "        model_skipgram=rm_stopw_context(model_skipgram)\n",
    "        \n",
    "    \n",
    "    #remove stop words in target word space and asarray for computing CI\n",
    "    print ('filter words for target....')\n",
    "    w,word2index,index2word=filter_w(w,word2index,index2word)\n",
    "    if  index2word_target!=None:\n",
    "        w_target,word2index_target,index2word_target=filter_w(w_target,word2index_target,index2word_target)\n",
    "    if model_type=='context2vec-skipgram?skipgram':\n",
    "        w_skipgram,word2index_skipgram,index2word_skipgram=filter_w(w_skipgram,word2index_skipgram,index2word_skipgram)\n",
    "    \n",
    "    #### 4. per word weight\n",
    "    \n",
    "    w2salience=None\n",
    "    for wt in weights:\n",
    "        if wt==LDA:\n",
    "            print ('load vectors and entropy')\n",
    "            w2salience=pickle.load(open(w2salience_f))\n",
    "        elif wt in [INVERSE_W_FREQ, INVERSE_S_FREQ]:\n",
    "            print ('load w2freq')\n",
    "            w2salience=load_w2salience(model_w2v,w2salience_f,wt)\n",
    "    \n",
    "\n",
    "\n",
    "    ##### 5. combine parameters for skipgram?context2vec-skipgram\n",
    "    if model_type=='context2vec-skipgram?skipgram':\n",
    "        model=(model,model_skipgram)\n",
    "        w=(w,w_skipgram)\n",
    "        index2word=(index2word,index2word_skipgram)\n",
    "        word2index=(word2index,word2index_skipgram)\n",
    "        w2salience=(w2salience,w2salience)\n",
    "        w_target=(w_target,w_target_skipgram)\n",
    "        word2index_target=(word2index_target,word2index_target_skipgram)\n",
    "        index2word_target=(index2word_target,index2word_target_skipgram)\n",
    "    \n",
    "    print (model_param_file,model_type,weights,data,w2salience_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    ##### 6. read in data and perform evaluation\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "\n",
    "\n",
    "#     data='./eval_data/data-chimeras/dataset_alacarte.l6.fixed.test.txt.punct'\n",
    "    print (os.path.basename(os.path.split(data)[0]))\n",
    "    if os.path.basename(os.path.split(data)[0])== 'data-chimeras':\n",
    "\n",
    "            eval_chimera(data,model,model_type,n_result,w,index2word,word2index,weights,w2salience,w_target,word2index_target,index2word_target)\n",
    "\n",
    "    elif os.path.basename(os.path.split(data)[0])== 'data-nonces':\n",
    "            ranks=eval_nonce(data,model,model_w2v,model_type,n_result,w,index2word,word2index,weights,w2salience,w_target,word2index_target,index2word_target)\n",
    "    \n",
    "    elif os.path.basename(os.path.split(data)[0])=='CRW':\n",
    "        \n",
    "            model_predicts,sps=eval_crw_stf(data,model_param_file,model,model_type,n_result,w,index2word,word2index,weights,w2salience,w_target,word2index_target,index2word_target,trials)\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
