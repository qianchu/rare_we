{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ql261/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import six\n",
    "import sys\n",
    "import os\n",
    "import traceback\n",
    "import re\n",
    "import pickle\n",
    "from copy import deepcopy\n",
    "\n",
    "from chainer import cuda\n",
    "from context2vec.common.context_models import Toks\n",
    "from context2vec.common.model_reader import ModelReader\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import logging\n",
    "from scipy.stats import spearmanr\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import gensim\n",
    "import math\n",
    "import collections\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_top_n_simwords(w_filter,context_embed,n_result,index2word,debug=False):\n",
    "        #assume that w_filter is already normalized\n",
    "        context_embed = context_embed / xp.sqrt((context_embed * context_embed).sum())\n",
    "        similarity_scores=[]\n",
    "        print('producing top {0} simwords'.format(n_result))\n",
    "        similarity = (w_filter.dot(context_embed)+1.0)/2\n",
    "        top_words_i=[]\n",
    "        top_words=[]\n",
    "        count = 0\n",
    "        for i in (-similarity).argsort():\n",
    "                    if xp.isnan(similarity[i]):\n",
    "                        continue\n",
    "                    if debug==True:\n",
    "                        print('{0}: {1}'.format(str(index2word[i]), str(similarity[i])))\n",
    "                    count += 1\n",
    "                    top_words_i.append(i)\n",
    "                    top_words.append(index2word[i])\n",
    "                    similarity_scores.append(similarity[i])\n",
    "                    if count == n_result:\n",
    "                        break\n",
    "\n",
    "        top_vec=w_filter[top_words_i,:]\n",
    "        \n",
    "        return top_vec,np.array(similarity_scores),top_words\n",
    "    \n",
    "def top_mutual_sim(top_vec,similarity_scores):\n",
    "\n",
    "    #normalize the top_vec\n",
    "    s = np.sqrt((top_vec * top_vec).sum(1))\n",
    "    s[s==0.] = 1.\n",
    "    top_vec /= s.reshape((s.shape[0], 1))\n",
    "    \n",
    "    # substitutes' similarity to sentence (similarity_scores) as weight matrix to mutual similarity\n",
    "    max_score=similarity_scores[0]\n",
    "    similarity_scores=np.array(similarity_scores)\n",
    "    sim_weights=(similarity_scores+similarity_scores.reshape(len(similarity_scores),1))/2.0\n",
    "    #weighted by the maximum score in the substitutes (highre max score means the context is more certain about the substitutes)\n",
    "    sim_weights=(sim_weights/float(sum(sum(sim_weights))))*max_score\n",
    "    # dot product weighted by substitute probability (sim_weights)\n",
    "    inf_score=sum(sum(top_vec.dot(top_vec.T)*sim_weights))\n",
    "    return inf_score\n",
    "\n",
    "def top_cluster_density(top_vec,similarity_scores):\n",
    "    #normalize the top_vec\n",
    "    s = np.sqrt((top_vec * top_vec).sum(1))\n",
    "    s[s==0.] = 1.\n",
    "    top_vec = top_vec/ s.reshape((s.shape[0], 1))\n",
    "    \n",
    "    #perform the centroid\n",
    "    max_score=similarity_scores[0]\n",
    "    similarity_scores=np.array(similarity_scores).reshape(len(similarity_scores),1)/sum(similarity_scores)\n",
    "    centroid_vector=sum(top_vec*similarity_scores)\n",
    "    # average of cosine distance to the centroid,weighted by max scores\n",
    "    inf_score=sum(top_vec.dot(centroid_vector))/len(top_vec)*max_score\n",
    "    return inf_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_w2salience(w2salience_f,weight_type):\n",
    "    w2salience={}\n",
    "    with open(w2salience_f) as f:\n",
    "        for line in f:\n",
    "            if line.strip()=='':\n",
    "                continue\n",
    "            w,w_count,s_count=line.strip().split('\\t')\n",
    "            if weight_type==INVERSE_W_FREQ:\n",
    "                w2salience[w]=1/float(w_count)\n",
    "            elif weight_type==INVERSE_S_FREQ:\n",
    "                w2salience[w]=math.log(1+84755431/float(s_count))\n",
    "    return w2salience\n",
    "\n",
    "def skipgram_context(model,words,pos,weight=None,w2entropy=None):\n",
    "    context_wvs=[]\n",
    "    weights=[]\n",
    "    for i,word in enumerate(words):\n",
    "        if i != pos: #surroudn context words\n",
    "            try:\n",
    "                if weight ==LDA:\n",
    "                    if word in w2entropy and word in model:\n",
    "                        print (word,w2entropy[word])\n",
    "                        weights.append(1/(w2entropy[word]+1.0))\n",
    "                        context_wvs.append(model[word])\n",
    "                elif weight in [INVERSE_W_FREQ,INVERSE_S_FREQ]:\n",
    "                    if word in w2entropy and word in model:\n",
    "                        print (word,w2entropy[word])\n",
    "                        weights.append(w2entropy[word])\n",
    "                        context_wvs.append(model[word])\n",
    "                else:\n",
    "                    #equal weights per word\n",
    "                    context_wvs.append(model[word])\n",
    "                    weights.append(1.0)\n",
    "            except KeyError as e:\n",
    "                print ('==warning==: key error in context {0}'.format(e))\n",
    "    print ('per word weights',weights)\n",
    "    context_embed=sum(np.array(context_wvs)*np.array(weights).reshape(len(weights),1))#/sum(weights)\n",
    "    return sum(weights),context_embed #  will be normalized later\n",
    "\n",
    "def lg_model_out_w2v(top_words,w_target,word2index_target):\n",
    "        # lg model substitutes in skipgram embedding\n",
    "        top_vec=[]\n",
    "        index_list=[]\n",
    "        for i,word in enumerate(top_words):\n",
    "            try :\n",
    "                top_vec.append(w_target[word2index_target[word]])\n",
    "                print ('target word substitute',w_target[word2index_target[word]][:10])\n",
    "                index_list.append(i)\n",
    "            except KeyError as e:\n",
    "                print (e)\n",
    "        return np.array(top_vec),index_list\n",
    "    \n",
    "def context_inform(test_s,test_w, model,model_type,n_result,w_filter,index2word,weight,w2entropy=None,w_target=None,word2index_target=None,index2word_target=None):\n",
    "    #produce context representation and infromative score for each context\n",
    "    test_s=test_s.replace(test_w, ' '+test_w+' ')\n",
    "    print(test_s)\n",
    "    words=test_s.split()\n",
    "    pos=words.index(test_w)\n",
    "    \n",
    "    score=1.0 #default score\n",
    "    \n",
    "    # Decide on the model\n",
    "    if model_type=='context2vec':\n",
    "        context_embed= model.context2vec(words, pos)\n",
    "        context_embed_out=context_embed\n",
    "    \n",
    "    elif model_type=='skipgram':\n",
    "        score,context_embed=skipgram_context(model,words,pos,weight,w2entropy)\n",
    "        context_embed_out=context_embed\n",
    "        \n",
    "    elif model_type=='context2vec-skipgram':\n",
    "        # context2vec substitutes in skipgram space\n",
    "        context_embed= model.context2vec(words, pos)\n",
    "        top_vec,sim_scores,top_words=produce_top_n_simwords(w_filter,context_embed,n_result,index2word)\n",
    "        top_vec,index_list=lg_model_out_w2v(top_words,w_target,word2index_target) \n",
    "        sim_scores=sim_scores[index_list] #weighted by substitute probability\n",
    "        if weight==SUBSTITUTE_PROB:\n",
    "            context_embed_out=sum(top_vec*sim_scores.reshape(len(sim_scores),1))\n",
    "        else:\n",
    "            context_embed_out=sum(top_vec*((sim_scores/sum(sim_scores)).reshape(len(sim_scores),1)))\n",
    "    else:\n",
    "        print ('model type {0} not recognized'.format(model_type))\n",
    "        sys.exit(1)\n",
    "        \n",
    "    print('context_embed original', context_embed[:10])\n",
    "    print ('context_embed_out',context_embed_out[:10])\n",
    "    #decide on weight per sentence\n",
    "    print ('weight mode',weight)\n",
    "    if weight==TOP_MUTUAL_SIM:\n",
    "#         if word2index_target==None: #not context2vec-skipgram\n",
    "#             context2vec word embedding space neighbours\n",
    "        top_vec,sim_scores,top_words=produce_top_n_simwords(w_filter,context_embed,n_result,index2word)\n",
    "        #skipgram word embedding space neighbours when context2vec-skipgram\n",
    "        score=top_mutual_sim(top_vec,sim_scores)\n",
    "    elif weight==TOP_CLUSTER_DENSITY:\n",
    "#         if word2index_target==None: #not context2vec-skipgram\n",
    "#             context2vec word embedding space neighbours\n",
    "        top_vec,sim_scores,top_words=produce_top_n_simwords(w_filter,context_embed,n_result,index2word)\n",
    "        score=top_cluster_density(top_vec,sim_scores)\n",
    "    elif weight==SUBSTITUTE_PROB:\n",
    "        score=sum(sim_scores)\n",
    "        print ('substitute prob score',score)\n",
    "    elif weight=='learned':\n",
    "        print ('learned not implemented')\n",
    "    elif weight=='gaussian':\n",
    "        print ('gaussian not implemented')\n",
    "    elif weight ==False or weight in [LDA,INVERSE_S_FREQ,INVERSE_W_FREQ]:\n",
    "        score=score\n",
    "    else:\n",
    "        print ('weight mode {0} not recognized'.format(weight))\n",
    "    return score,context_embed_out\n",
    "\n",
    "def additive_model(test_ss,test_w, model_type,model,n_result,w_filter,index2word,weight=False,w2entropy=None,w_target=None,word2index_target=None,index2word_target=None,f_w=None):\n",
    "    #produce context representation across contexts using weighted average\n",
    "    context_out=[]\n",
    "    context_weights=[]\n",
    "    for test_s in test_ss.split('@@'):\n",
    "        test_s=test_s.strip()\n",
    "        #produce context representation with scores\n",
    "        score,context_embed=context_inform(test_s,test_w, model,model_type,n_result,w_filter,index2word,weight,w2entropy,w_target,word2index_target,index2word_target)\n",
    "        print ('weight is {0}'.format(score))\n",
    "        print ('context_embed is ', context_embed[:10])\n",
    "        context_out.append(context_embed)\n",
    "        context_weights.append(score)\n",
    "    \n",
    "    \n",
    "    print ('context_weights',context_weights)\n",
    "    #sum representation across contexts\n",
    "    context_out=np.array(context_out)\n",
    "    \n",
    "    \n",
    "    if model_type=='skipgram' or weight==SUBSTITUTE_PROB:\n",
    "        # context representation by weighted sum of all context words in all contexts\n",
    "        context_avg=sum(context_out)/sum(context_weights)\n",
    "    else:\n",
    "        norm_weights=np.array(context_weights).reshape(len(context_weights),1)/float(sum(context_weights))\n",
    "        if f_w!=None:\n",
    "            f_w.write(','.join([str(i[0]) for i in norm_weights])+'\\n')\n",
    "        print ('normalized weight: \\n  {0}'.format(norm_weights))\n",
    "        # context represenatation by weighted sum of contexts\n",
    "        context_avg=sum(norm_weights*context_out)\n",
    "    \n",
    "    \n",
    "    # check new embedding neighbours\n",
    "\n",
    "    print('producing top {0} words for new embedding'.format(n_result))\n",
    "    if index2word_target==None:\n",
    "        top_vec,scores,top_words=produce_top_n_simwords(w_filter,context_avg,n_result,index2word)\n",
    "    else:\n",
    "        #print the target space neighbours for context2vec-skipgram\n",
    "        print (w_target.shape)\n",
    "        top_vec,scores,top_words=produce_top_n_simwords(w_target,context_avg,n_result,index2word_target)\n",
    "    \n",
    "    return context_avg\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_w(w,word2index,index2word):\n",
    "    #filter out words with no letters in, and stopwords\n",
    "    stopw=stopwords.words('english')\n",
    "    stopw=[word.encode('utf-8') for word in stopw]\n",
    "    index2word_filter={}\n",
    "    word2index_filter={}\n",
    "    index_filter2index=[]\n",
    "    counter=0\n",
    "    for word in word2index:\n",
    "            if word not in stopw:\n",
    "                    index_filter2index.append(word2index[word])\n",
    "                    word2index_filter[word]=counter\n",
    "                    index2word_filter[counter]=word\n",
    "                    counter+=1\n",
    "    w_filter= w[index_filter2index,:]\n",
    "    return w_filter,word2index_filter,index2word_filter\n",
    "\n",
    "def rm_stopw_context(model):\n",
    "    stopw=stopwords.words('english')\n",
    "    stopw=[word.encode('utf-8') for word in stopw]\n",
    "    \n",
    "    model2={word:model.wv.__getitem__(word) for word in model.wv.vocab if word not in stopw}\n",
    "    return model2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_nonce(sent):\n",
    "    \n",
    "    sents_out=[]\n",
    "    \n",
    "    results=re.finditer('___ ',sent)\n",
    "    matches=[m for m in results]\n",
    "    for i in range(len(matches)):\n",
    "        sent_masked=sent\n",
    "        matches_mask=[(m2.start(0),m2.end(0)) for i2,m2 in enumerate(matches) if i2!=i]\n",
    "        matches_mask=sorted(matches_mask, key=lambda x:x[0],reverse=True)\n",
    "        for m in matches_mask:\n",
    "            sent_masked=sent_masked[:m[0]]+sent_masked[m[1]:]\n",
    "        sents_out.append(sent_masked+' .')\n",
    "    return ' @@ '.join(sents_out)\n",
    "\n",
    "def eval_nonce(nonce_data_f,context_model,model_w2v,model_type,n_result,w,index2word,word2index,weight=False,w2entropy=None,w_target=None,word2index_target=None,index2word_target=None):\n",
    "        ranks = []\n",
    "        mrr = 0.0\n",
    "        data=pd.read_csv(os.path.join(nonce_data_f),delimiter='\\t',header=None,comment='#')\n",
    "        c = 0\n",
    "        for index, row in data.iterrows():\n",
    "#             if index<618:\n",
    "#                 continue\n",
    "            print (index)\n",
    "            sents=preprocess_nonce(row[1])\n",
    "            nonce=row[0]\n",
    "            if nonce not in model_w2v:\n",
    "                print ('{0} not known'.format(nonce))\n",
    "                continue\n",
    "            #compute context representation\n",
    "            if model_type=='context2vec-skipgram?skipgram':\n",
    "                    #context2vevc                \n",
    "                    context_avg_1=additive_model(sents.lower(),'___', model_type.split('?')[0],context_model[0],n_result,w[0],index2word[0],weight[0],w2entropy[0],w_target[0],word2index_target[0],index2word_target[0])\n",
    "                    print ('context2vec avg embed',context_avg_1[:10])\n",
    "                    context_avg_2=additive_model(sents.lower(),'___', model_type.split('?')[1],context_model[1],n_result,w[1],index2word[1],weight[1],w2entropy[1],w_target[1],word2index_target[1],index2word_target[1])\n",
    "                    print ('skipgram avg embed', context_avg_2[:10])\n",
    "                    context_avg=(context_avg_1+context_avg_2)/2\n",
    "                    print ('context2vec avg skipgram', context_avg[:10])\n",
    "                    #compute probe embeddings in skipgram space\n",
    "                    w_out=w[1]\n",
    "                    w_target_out=w_target[1]\n",
    "                    word2index_out=word2index[1]\n",
    "                    word2index_target_out=word2index_target[1]\n",
    "                    \n",
    "            else:\n",
    "                    \n",
    "                    context_avg=additive_model(sents.lower(),'___', model_type,context_model,n_result,w,index2word,weight,w2entropy,w_target,word2index_target,index2word_target)\n",
    "                    print ('context avg out', context_avg[:10])\n",
    "                    w_out=w\n",
    "                    w_target_out=w_target\n",
    "                    word2index_out=word2index\n",
    "                    word2index_target_out=word2index_target\n",
    "            \n",
    "#             context_avg = context_avg / xp.sqrt((context_avg * context_avg).sum())\n",
    "\n",
    "            print ('vector norm: {0}'.format(np.linalg.norm(context_avg)))\n",
    "            # MRR Rank calculation\n",
    "            nns=model_w2v.similar_by_vector(context_avg,topn=len(model_w2v.wv.vocab))\n",
    "\n",
    "            rr = 0\n",
    "            n = 1\n",
    "            for nn in nns:\n",
    "                word = nn[0]\n",
    "                if word == nonce:\n",
    "                    print (word)\n",
    "                    rr = n\n",
    "                    ranks.append(rr)\n",
    "                else:\n",
    "                  n+=1\n",
    "\n",
    "            if rr != 0:\n",
    "                mrr+=float(1)/float(rr)\t\n",
    "            print rr,mrr\n",
    "            c+=1\n",
    "        print (\"Final MRR: \",mrr,c,float(mrr)/float(c))\n",
    "\n",
    "        print ('mediam : {0}'.format(np.median(ranks)))\n",
    "        return ranks\n",
    "            \n",
    "\n",
    "\n",
    "def eval_chimera(chimeras_data_f,context_model,model_type,n_result,w,index2word,word2index,weight=False,w2entropy=None,w_target=None,word2index_target=None,index2word_target=None):\n",
    "    chimeras_data_dir='/'.join(chimeras_data_f.split('/')[:-1])\n",
    "    num_sent=chimeras_data_f.split('/')[-1].split('.')[1][1]\n",
    "    print (chimeras_data_dir)\n",
    "    print (num_sent)\n",
    "    with open(chimeras_data_dir+'/weights_{0}_{1}_{2}'.format(num_sent,model_type,str(weight)),'w') as f_w:\n",
    "        spearmans=[]\n",
    "        data=pd.read_csv(os.path.join(chimeras_data_f),delimiter='\\t',header=None)\n",
    "\n",
    "        for index, row in data.iterrows():\n",
    "            golds=[]\n",
    "            model_predict=[]\n",
    "            probes=[]\n",
    "            #compute context representation\n",
    "            if model_type=='context2vec-skipgram?skipgram':\n",
    "                    #context2vevc\n",
    "                    \n",
    "                    context_avg_1=additive_model(row[1].lower(),'___', model_type.split('?')[0],context_model[0],n_result,w[0],index2word[0],weight[0],w2entropy[0],w_target[0],word2index_target[0],index2word_target[0],f_w)\n",
    "                    print ('context2vec avg embed',context_avg_1[:10])\n",
    "                    context_avg_2=additive_model(row[1].lower(),'___', model_type.split('?')[1],context_model[1],n_result,w[1],index2word[1],weight[1],w2entropy[1],w_target[1],word2index_target[1],index2word_target[1],f_w)\n",
    "                    print ('skipgram avg embed', context_avg_2[:10])\n",
    "                    context_avg=(context_avg_1+context_avg_2)/2\n",
    "                    print ('context2vec avg skipgram', context_avg[:10])\n",
    "                    #compute probe embeddings in skipgram space\n",
    "                    w_out=w[1]\n",
    "                    w_target_out=w_target[1]\n",
    "                    word2index_out=word2index[1]\n",
    "                    word2index_target_out=word2index_target[1]\n",
    "                    \n",
    "            else:\n",
    "                    \n",
    "                    context_avg=additive_model(f_w,row[1].lower(),'___', model_type,context_model,n_result,w,index2word,weight,w2entropy,w_target,word2index_target,index2word_target)\n",
    "                    print ('context avg out', context_avg[:10])\n",
    "                    w_out=w\n",
    "                    w_target_out=w_target\n",
    "                    word2index_out=word2index\n",
    "                    word2index_target_out=word2index_target\n",
    "            \n",
    "            context_avg = context_avg / xp.sqrt((context_avg * context_avg).sum())\n",
    "\n",
    "           \n",
    "            \n",
    "            #cosine similarity with probe embedding\n",
    "            for gold,probe in zip(row[3].split(','),row[2].split(',')):\n",
    "                try:\n",
    "                    if word2index_target_out==None:\n",
    "                        probe_w_vec=xp.array(w_out[word2index_out[probe]])\n",
    "                    else:\n",
    "                        probe_w_vec=xp.array(w_target_out[word2index_target_out[probe]])\n",
    "                    probe_w_vec=probe_w_vec/xp.sqrt((probe_w_vec*probe_w_vec).sum())\n",
    "                    cos=probe_w_vec.dot(context_avg)\n",
    "                    if xp.isnan(cos):\n",
    "                        continue\n",
    "                    else:\n",
    "                        model_predict.append(cos)\n",
    "                        golds.append(gold)\n",
    "                        probes.append(probe)\n",
    "                except KeyError as e:\n",
    "                    print (\"====warning key error for probe=====: {0}\".format(e))\n",
    "            print ('probes',probes)\n",
    "            print ('gold',golds)\n",
    "            print ('model_predict',model_predict)\n",
    "            sp=spearmanr(golds,model_predict)[0]\n",
    "            print ('spearman correlation is {0}'.format(sp))\n",
    "            if not math.isnan(sp):\n",
    "                spearmans.append(sp)\n",
    "        print (\"AVERAGE RHO:\",float(sum(spearmans))/float(len(spearmans)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read model....\n",
      "filter words for context....\n",
      "filter words for target....\n",
      "('../models/wiki_all.model/wiki_all.sent.split.model', 'skipgram', False, './eval_data/data-nonces/n2v.definitional.dataset.train.txt', None)\n"
     ]
    }
   ],
   "source": [
    "TOP_MUTUAL_SIM='top_mutual_sim'\n",
    "TOP_CLUSTER_DENSITY='top_cluster_density'\n",
    "LDA='lda'\n",
    "INVERSE_S_FREQ='inverse_s_freq'\n",
    "INVERSE_W_FREQ='inverse_w_q'\n",
    "SUBSTITUTE_PROB='substitute_prob'\n",
    "WEIGHT_DICT={0:False,1:TOP_MUTUAL_SIM,2:LDA,3:INVERSE_S_FREQ,4:INVERSE_W_FREQ,5:TOP_CLUSTER_DENSITY, 6:SUBSTITUTE_PROB}\n",
    "\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    \n",
    "    #params read in\n",
    "    if sys.argv[0]=='/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py':\n",
    "        \n",
    "#         data='./eval_data/data-chimeras/dataset.l2.fixed.test.txt.punct'\n",
    "        data='./eval_data/data-nonces/n2v.definitional.dataset.train.txt'\n",
    "        weight=WEIGHT_DICT[0]\n",
    "        gpu=-1\n",
    "#         ##context2vec\n",
    "##         model_param_file='../models/context2vec/model_dir/context2vec.ukwac.model.params'\n",
    "#         model_param_file='../models/context2vec/model_dir/MODEL-wiki.params.14'\n",
    "        \n",
    "#         model_type='context2vec'\n",
    "\n",
    "####skipgram\n",
    "        model_param_file='../models/wiki_all.model/wiki_all.sent.split.model'\n",
    "        model_type='skipgram'\n",
    "#         weight='inverse_w_freq'\n",
    "#         w2salience_f='../corpora/corpora/wiki.all.utf8.sent.split.tokenized.vocab'\n",
    "#         w2salience_f='../models/lda/w2entropy'\n",
    "        n_result=20\n",
    "        w2salience_f=None\n",
    "\n",
    "####context2vec-skipgram\n",
    "#         model_param_file='../models/context2vec/model_dir/MODEL-wiki.params.14?../models/wiki_all.model/wiki_all.sent.split.model'\n",
    "# #         model_param_file='../models/context2vec/model_dir/context2vec.ukwac.model.params?../models/wiki_all.model/wiki_all.sent.split.model'\n",
    "#         model_type='context2vec-skipgram?skipgram'\n",
    "#         n_result=20\n",
    "#         w2salience_f=None\n",
    "\n",
    "# #####skipgram?context2vec-skipgram\n",
    "#         model_param_file='../models/context2vec/model_dir/MODEL-wiki.params.14?../models/wiki_all.model/wiki_all.sent.split.model'\n",
    "#         model_type='context2vec-skipgram?skipgram'\n",
    "# #         weight='inverse_w_freq'\n",
    "# #         w2salience_f='../corpora/corpora/wiki.all.utf8.sent.split.tokenized.vocab'\n",
    "# #         w2salience_f='../models/lda/w2entropy'\n",
    "#         n_result=20\n",
    "    \n",
    "    else:\n",
    "        if len(sys.argv) < 6:\n",
    "            print >> sys.stderr, \"Usage: {0} <model_param_file> <model_type: context2vec; context2vec-skipgram (context2vec substitutes in skipgram space); context2vec-skipgram?skipgram (context2vec substitutes in skipgram space plus skipgram context words)> <weight:{1}> <eval_data> <gpu> <w2salience> \"  .format (sys.argv[0],WEIGHT_DICT.items())\n",
    "            sys.exit(1)\n",
    "        \n",
    "        model_param_file = sys.argv[1]\n",
    "        model_type=sys.argv[2]\n",
    "        \n",
    "        if '-' in sys.argv[3]:\n",
    "            weight,n_result=sys.argv[3].split('-')\n",
    "            weight=WEIGHT_DICT[int(weight)]\n",
    "            n_result=int(n_result)\n",
    "        else:\n",
    "            weight=WEIGHT_DICT[int(sys.argv[3])]\n",
    "            n_result=20 #default is 20 top\n",
    "            \n",
    "#         context_rm_stopw=int(sys.argv[4])\n",
    "        data =sys.argv[4]\n",
    "        \n",
    "        gpu=int(sys.argv[5])\n",
    "        \n",
    "        if len(sys.argv)>6:\n",
    "            w2salience_f=argv[6]\n",
    "        else:\n",
    "            w2salience_f=None\n",
    "    \n",
    "    #gpu setup \n",
    "   \n",
    "\n",
    "    if gpu >= 0:\n",
    "        cuda.check_cuda_available()\n",
    "        cuda.get_device(gpu).use()    \n",
    "    xp = cuda.cupy if gpu >= 0 else np\n",
    "    \n",
    "    # logging\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "\n",
    "    \n",
    "    #choose model type\n",
    "    print ('read model....')\n",
    "    if model_type=='context2vec':\n",
    "        #read in model\n",
    "        \n",
    "        model_reader = ModelReader(model_param_file)\n",
    "        w = model_reader.w\n",
    "        index2word = model_reader.index2word\n",
    "        word2index=model_reader.word2index\n",
    "        model = model_reader.model\n",
    "        w_target=None\n",
    "        word2index_target=None\n",
    "        index2word_target=None\n",
    "        \n",
    "    elif model_type=='skipgram':\n",
    "        model_w2v = gensim.models.Word2Vec.load(model_param_file)\n",
    "        w=deepcopy(model_w2v.wv.vectors)\n",
    "        #vector normalize for target w embedding, consistent with context2vec w and convenient for cosine computation among substitutes\n",
    "        s = np.sqrt((w * w).sum(1))\n",
    "        s[s==0.] = 1.\n",
    "        w /= s.reshape((s.shape[0], 1))\n",
    "        \n",
    "        index2word=model_w2v.wv.index2word\n",
    "        word2index={key: model_w2v.wv.vocab[key].index for key in model_w2v.wv.vocab}\n",
    "        w_target=None\n",
    "        word2index_target=None\n",
    "        index2word_target=None\n",
    "        \n",
    "        print ('filter words for context....')\n",
    "\n",
    "        model=rm_stopw_context(model_w2v)\n",
    "        \n",
    "    elif model_type=='context2vec-skipgram':\n",
    "        model_param_context,model_param_w2v=model_param_file.split('?')\n",
    "        model_reader = ModelReader(model_param_context)\n",
    "        w = model_reader.w\n",
    "        index2word = model_reader.index2word\n",
    "        word2index=model_reader.word2index\n",
    "        model = model_reader.model\n",
    "        \n",
    "        model_w2v = gensim.models.Word2Vec.load(model_param_w2v)\n",
    "        w_target=model_w2v.wv.vectors\n",
    "        index2word_target=model_w2v.wv.index2word\n",
    "        word2index_target={key: model_w2v.wv.vocab[key].index for key in model_w2v.wv.vocab}\n",
    "    \n",
    "    elif model_type=='context2vec-skipgram?skipgram':\n",
    "        model_param_context,model_param_w2v=model_param_file.split('?')\n",
    "        #context2vec-skipgram\n",
    "        model_reader = ModelReader(model_param_context)\n",
    "        w = model_reader.w\n",
    "        index2word = model_reader.index2word\n",
    "        word2index =model_reader.word2index\n",
    "        model = model_reader.model\n",
    "        \n",
    "        model_w2v = gensim.models.Word2Vec.load(model_param_w2v)\n",
    "        w_target=model_w2v.wv.vectors\n",
    "        index2word_target=model_w2v.wv.index2word\n",
    "        word2index_target={key: model_w2v.wv.vocab[key].index for key in model_w2v.wv.vocab}\n",
    "    \n",
    "        # skigpram\n",
    "        model_skipgram = model_w2v\n",
    "        w_skipgram=deepcopy(model_skipgram.wv.vectors)\n",
    "        #vector normalize for probe w embedding\n",
    "        s = np.sqrt((w_skipgram * w_skipgram).sum(1))\n",
    "        s[s==0.] = 1.\n",
    "        w_skipgram /= s.reshape((s.shape[0], 1))\n",
    "        \n",
    "        index2word_skipgram=model_skipgram.wv.index2word\n",
    "        word2index_skipgram={key: model_skipgram.wv.vocab[key].index for key in model_skipgram.wv.vocab}\n",
    "        w_target_skipgram=None\n",
    "        word2index_target_skipgram=None\n",
    "        index2word_target_skipgram=None\n",
    "        \n",
    "        print ('filter words for context....')\n",
    "\n",
    "        model_skipgram=rm_stopw_context(model_skipgram)\n",
    "        \n",
    "                \n",
    "        \n",
    "    \n",
    "    #remove stop words in target word space\n",
    "    print ('filter words for target....')\n",
    "    w,word2index,index2word=filter_w(w,word2index,index2word)\n",
    "    if  index2word_target!=None:\n",
    "        w_target,word2index_target,index2word_target=filter_w(w_target,word2index_target,index2word_target)\n",
    "    if model_type=='context2vec-skipgram?skipgram':\n",
    "        w_skipgram,word2index_skipgram,index2word_skipgram=filter_w(w_skipgram,word2index_skipgram,index2word_skipgram)\n",
    "    \n",
    "    #per word weight\n",
    "    \n",
    "    w2salience=None\n",
    "    if weight==LDA:\n",
    "        print ('load vectors and entropy')\n",
    "        w2salience=pickle.load(open(w2salience_f))\n",
    "    elif weight==INVERSE_W_FREQ:\n",
    "        print ('load w2freq')\n",
    "        w2salience=load_w2salience(w2salience_f,weight)\n",
    "    elif weight==INVERSE_S_FREQ:\n",
    "        print ('load w2freq')\n",
    "        w2salience=load_w2salience(w2salience_f,weight)\n",
    "\n",
    "\n",
    "    #combine parameters for skipgram?context2vec-skipgram\n",
    "    if model_type=='context2vec-skipgram?skipgram':\n",
    "        model=(model,model_skipgram)\n",
    "        w=(w,w_skipgram)\n",
    "        index2word=(index2word,index2word_skipgram)\n",
    "        word2index=(word2index,word2index_skipgram)\n",
    "        weight=(weight,WEIGHT_DICT[0])#assume that skipgram has no weight\n",
    "        w2salience=(w2salience,w2salience)\n",
    "        w_target=(w_target,w_target_skipgram)\n",
    "        word2index_target=(word2index_target,word2index_target_skipgram)\n",
    "        index2word_target=(index2word_target,index2word_target_skipgram)\n",
    "    \n",
    "    print (model_param_file,model_type,weight,data,w2salience_f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      " ___  is an inwardly directed emotion that carries two common meanings  .\n",
      "==warning==: key error in context 'is'\n",
      "==warning==: key error in context 'an'\n",
      "==warning==: key error in context 'that'\n",
      "==warning==: key error in context '.'\n",
      "('per word weights', [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0])\n",
      "('context_embed original', array([ 0.79432904, -0.25426867, -0.25418369, -0.76199849,  1.25186341,\n",
      "       -0.39327487,  0.13843274,  0.11596303, -0.03449698,  0.26046094]))\n",
      "('context_embed_out', array([ 0.79432904, -0.25426867, -0.25418369, -0.76199849,  1.25186341,\n",
      "       -0.39327487,  0.13843274,  0.11596303, -0.03449698,  0.26046094]))\n",
      "('weight mode', False)\n",
      "weight is 7.0\n",
      "('context_embed is ', array([ 0.79432904, -0.25426867, -0.25418369, -0.76199849,  1.25186341,\n",
      "       -0.39327487,  0.13843274,  0.11596303, -0.03449698,  0.26046094]))\n",
      "('context_weights', [7.0])\n",
      "producing top 20 words for new embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:27: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Unsupported type <type 'numpy.ndarray'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-128c806a70a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32melif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m \u001b[0;34m'data-nonces'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mranks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_nonce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_w2v\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_result\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex2word\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mword2index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw2salience\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw_target\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mword2index_target\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex2word_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-9d9e9a20d8e3>\u001b[0m in \u001b[0;36meval_nonce\u001b[0;34m(nonce_data_f, context_model, model_w2v, model_type, n_result, w, index2word, word2index, weight, w2entropy, w_target, word2index_target, index2word_target)\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m                     \u001b[0mcontext_avg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madditive_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'___'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcontext_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_result\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex2word\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw2entropy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw_target\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mword2index_target\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex2word_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m                     \u001b[0;32mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'context avg out'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_avg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m                     \u001b[0mw_out\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-7634a402b552>\u001b[0m in \u001b[0;36madditive_model\u001b[0;34m(test_ss, test_w, model_type, model, n_result, w_filter, index2word, weight, w2entropy, w_target, word2index_target, index2word_target, f_w)\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'producing top {0} words for new embedding'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mindex2word_target\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m         \u001b[0mtop_vec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtop_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproduce_top_n_simwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_filter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcontext_avg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_result\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex2word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;31m#print the target space neighbours for context2vec-skipgram\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-aaf280294608>\u001b[0m in \u001b[0;36mproduce_top_n_simwords\u001b[0;34m(w_filter, context_embed, n_result, index2word, debug)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mproduce_top_n_simwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_filter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcontext_embed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_result\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex2word\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m         \u001b[0;31m#assume that w_filter is already normalized\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mcontext_embed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext_embed\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext_embed\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mcontext_embed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0msimilarity_scores\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'producing top {0} simwords'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mcupy/core/core.pyx\u001b[0m in \u001b[0;36mcupy.core.core.ndarray.__div__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mcupy/core/elementwise.pxi\u001b[0m in \u001b[0;36mcupy.core.core.ufunc.__call__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mcupy/core/elementwise.pxi\u001b[0m in \u001b[0;36mcupy.core.core._preprocess_args\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Unsupported type <type 'numpy.ndarray'>"
     ]
    }
   ],
   "source": [
    "    #read in data\n",
    "    if data.split('/')[-2]== 'data-chimeras':\n",
    "\n",
    "            eval_chimera(data,model,model_type,n_result,w,index2word,word2index,weight,w2salience,w_target,word2index_target,index2word_target)\n",
    "\n",
    "    elif data.split('/')[-2]== 'data-nonces':\n",
    "            ranks=eval_nonce(data,model,model_w2v,model_type,n_result,w,index2word,word2index,weight,w2salience,w_target,word2index_target,index2word_target)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
