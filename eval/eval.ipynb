{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import six\n",
    "import sys\n",
    "import os\n",
    "import traceback\n",
    "import re\n",
    "import pickle\n",
    "from copy import deepcopy\n",
    "\n",
    "from chainer import cuda\n",
    "from context2vec.common.context_models import Toks\n",
    "from context2vec.common.model_reader import ModelReader\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import logging\n",
    "from scipy.stats import spearmanr\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import gensim\n",
    "import math\n",
    "import collections\n",
    "import argparse\n",
    "import h5py\n",
    "\n",
    "from collections import defaultdict\n",
    "from gensim.models import KeyedVectors\n",
    "from scipy.stats import pearsonr\n",
    "FLOAT = np.float32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def produce_top_n_simwords(w_filter,context_embed,n_result,index2word,debug=False):\n",
    "        #assume that w_filter is already normalized\n",
    "        context_embed = context_embed / xp.sqrt((context_embed * context_embed).sum())\n",
    "        s = xp.sqrt((w_filter * w_filter).sum(1))\n",
    "        s[s==0.] = 1.\n",
    "        w_filter /= s.reshape((s.shape[0], 1))\n",
    "        similarity_scores=[]\n",
    "#         print('producing top {0} simwords'.format(n_result))\n",
    "        similarity = (w_filter.dot(context_embed)+1.0)/2\n",
    "        top_words_i=[]\n",
    "        top_words=[]\n",
    "        count = 0\n",
    "        for i in (-similarity).argsort():\n",
    "                    if xp.isnan(similarity[i]):\n",
    "                        continue\n",
    "                    if debug==True:\n",
    "                        try:\n",
    "                            print('{0}: {1}'.format(str(index2word[int(i)]), str(similarity[int(i)])))\n",
    "                        except UnicodeEncodeError as e:\n",
    "                            print (e)\n",
    "                            \n",
    "                    count += 1\n",
    "                    top_words_i.append(int(i))\n",
    "                    top_words.append(index2word[int(i)])\n",
    "                    similarity_scores.append(float(similarity[int(i)]))\n",
    "                    if count == n_result:\n",
    "                        break\n",
    "\n",
    "        top_vec=w_filter[top_words_i,:]\n",
    "        return top_vec,xp.array(similarity_scores),top_words\n",
    "\n",
    "def top_cluster_density(top_vec,similarity_scores):\n",
    "    #normalize the top_vec\n",
    "    s = xp.sqrt((top_vec * top_vec).sum(1))\n",
    "    s[s==0.] = 1.\n",
    "    top_vec = top_vec/ s.reshape((s.shape[0], 1))\n",
    "    #perform the centroid\n",
    "    max_score=similarity_scores[0]\n",
    "    similarity_scores=xp.array(similarity_scores).reshape(len(similarity_scores),1)/sum(similarity_scores)\n",
    "    centroid_vector=sum(top_vec*similarity_scores)\n",
    "    # average of cosine distance to the centroid,weighted by max scores\n",
    "    inf_score=float(sum(top_vec.dot(centroid_vector))/len(top_vec)*max_score)\n",
    "    return inf_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_w2salience(model_w2v,w2salience_f,weight_type):\n",
    "    w2salience={}\n",
    "    with open(w2salience_f) as f:\n",
    "        for line in f:\n",
    "            line=line.strip()\n",
    "            if line=='':\n",
    "                continue\n",
    "            if line.startswith('sentence total'):\n",
    "                sent_total=int(line.split(':')[1])\n",
    "                continue\n",
    "            w,w_count,s_count=line.split('\\t')\n",
    "            if model_w2v.wv.__contains__(w):\n",
    "                if weight_type==INVERSE_W_FREQ:\n",
    "                    w2salience[w]=1/float(w_count)\n",
    "                elif weight_type==INVERSE_S_FREQ:\n",
    "                    w2salience[w]=math.log(1+sent_total/float(s_count))\n",
    "    #                 w2salience[w]=math.log(1+84755431/float(s_count))\n",
    "    return w2salience\n",
    "\n",
    "def skipgram_context(model,words,pos,weight=None,w2entropy=None):\n",
    "    context_wvs=[]\n",
    "    weights=[]\n",
    "    for i,word in enumerate(words):\n",
    "        if i != pos: #surroudn context words\n",
    "            if word in model:\n",
    "                if weight ==LDA:\n",
    "                    if word in w2entropy:\n",
    "                        weights.append(1/(w2entropy[word]+1.0))\n",
    "                        context_wvs.append(model[word])\n",
    "        \n",
    "                elif weight in [INVERSE_W_FREQ,INVERSE_S_FREQ]:\n",
    "                    if word in w2entropy:\n",
    "                        model[word]\n",
    "                        weights.append(w2entropy[word])\n",
    "                        context_wvs.append(model[word])\n",
    "                    \n",
    "                else:\n",
    "                    #equal weights per word\n",
    "                    context_wvs.append(model[word])\n",
    "                    weights.append(1.0)\n",
    "            else:\n",
    "                pass\n",
    "#                 print ('==warning==: key error in context {0}'.format(word))\n",
    "#     print ('per word weights',weights)\n",
    "    context_embed=sum(np.array(context_wvs)*np.array(weights).reshape(len(weights),1))#/sum(weights)\n",
    "#     print ('skipgram context sum:', context_embed[:10])\n",
    "    return sum(weights),context_embed #  will be normalized later\n",
    "\n",
    "def lg_model_out_w2v(top_words,w_target,word2index_target):\n",
    "        # lg model substitutes in skipgram embedding\n",
    "        top_vec=[]\n",
    "        index_list=[]\n",
    "        for i,word in enumerate(top_words):\n",
    "            try :\n",
    "                top_vec.append(w_target[word2index_target[word]])\n",
    "                index_list.append(i)\n",
    "            except KeyError as e:\n",
    "                pass\n",
    "#                 print ('lg subs {0} not in w2v'.format(e))\n",
    "        if top_vec==[]:\n",
    "            print ('no lg subs in w2v space')\n",
    "            return xp.array([]),[]\n",
    "        else:\n",
    "            return xp.stack(top_vec),index_list\n",
    "    \n",
    "def context_inform(test_s,test_w, model,model_type,n_result,w_filter,index2word,weight,w2entropy=None,w_target=None,word2index_target=None,index2word_target=None):\n",
    "    #produce context representation and infromative score for each context\n",
    "\n",
    "    test_s=test_s.replace(test_w, ' '+test_w+' ')\n",
    "    words=test_s.split()\n",
    "    pos=words.index(test_w)\n",
    "\n",
    "    score=1.0 #default score\n",
    "    \n",
    "    # Decide on the model\n",
    "    if model_type=='context2vec':\n",
    "            context_embed= model.context2vec(words, pos)\n",
    "\n",
    "    elif model_type=='skipgram' or model_type=='a la carte':\n",
    "        score,context_embed=skipgram_context(model,words,pos,weight,w2entropy)\n",
    "        context_embed_out=xp.array(context_embed)\n",
    "        \n",
    "    elif model_type=='context2vec-skipgram':\n",
    "        context_embed= model.context2vec(words, pos)\n",
    "\n",
    "        top_vec,sim_scores,top_words=produce_top_n_simwords(w_filter,context_embed,n_result,index2word,debug=True)\n",
    "        top_vec,index_list=lg_model_out_w2v(top_words,w_target,word2index_target) \n",
    "        sim_scores=sim_scores[index_list] #weighted by substitute probability\n",
    "        if weight==SUBSTITUTE_PROB:\n",
    "            context_embed_out=xp.array(sum(top_vec*sim_scores.reshape(len(sim_scores),1)))\n",
    "        else:\n",
    "            context_embed_out=xp.array(sum(top_vec*((sim_scores/sum(sim_scores)).reshape(len(sim_scores),1))))\n",
    "        \n",
    "    else:\n",
    "        print ('model type {0} not recognized'.format(model_type))\n",
    "        sys.exit(1)\n",
    "        \n",
    "    #decide on weight per sentence\n",
    "    \n",
    "    if weight==TOP_MUTUAL_SIM:\n",
    "#         if word2index_target==None: #not context2vec-skipgram\n",
    "#             context2vec word embedding space neighbours\n",
    "        top_vec,sim_scores,top_words=produce_top_n_simwords(w_filter,context_embed,n_result,index2word)\n",
    "        #skipgram word embedding space neighbours when context2vec-skipgram\n",
    "        score=top_mutual_sim(top_vec,sim_scores)\n",
    "    elif weight==TOP_CLUSTER_DENSITY:\n",
    "\n",
    "        top_vec,sim_scores,top_words=produce_top_n_simwords(w_filter,context_embed,n_result,index2word)\n",
    "        score=top_cluster_density(top_vec,sim_scores)\n",
    "    elif weight==SUBSTITUTE_PROB:\n",
    "        score=sum(sim_scores)\n",
    "        print ('substitute prob score',score)\n",
    "    elif weight=='learned':\n",
    "        print ('learned not implemented')\n",
    "    elif weight=='gaussian':\n",
    "        print ('gaussian not implemented')\n",
    "    elif weight ==False or weight in [LDA,INVERSE_S_FREQ,INVERSE_W_FREQ]:\n",
    "        score=score\n",
    "    else:\n",
    "        print ('weight mode {0} not recognized'.format(weight))\n",
    "    return float(score),context_embed_out\n",
    "\n",
    "def additive_model(test_ss,test_w, model_type,model,n_result,w_filter,index2word,weight=False,w2entropy=None,w_target=None,word2index_target=None,index2word_target=None,f_w=None,context2vec_preembeds=None,scores=None,M=None):\n",
    "    #produce context representation across contexts using weighted average\n",
    "    print ('model type is :{0}'.format(model_type))\n",
    "    context_out=[]\n",
    "    context_weights=[]\n",
    "    for test_id in range(len(test_ss)):\n",
    "        test_s=test_ss[test_id]\n",
    "        test_s=test_s.lower().strip()\n",
    "        #produce context representation with scores\n",
    "        if type(context2vec_preembeds)!=type(None):\n",
    "            context_embed=xp.array(context2vec_preembeds[test_id])\n",
    "            score=float(scores[test_id])\n",
    "        else:\n",
    "            \n",
    "            score,context_embed=context_inform(test_s,test_w, model,model_type,n_result,w_filter,index2word,weight,w2entropy,w_target,word2index_target,index2word_target)\n",
    "        \n",
    "        if score==0 or context_embed.all()==0:\n",
    "            print ('empty context vector')\n",
    "           \n",
    "        else:\n",
    "            context_out.append(context_embed)\n",
    "            context_weights.append(score)\n",
    "#     print ('context_weights',context_weights)\n",
    "    #sum representation across contexts\n",
    "    if context_out==[]:\n",
    "        return None\n",
    "    else:\n",
    "        context_out=xp.stack(context_out)\n",
    "    \n",
    "    print (type(M))\n",
    "    if type(M)!=type(None):\n",
    "        context_avg=sum(context_out)\n",
    "        context_avg=M.dot(context_avg)\n",
    "        print ('add alacarte')\n",
    "        #.dot(M.T)\n",
    "\n",
    "    elif model_type=='skipgram' or weight==SUBSTITUTE_PROB:\n",
    "        # context representation by weighted sum of all context words in all contexts\n",
    "#         print ('context out: ', context_out[:10])\n",
    "#         print ('context_weights',context_weights)\n",
    "        context_avg=sum(context_out)/sum(context_weights)\n",
    "   \n",
    "    else:\n",
    "        norm_weights=xp.array(context_weights).reshape(len(context_weights),1)/float(sum(context_weights))\n",
    "        if f_w!=None:\n",
    "            f_w.write(','.join([str(i[0]) for i in norm_weights])+'\\n')\n",
    "        # context represenatation by weighted sum of contexts\n",
    "        \n",
    "        context_avg=sum(norm_weights*context_out)\n",
    "    \n",
    "    \n",
    "    # check new embedding neighbours\n",
    "\n",
    "    print('producing top {0} words for new embedding'.format(n_result))\n",
    "    if index2word_target==None:\n",
    "        top_vec,scores,top_words=produce_top_n_simwords(w_filter,context_avg,n_result,index2word,debug=True)\n",
    "    else:\n",
    "        #print the target space neighbours for context2vec-skipgram\n",
    "        top_vec,scores,top_words=produce_top_n_simwords(w_target,context_avg,n_result,index2word_target,debug=True)\n",
    "    return context_avg\n",
    "\n",
    "\n",
    "def contexts_per_tgw(sents,model_type,context_model,n_result,w,index2word,weight,w2entropy,w_target,word2index_target,index2word_target,context2vec_preembeds=None,scores=None,M=None):\n",
    "    \n",
    "    if model_type=='context2vec-skipgram?skipgram':\n",
    "#             \n",
    "            #context2vevc      \n",
    "            context_avg_1=additive_model(sents,'___', model_type.split('?')[0],context_model[0],n_result,w[0],index2word[0],weight[0],w2entropy[0],w_target[0],word2index_target[0],index2word_target[0],context2vec_preembeds=context2vec_preembeds,scores=scores,M=M)\n",
    "            #skipgram\n",
    "\n",
    "            context_avg_2=additive_model(sents,'___', model_type.split('?')[1],context_model[1],n_result,w[1],index2word[1],weight[1],w2entropy[1],w_target[1],word2index_target[1],index2word_target[1],M=M)\n",
    "\n",
    "            if type(context_avg_1)!=type(None) and type(context_avg_2)!=type(None):\n",
    "                context_avg=(context_avg_1+context_avg_2)/2\n",
    "\n",
    "            elif type(context_avg_1)!=type(None):\n",
    "                context_avg=context_avg_1\n",
    "\n",
    "            elif type(context_avg_2)!=type(None):\n",
    "                context_avg=context_avg_2\n",
    "            else:\n",
    "                context_avg=None\n",
    "            \n",
    "    else:\n",
    "\n",
    "            context_avg=additive_model(sents,'___', model_type,context_model,n_result,w,index2word,weight[0],w2entropy,w_target,word2index_target,index2word_target,context2vec_preembeds=context2vec_preembeds,scores=scores,M=M)\n",
    "#             print ('context avg out', context_avg[:10])\n",
    "    return context_avg\n",
    "  \n",
    "def output_embedding(w,w_target,word2index,word2index_target):\n",
    "    if model_type=='context2vec-skipgram?skipgram':\n",
    "        #compute probe embeddings in skipgram space\n",
    "            w_out=w[1]\n",
    "            w_target_out=w_target[1]\n",
    "            word2index_out=word2index[1]\n",
    "            word2index_target_out=word2index_target[1]\n",
    "    else:\n",
    "            w_out=w\n",
    "            w_target_out=w_target\n",
    "            word2index_out=word2index\n",
    "            word2index_target_out=word2index_target\n",
    "    if word2index_target_out==None:\n",
    "        return w_out,word2index_out\n",
    "    else:\n",
    "        return w_target_out,word2index_target_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_w(w,word2index,index2word):\n",
    "    #filter out words with no letters in, and stopwords\n",
    "    stopw=stopwords.words('english')\n",
    "    stopw=[word.encode('utf-8') for word in stopw]\n",
    "    index2word_filter={}\n",
    "    word2index_filter={}\n",
    "    index_filter2index=[]\n",
    "    counter=0\n",
    "    for word in word2index:\n",
    "            if word not in stopw: #and re.search(r'[^a-zA-Z]',word)==None:\n",
    "                    index_filter2index.append(word2index[word])\n",
    "                    word2index_filter[word]=counter\n",
    "                    index2word_filter[counter]=word\n",
    "                    counter+=1\n",
    "    w_filter= w[index_filter2index,:]\n",
    "    return w_filter,word2index_filter,index2word_filter\n",
    "\n",
    "def rm_stopw_context(model):\n",
    "    stopw=stopwords.words('english')\n",
    "    stopw=[word.encode('utf-8') for word in stopw]\n",
    "    \n",
    "    model2={word:model.wv.__getitem__(word) for word in model.wv.vocab if word not in stopw}\n",
    "    return model2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine(context_avg,probe_w_vec):\n",
    "    if context_avg.all()==0. or probe_w_vec.all()==0.:\n",
    "        return 0.\n",
    "        \n",
    "    context_avg = context_avg / xp.sqrt((context_avg * context_avg).sum())\n",
    "    probe_w_vec=probe_w_vec/xp.sqrt((probe_w_vec*probe_w_vec).sum())\n",
    "    cos=float(probe_w_vec.dot(context_avg))\n",
    "    print (cos)\n",
    "    if np.isnan(cos):\n",
    "        print ('Warning: cos is nan')\n",
    "        sys.exit(1)\n",
    "    return cos\n",
    "\n",
    "def preprocess_nonce(sent,contexts):\n",
    "    \n",
    "    sents_out=[]\n",
    "    \n",
    "    sent=sent.lower()\n",
    "    results=re.finditer('___ ',sent)\n",
    "    matches=[m for m in results]\n",
    "    for i in range(len(matches)):\n",
    "        sent_masked=sent\n",
    "        matches_mask=[(m2.start(0),m2.end(0)) for i2,m2 in enumerate(matches) if i2!=i]\n",
    "        matches_mask=sorted(matches_mask, key=lambda x:x[0],reverse=True)\n",
    "        for m in matches_mask:\n",
    "            sent_masked=sent_masked[:m[0]]+sent_masked[m[1]:]\n",
    "        sents_out.append(sent_masked+' .')\n",
    "    return sents_out\n",
    "\n",
    "def update_mrr(nns,nonce,mrr,ranks):\n",
    "    rr = 0\n",
    "    n = 1\n",
    "    for nn in nns:\n",
    "        word = nn[0]\n",
    "        if word == nonce:\n",
    "            print (word)\n",
    "            rr = n\n",
    "            ranks.append(rr)\n",
    "        else:\n",
    "            n+=1\n",
    "\n",
    "    if rr != 0:\n",
    "        mrr+=float(1)/float(rr)\t\n",
    "    print rr,mrr\n",
    "    return mrr,ranks\n",
    "\n",
    "def eval_nonce(nonce_data_f,context_model,model_w2v,model_type,n_result,w,index2word,word2index,weight=False,w2entropy=None,w_target=None,word2index_target=None,index2word_target=None,contexts=None,M=None):\n",
    "        #read in contexts\n",
    "        ranks = []\n",
    "        mrr = 0.0\n",
    "        data=pd.read_csv(os.path.join(nonce_data_f),delimiter='\\t',header=None,comment='#')\n",
    "        \n",
    "        for index, row in data.iterrows():\n",
    "            if index>100 and index%100==0:\n",
    "                print (index)\n",
    "            sents=preprocess_nonce(row[1],contexts)\n",
    "            nonce=row[0]\n",
    "            if nonce not in model_w2v:\n",
    "                print ('{0} not known'.format(nonce))\n",
    "                continue\n",
    "            context_avg=contexts_per_tgw(sents,model_type,context_model,n_result,w,index2word,weight,w2entropy,w_target,word2index_target,index2word_target,M=M)\n",
    "            if xp==cuda.cupy:\n",
    "                context_avg=xp.asnumpy(context_avg)\n",
    "                \n",
    "            # MRR Rank calculation\n",
    "            nns=model_w2v.similar_by_vector(context_avg,topn=len(model_w2v.wv.vocab))\n",
    "            mrr,ranks=update_mrr(nns,nonce,mrr,ranks)\n",
    "\n",
    "        print (\"Final MRR: \",mrr,len(ranks),float(mrr)/float(len(ranks)))\n",
    "        print ('mean: ', np.mean(ranks))\n",
    "        print ('mediam : {0}'.format(np.median(ranks)))\n",
    "        return ranks\n",
    "def load_transform(Afile,model):\n",
    "    '''loads the transform from a text file\n",
    "    Args:\n",
    "    Afile: string; transform file name\n",
    "    Returns:\n",
    "    numpy array\n",
    "    '''\n",
    "    if Afile.endswith('bin'):\n",
    "        M = np.fromfile(Afile, dtype=FLOAT)\n",
    "        d = int(np.sqrt(M.shape[0]))\n",
    "        print (d)\n",
    "        assert d == next(iter(model.values())).shape[0], \"induction matrix dimension and word embedding dimension must be the same\"\n",
    "        M = M.reshape(d, d)\n",
    "        M=xp.array(M)\n",
    "        return M\n",
    "    elif Afile.endswith('txt'):\n",
    "        with open(Afile, 'r') as f:\n",
    "            return xp.array(np.vstack([np.array([FLOAT(x) for x in line.split()]) for line in f]))\n",
    "            \n",
    "def eval_nonce_withcontexts(nonce_data_f,context_model,model_w2v,model_type,n_result,w,index2word,word2index,weight=False,w2entropy=None,w_target=None,word2index_target=None,index2word_target=None,trials=100,M=None):\n",
    "\n",
    "    data=pd.read_csv(os.path.join(nonce_data_f),delimiter='\\t',header=None,comment='#')\n",
    "    ranks=defaultdict(lambda: defaultdict(list))\n",
    "    mrrs=defaultdict(lambda: defaultdict(int))\n",
    "    context2vec_preembeds_all=None\n",
    "    context2vec_preembeds=None\n",
    "    scores_all=None\n",
    "    orders_inf=None\n",
    "    scores=None\n",
    "#     start evaluation\n",
    "    contexts_f=os.path.join(os.path.dirname(nonce_data_f),'contexts')\n",
    "    \n",
    "    for index, row in data.iterrows():\n",
    "        rw=row[0]\n",
    "        if rw not in model_w2v:\n",
    "                print ('{0} not known'.format(rw))\n",
    "                continue\n",
    "        contexts_w_f=os.path.join(contexts_f,rw+'.txt')\n",
    "        if not os.path.isfile(contexts_w_f):\n",
    "            print ('{0} does not have contexts'.format(rw))\n",
    "            continue\n",
    "            \n",
    "        print ('\\n==========processing rareword {0}'.format(rw))\n",
    "\n",
    "        #contexts:\n",
    "        #load sentences\n",
    "        sents_all=load_sents(contexts_w_f,rw)\n",
    "        #load contexts\n",
    "        if 'context2vec' in model_type.split('?')[0]:\n",
    "                if model_type=='context2vec-skipgram?skipgram':\n",
    "                    context2vec_preembeds_all,scores_all=load_contexts(rw,sents_all,context_model[0],model_type.split('?')[0],n_result,w[0],index2word[0],weight[0],w2entropy[0],w_target[0],word2index_target[0],index2word_target[0])\n",
    "                elif model_type=='context2vec-skipgram':\n",
    "                    context2vec_preembeds_all,scores_all=load_contexts(rw,sents_all,context_model,model_type.split('?')[0],n_result,w,index2word,weight[0],w2entropy,w_target,word2index_target,index2word_target)\n",
    "                orders_inf=(-scores_all).argsort()\n",
    "        #do trials\n",
    "        for trial in range(trials):\n",
    "            print ('\\n=====Trial no. {0}'.format(trial))\n",
    "            perm = np.random.permutation(255)\n",
    "            for logfreq in range(8):\n",
    "                freq = 2**logfreq\n",
    "                print ('\\ncontext num is {0}'.format(freq))\n",
    "                context2vec_preembeds, scores,sents=produce_contexts_per_trial(trials,freq,perm,scores_all,context2vec_preembeds_all,orders_inf,sents_all)\n",
    "                context_avg=contexts_per_tgw(sents,model_type,context_model,n_result,w,index2word,weight,w2entropy,w_target,word2index_target,index2word_target,context2vec_preembeds,scores,M=M)\n",
    "                \n",
    "                if type(context_avg)!=type(None):\n",
    "                    if xp==cuda.cupy:\n",
    "                        context_avg=xp.asnumpy(context_avg)\n",
    "                \n",
    "                    # MRR Rank calculation\n",
    "                    nns=model_w2v.similar_by_vector(context_avg,topn=len(model_w2v.wv.vocab))\n",
    "                    mrrs[trial][freq],ranks[trial][freq]=update_mrr(nns,rw,mrrs[trial][freq],ranks[trial][freq])\n",
    "    mrr_res=defaultdict(list)\n",
    "    median_res=defaultdict(list)\n",
    "    for trial in mrrs:\n",
    "        for freq in mrrs[trial]:\n",
    "            mrr_res[freq].append(float(mrrs[trial][freq])/float(len(ranks[trial][freq])))\n",
    "            median_res[freq].append(np.median(ranks[trial][freq]))\n",
    "    print ('{0}\\t{1}\\t{2}\\t{3}\\t{4}'.format('freq','MRR','MRR_STD','MEDIAN','MEDIAN_STD'))\n",
    "    for freq in sorted(mrr_res.keys()):\n",
    "        print ('{0}\\t{1}\\t{2}\\t{3}\\t{4} '.format(freq,np.mean(np.array(mrr_res[freq])),np.std(np.array(mrr_res[freq])),np.mean(np.array(median_res[freq])),np.std(np.array(median_res[freq]))))\n",
    "    return mrr_res,median_res\n",
    "\n",
    "def eval_chimera(chimeras_data_f,context_model,model_type,n_result,w,index2word,word2index,weight=False,w2entropy=None,w_target=None,word2index_target=None,index2word_target=None,M=None):\n",
    "    chimeras_data_dir=os.path.dirname(chimeras_data_f)\n",
    "    num_sent=os.path.basename(chimeras_data_f).split('.')[1][1]\n",
    "    print (chimeras_data_dir)\n",
    "    print (num_sent)\n",
    "    with open(chimeras_data_dir+'/weights_{0}_{1}_{2}'.format(num_sent,model_type,str(weight)),'w') as f_w:\n",
    "        spearmans=[]\n",
    "        data=pd.read_csv(os.path.join(chimeras_data_f),delimiter='\\t',header=None)\n",
    "        w_target_out,word2index_target_out=output_embedding(w,w_target,word2index,word2index_target)\n",
    "        for index, row in data.iterrows():\n",
    "            if index>100 and index%100==0:\n",
    "                print (index)\n",
    "            golds=[]\n",
    "            model_predict=[]\n",
    "            probes=[]\n",
    "            sents=row[1].lower().split('@@')\n",
    "            #compute context representation\n",
    "            context_avg=contexts_per_tgw(sents,model_type,context_model,n_result,w,index2word,weight,w2entropy,w_target,word2index_target,index2word_target,M=M)\n",
    "            #cosine similarity with probe embedding\n",
    "            for gold,probe in zip(row[3].split(','),row[2].split(',')):\n",
    "                try:\n",
    "                    probe_w_vec=w_target_out[word2index_target_out[probe]]\n",
    "                    cos=cosine(context_avg,probe_w_vec)\n",
    "                    model_predict.append(cos)\n",
    "                    golds.append(gold)\n",
    "                    probes.append(probe)\n",
    "                except KeyError as e:\n",
    "                    print (\"====warning key error for probe=====: {0}\".format(e))\n",
    "            print ('probes',probes)\n",
    "            print ('gold',golds)\n",
    "            print ('model_predict',model_predict)\n",
    "            sp=spearmanr(golds,model_predict)[0]\n",
    "            print ('spearman correlation is {0}'.format(sp))\n",
    "            if not math.isnan(sp):\n",
    "                spearmans.append(sp)\n",
    "        print (\"AVERAGE RHO:\",float(sum(spearmans))/float(len(spearmans)))\n",
    "\n",
    "\n",
    "def load_contexts(rw,sents,model,model_type,n_result,w_filter,index2word,weight,w2entropy,w_target,word2index_target,index2word_target):          \n",
    "    print ('loading contexts..')\n",
    "    contexts=[]\n",
    "    scores=[]\n",
    "    for i,test_s in enumerate(sents):\n",
    "        test_s=sents[i]\n",
    "        test_s=test_s.replace(TARGET_W, ' '+TARGET_W+' ')\n",
    "        print(i),\n",
    "        words=test_s.split()\n",
    "        pos=words.index(TARGET_W)\n",
    "        score,context_embed=context_inform(test_s,TARGET_W, model,model_type,n_result,w_filter,index2word,weight,w2entropy,w_target,word2index_target,index2word_target)\n",
    "        if context_embed.all()==0:\n",
    "            context_embed=xp.zeros(w_target.shape[-1])\n",
    "            score=0\n",
    "        contexts.append(context_embed)\n",
    "        scores.append(score)\n",
    "    contexts=xp.stack(contexts,axis=0)\n",
    "    scores=np.array(scores)\n",
    "    print ('\\ncontexts for {0} completed'.format(rw))\n",
    "    return contexts,scores\n",
    "\n",
    "def load_sents(contexts_w_f,rw):\n",
    "    sents_all=[]\n",
    "    with open (contexts_w_f) as f:\n",
    "        for line in f:\n",
    "            line=line.replace(rw,TARGET_W).replace(rw.replace('_',' '),TARGET_W).replace(rw.replace('-',' '),TARGET_W).strip().lower()\n",
    "            if not line.endswith('.'):\n",
    "                line=line+' .'\n",
    "            sents_all.append(line)\n",
    "    sents_all=np.array(sents_all)\n",
    "    return sents_all\n",
    "\n",
    "def produce_contexts_per_trial(trials,freq,perm,scores_all,context2vec_preembeds_all,orders_inf,sents_all):\n",
    "    if trials==1 and type(scores_all)!=type(None):\n",
    "        context_inds=orders_inf[:freq]\n",
    "    else:\n",
    "        context_inds=perm[np.array(range(freq-1, 2*freq-1)),]\n",
    "\n",
    "    sents=sents_all[np.array(context_inds),]\n",
    "    print (context_inds)\n",
    "    if type(context2vec_preembeds_all)!=type(None):\n",
    "        context2vec_preembeds=context2vec_preembeds_all[context_inds,]\n",
    "        scores=scores_all[context_inds,]\n",
    "        return context2vec_preembeds, scores,sents\n",
    "    else:\n",
    "        return None,None,sents\n",
    "    \n",
    "\n",
    "def eval_crw_stf(crw_stf_f,model_param_f,context_model,model_type,n_result,w,index2word,word2index,weight=False,w2entropy=None,w_target=None,word2index_target=None,index2word_target=None,trials=100,M=None):\n",
    "    data=pd.read_csv(os.path.join(crw_stf_f),delimiter='\\t',header=None,comment='#')\n",
    "    model_predicts=defaultdict(lambda: defaultdict(list))\n",
    "    golds=defaultdict(lambda: defaultdict(list))\n",
    "    context2vec_preembeds_all=None\n",
    "    context2vec_preembeds=None\n",
    "    scores_all=None\n",
    "    orders_inf=None\n",
    "    scores=None\n",
    "    rw_prev=''\n",
    "#     start evaluation\n",
    "    w_target_out,word2index_target_out=output_embedding(w,w_target,word2index,word2index_target)\n",
    "    contexts_f=os.path.join(os.path.dirname(crw_stf_f),'context')\n",
    "    \n",
    "    for index, row in data.iterrows():\n",
    "        probe_w=row[0]\n",
    "        rw=row[1]\n",
    "        if probe_w not in word2index_target_out: \n",
    "            continue\n",
    "        gold=row[2]\n",
    "        print ('\\n==========processing rareword {0}'.format(rw))\n",
    "\n",
    "        #load sentences\n",
    "        contexts_w_f=os.path.join(contexts_f,rw+'.txt')\n",
    "        sents_all=load_sents(contexts_w_f,rw)\n",
    "        #load contexts\n",
    "        if 'context2vec' in model_type.split('?')[0]:\n",
    "            if rw_prev!=rw:\n",
    "                rw_prev=rw\n",
    "                if model_type=='context2vec-skipgram?skipgram':\n",
    "                    context2vec_preembeds_all,scores_all=load_contexts(rw,sents_all,context_model[0],model_type.split('?')[0],n_result,w[0],index2word[0],weight[0],w2entropy[0],w_target[0],word2index_target[0],index2word_target[0])\n",
    "                elif model_type=='context2vec-skipgram':\n",
    "                    context2vec_preembeds_all,scores_all=load_contexts(rw,sents_all,context_model,model_type.split('?')[0],n_result,w,index2word,weight[0],w2entropy,w_target,word2index_target,index2word_target)\n",
    "                orders_inf=(-scores_all).argsort()\n",
    "        #do trials\n",
    "        for trial in range(trials):\n",
    "            print ('\\n=====Trial no. {0}'.format(trial))\n",
    "            perm = np.random.permutation(255)\n",
    "            for logfreq in range(8):\n",
    "                freq = 2**logfreq\n",
    "                print ('\\ncontext num is {0}'.format(freq))\n",
    "                context2vec_preembeds, scores,sents=produce_contexts_per_trial(trials,freq,perm,scores_all,context2vec_preembeds_all,orders_inf,sents_all)\n",
    "                context_avg=contexts_per_tgw(sents,model_type,context_model,n_result,w,index2word,weight,w2entropy,w_target,word2index_target,index2word_target,context2vec_preembeds,scores,M=M)\n",
    "                \n",
    "                if type(context_avg)!=type(None):\n",
    "                    #cosine similarity\n",
    "                    probe_w_vec=w_target_out[word2index_target_out[probe_w]]\n",
    "                    cos=cosine(context_avg,probe_w_vec)\n",
    "                    model_predicts[trial][freq].append(cos)\n",
    "                    golds[trial][freq].append(gold)\n",
    "     \n",
    "    #\n",
    "    sps=defaultdict(list)\n",
    "    for trial in model_predicts:\n",
    "        for freq in model_predicts[trial]:\n",
    "            sp=spearmanr(golds[trial][freq],model_predicts[trial][freq])[0]\n",
    "            sps[freq].append(sp)\n",
    "    print ('{0}\\t{1}\\t{2}'.format('freq','SPEARMAN RANKS MEAN','SPEARMAN RANKS STD'))\n",
    "    for freq in sorted(sps.keys()):\n",
    "        print ('{0}\\t{1}\\t{2} '.format(freq,np.mean(np.array(sps[freq])),np.std(np.array(sps[freq]))))\n",
    "    return model_predicts,sps\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "def eval_card(card_f,model_param_f,context_model,model_type,n_result,w,index2word,word2index,weight=False,w2entropy=None,w_target=None,word2index_target=None,index2word_target=None,trials=100,contexts=False):\n",
    "    data=pd.read_csv(os.path.join(card_f),delimiter='\\t',header=None)\n",
    "    model_predicts=defaultdict(lambda: defaultdict(list))\n",
    "    golds=defaultdict(lambda: defaultdict(list))\n",
    "    golds_exist=[]\n",
    "    model_preds_exist=[]\n",
    "    context2vec_preembeds_all=None\n",
    "    context2vec_preembeds=None\n",
    "    scores_all=None\n",
    "    orders_inf=None\n",
    "    scores=None\n",
    "    \n",
    "    missing_w=defaultdict(list)\n",
    "    w_target_out,word2index_target_out=output_embedding(w,w_target,word2index,word2index_target)\n",
    "    contexts_f=os.path.join(os.path.dirname(card_f),'contexts')\n",
    "    \n",
    "    for index, row in data.iterrows():\n",
    "        print (index)\n",
    "        rw_1=str(row[0])\n",
    "        rw_2=str(row[1])\n",
    "        gold=float(row[2])\n",
    "        rw_embeds=[]\n",
    "        print (rw_1,rw_2,gold)\n",
    "        \n",
    "        #give unseen words 0 embedding\n",
    "        for rw in [rw_1,rw_2]:\n",
    "            if rw not in word2index_target_out:\n",
    "                if contexts==True:\n",
    "                    contexts_w_f=os.path.join(contexts_f,rw.lower().replace('/','_')+'.txt')\n",
    "                    if os.path.isfile(contexts_w_f):\n",
    "                            rw_embeds.append((rw,contexts_w_f))\n",
    "                            continue\n",
    "                rw_embeds.append(np.zeros(len(w_target_out[0])))\n",
    "                missing_w[index].append(rw)\n",
    "            else:\n",
    "                rw_embeds.append(w_target_out[word2index_target_out[rw]])\n",
    "                \n",
    "        # load contexts\n",
    "        if  not any(type(rw_embed)==tuple for rw_embed in rw_embeds):\n",
    "            model_preds_exist.append(cosine(rw_embeds[1],rw_embeds[0]))\n",
    "            golds_exist.append(gold)\n",
    "        elif any(type(rw_embed)!=tuple and rw_embed.all()==0 for rw_embed in rw_embeds):\n",
    "            model_preds_exist.append(0.)\n",
    "            golds_exist.append(gold)\n",
    "        else:\n",
    "#         elif (type(rw_embeds[0])==tuple and not rw_embeds[1].all()==0) or (type(rw_embeds[1])==tuple and not rw_embeds[0].all()==0):\n",
    "            for i,rw_embed in enumerate(rw_embeds):\n",
    "                if type(rw_embed)==tuple:\n",
    "                    #load sentences\n",
    "                    rw,contexts_w_f=rw_embed\n",
    "                    sents_all=load_sents(contexts_w_f,rw.lower())\n",
    "#                         load contexts\n",
    "                    if 'context2vec' in model_type.split('?')[0]:\n",
    "                            if model_type=='context2vec-skipgram?skipgram':\n",
    "                                context2vec_preembeds_all,scores_all=load_contexts(rw,sents_all,context_model[0],model_type.split('?')[0],n_result,w[0],index2word[0],weight[0],w2entropy[0],w_target[0],word2index_target[0],index2word_target[0])\n",
    "                            elif model_type=='context2vec-skipgram':\n",
    "                                context2vec_preembeds_all,scores_all=load_contexts(rw,sents_all,context_model,model_type.split('?')[0],n_result,w,index2word,weight[0],w2entropy,w_target,word2index_target,index2word_target)\n",
    "                            orders_inf=(-scores_all).argsort()\n",
    "                    rw_embeds[i]=(context2vec_preembeds_all,scores_all,orders_inf,sents_all)\n",
    "            #do trials\n",
    "            for trial in range(trials):\n",
    "                print ('\\n=====Trial no. {0}'.format(trial))\n",
    "                perm = np.random.permutation(255)\n",
    "                for logfreq in range(8):\n",
    "                    freq = 2**logfreq\n",
    "                    print ('\\ncontext num is {0}'.format(freq))\n",
    "                    rw_out=[]\n",
    "                    for i, rw_embed in enumerate(rw_embeds):\n",
    "                        if type(rw_embed)==tuple:\n",
    "                            context2vec_preembeds_all,scores_all,orders_inf,sents_all=rw_embed\n",
    "                            context2vec_preembeds, scores,sents=produce_contexts_per_trial(trials,freq,perm,scores_all,context2vec_preembeds_all,orders_inf,sents_all)\n",
    "                            context_avg=contexts_per_tgw(sents,model_type,context_model,n_result,w,index2word,weight,w2entropy,w_target,word2index_target,index2word_target,context2vec_preembeds,scores)\n",
    "                            rw_out.append(context_avg)\n",
    "                        else:\n",
    "                            rw_out.append(rw_embed)\n",
    "                    if all (type(rw_o)!=type(None) for rw_o in rw_out):\n",
    "                        #cosine similarity\n",
    "                        cos=cosine(rw_out[0],rw_out[1])\n",
    "                        model_predicts[trial][freq].append(cos)\n",
    "                        golds[trial][freq].append(gold)\n",
    "                        print (model_predicts[trial][freq])\n",
    "                        print (golds[trial][freq])\n",
    "\n",
    "\n",
    "\n",
    "    sps=defaultdict(list)\n",
    "    prs=defaultdict(list)\n",
    "    for trial in model_predicts:\n",
    "        for freq in model_predicts[trial]:\n",
    "            sp=spearmanr(golds_exist+golds[trial][freq],model_preds_exist+model_predicts[trial][freq])[0]\n",
    "#             sp=spearmanr(golds[trial][freq],model_predicts[trial][freq])[0]\n",
    "\n",
    "            sps[freq].append(sp)\n",
    "            pr=pearsonr(golds_exist+golds[trial][freq],model_preds_exist+model_predicts[trial][freq])[0]\n",
    "#             pr=pearsonr(golds[trial][freq],model_predicts[trial][freq])[0]\n",
    "            prs[freq].append(pr)\n",
    "    print ('{0}\\t{1}\\t{2}\\t{3}\\t{4}'.format('freq','SPEARMAN RANKS MEAN','SPEARMAN RANKS STD','PEARSON RANKS MEAN','PEARSON RANKS STD'))\n",
    "    if sps.keys()!=[]:\n",
    "        for freq in sorted(sps.keys()):\n",
    "            print ('{0}\\t{1}\\t{2}\\t{3}\\t{4} '.format(freq,np.mean(np.array(sps[freq])),np.std(np.array(sps[freq])),np.mean(np.array(prs[freq])),np.std(np.array(prs[freq]))))\n",
    "    else:\n",
    "            print ('{0}\\t{1}\\t{2}\\t{3}\\t{4} '.format(None,spearmanr(golds_exist,model_preds_exist)[0],None,pearsonr(golds_exist,model_preds_exist)[0],None))\n",
    "    missed_pairs=[missing_w[index] for index in missing_w]\n",
    "    missed_words=[w  for index in missing_w for w in missing_w[index]]\n",
    "    print ('missed pairs {0}, missed words {1}'.format(len(missed_pairs),len(missed_words)))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if __name__==\"__main__\":\n",
    "    \n",
    "    TOP_MUTUAL_SIM='top_mutual_sim'\n",
    "    TOP_CLUSTER_DENSITY='top_cluster_density'\n",
    "    LDA='lda'\n",
    "    TARGET_W='___'\n",
    "    INVERSE_S_FREQ='inverse_s_freq'\n",
    "    INVERSE_W_FREQ='inverse_w_q'\n",
    "    SUBSTITUTE_PROB='substitute_prob'\n",
    "    WEIGHT_DICT={0:False,1:TOP_MUTUAL_SIM,2:LDA,3:INVERSE_S_FREQ,4:INVERSE_W_FREQ,5:TOP_CLUSTER_DENSITY, 6:SUBSTITUTE_PROB}\n",
    "\n",
    "    ##### 1. params read in\n",
    "    if sys.argv[0]=='/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py':\n",
    "        \n",
    "        ###data:\n",
    "#         data='./eval_data/data-chimeras/dataset_alacarte.l2.fixed.test.txt.punct'\n",
    "#         data='./eval_data/data-nonces/n2v.definitional.dataset.test.txt'\n",
    "#         data='./eval_data/card-660/dataset.tsv'\n",
    "        data='./eval_data/CRW/CRW-562.txt'\n",
    "#         weights=[WEIGHT_DICT[0],WEIGHT_DICT[3]]\n",
    "        weights=[WEIGHT_DICT[0]]\n",
    "        gpu=-1\n",
    "        model_type='context2vec-skipgram'\n",
    "        w2salience_f=None\n",
    "        matrix_f=None\n",
    "        n_result=20\n",
    "        trials=100\n",
    "        context_flag=True\n",
    "#         matrix_f='../models/ALaCarte/transform/nonce_samecorpus.bin'\n",
    "        skipgram_model_f='./eval_data/CRW/vectors.txt'\n",
    "#         skipgram_model_f='../models/wiki_all.model/wiki_all.sent.split.model'\n",
    "#         skipgram_model_f='../models/glove/glove.840B.300d.w2vformat.txt'\n",
    "#         skipgram_model_f='../models/conceptnet/numberbatch-en-17.06.txt'\n",
    "#         context2vec_model_f='../models/context2vec/model_dir/MODEL-wiki.params.12'\n",
    "        context2vec_model_f='../models/context2vec/model_dir/MODEL-WWCsub-400dim.params.8'\n",
    "        ######w2salience_f\n",
    "#         w2salience_f='../corpora/corpora/wiki.all.utf8.sent.split.tokenized.vocab'\n",
    "#         w2salience_f='../corpora/corpora/WWC_norarew.txt.tokenized.vocab'\n",
    "#         w2salience_f='../models/lda/w2entropy'\n",
    "\n",
    "\n",
    "        if model_type=='skipgram' or model_type =='a la carte':\n",
    "            model_param_file=skipgram_model_f\n",
    "        elif model_type=='context2vec-skipgram':\n",
    "            model_param_file=context2vec_model_f+'?'+skipgram_model_f\n",
    "        elif model_type=='context2vec-skipgram?skipgram':\n",
    "            model_param_file=context2vec_model_f+'?'+skipgram_model_f\n",
    "\n",
    "    \n",
    "    else:\n",
    "        \n",
    "\n",
    "        parser = argparse.ArgumentParser(description='Evauate on rare words.')\n",
    "        parser.add_argument('--f',  type=str,\n",
    "                            help='model_param_file',dest='model_param_file')\n",
    "        parser.add_argument('--m', dest='model_type', type=str,\n",
    "                            help='<model_type: context2vec; context2vec-skipgram (context2vec substitutes in skipgram space); context2vec-skipgram?skipgram (context2vec substitutes in skipgram space plus skipgram context words)>')\n",
    "        parser.add_argument('--w', dest='weights', type=int, nargs='+',help='<weight:{0}>'.format (sys.argv[0],WEIGHT_DICT.items()))       \n",
    "        parser.add_argument('--d', dest='data', type=str, help='data file')\n",
    "        parser.add_argument('--g', dest='gpu',type=int, default=-1,help='gpu, default is -1')\n",
    "        parser.add_argument('--ws', dest='w2salience_f',type=str, default=None,help='word2salience file, optional')\n",
    "        parser.add_argument('--n_result',default=20,dest='n_result',type=int,help='top n result for language model substitutes')\n",
    "        parser.add_argument('--t', dest='trials',type=int, default=100, help='trial number. When trial==1, only test on the most infortive contexts')\n",
    "        parser.add_argument('--c', dest='context_flag',type=bool, default=False, help='context flag on the nonce experiment')\n",
    "        parser.add_argument('--ma', dest='matrix_f',type=str,default=None,help='matrix file for a la carte')\n",
    "        args = parser.parse_args()\n",
    "        model_param_file = args.model_param_file\n",
    "        model_type=args.model_type\n",
    "        n_result=args.n_result\n",
    "        weights=[WEIGHT_DICT[w_i] for w_i in args.weights]\n",
    "        trials=args.trials \n",
    "        data =args.data\n",
    "        gpu=args.gpu\n",
    "        w2salience_f=args.w2salience_f\n",
    "        context_flag=args.context_flag\n",
    "        matrix_f=args.matrix_f\n",
    "    \n",
    "    #### 2. gpu setup \n",
    "   \n",
    "    if gpu >= 0:\n",
    "        cuda.check_cuda_available()\n",
    "        cuda.get_device(gpu).use()    \n",
    "    xp = cuda.cupy if gpu >= 0 else np\n",
    "    \n",
    "    \n",
    "    #### 3. initialize according to model types\n",
    "    \n",
    "    def read_w2v(model_param_file):\n",
    "        if not model_param_file.endswith('txt'):\n",
    "            model_w2v = gensim.models.Word2Vec.load(model_param_file)\n",
    "        else:\n",
    "            model_w2v = KeyedVectors.load_word2vec_format(model_param_file)\n",
    "       \n",
    "        return model_w2v\n",
    "    \n",
    "    def read_context2vec(model_param_file,gpu):\n",
    "        model_reader = ModelReader(model_param_file,gpu)\n",
    "        w = xp.array(model_reader.w)\n",
    "        index2word = model_reader.index2word\n",
    "        word2index=model_reader.word2index\n",
    "        model = model_reader.model\n",
    "        return w,index2word,word2index,model\n",
    "    \n",
    "    print ('read model....')\n",
    "    if model_type=='context2vec':\n",
    "        #read in model\n",
    "        w,index2word,word2index,model=read_context2vec(model_param_file,gpu)\n",
    "        w_target=None\n",
    "        word2index_target=None\n",
    "        index2word_target=None\n",
    "        \n",
    "    elif model_type=='skipgram':\n",
    "        model_w2v=read_w2v(model_param_file)\n",
    "        w=xp.array(deepcopy(model_w2v.wv.vectors))\n",
    "        #vector normalize for target w embedding, consistent with context2vec w and convenient for cosine computation among substitutes\n",
    "        s = xp.sqrt((w * w).sum(1))\n",
    "        s[s==0.] = 1.\n",
    "        w /= s.reshape((s.shape[0], 1))\n",
    "        \n",
    "        index2word=model_w2v.wv.index2word\n",
    "        word2index={key: model_w2v.wv.vocab[key].index for key in model_w2v.wv.vocab}\n",
    "        w_target=None\n",
    "        word2index_target=None\n",
    "        index2word_target=None\n",
    "        print ('filter words for context....')\n",
    "        model=rm_stopw_context(model_w2v)\n",
    "        \n",
    "    elif model_type=='context2vec-skipgram':\n",
    "        model_param_context,model_param_w2v=model_param_file.split('?')\n",
    "        w,index2word,word2index,model=read_context2vec(model_param_context,gpu)\n",
    "        model_w2v=read_w2v(model_param_w2v)\n",
    "        w_target=xp.array(model_w2v.wv.vectors)\n",
    "        index2word_target=model_w2v.wv.index2word\n",
    "        word2index_target={key: model_w2v.wv.vocab[key].index for key in model_w2v.wv.vocab}\n",
    "    \n",
    "    elif model_type=='context2vec-skipgram?skipgram':\n",
    "        model_param_context,model_param_w2v=model_param_file.split('?')\n",
    "        #context2vec-skipgram\n",
    "        w,index2word,word2index,model=read_context2vec(model_param_context,gpu)\n",
    "        model_w2v=read_w2v(model_param_w2v)\n",
    "        w_target=xp.array(model_w2v.wv.vectors)\n",
    "        index2word_target=model_w2v.wv.index2word\n",
    "        word2index_target={key: model_w2v.wv.vocab[key].index for key in model_w2v.wv.vocab}\n",
    "    \n",
    "        # skigpram\n",
    "        model_skipgram = model_w2v\n",
    "        w_skipgram=xp.array(deepcopy(model_skipgram.wv.vectors))\n",
    "        #vector normalize for probe w embedding for calculating top vector similarity\n",
    "        s = xp.sqrt((w_skipgram * w_skipgram).sum(1))\n",
    "        s[s==0.] = 1.\n",
    "        w_skipgram /= s.reshape((s.shape[0], 1))\n",
    "        \n",
    "        index2word_skipgram=model_skipgram.wv.index2word\n",
    "        word2index_skipgram={key: model_skipgram.wv.vocab[key].index for key in model_skipgram.wv.vocab}\n",
    "        w_target_skipgram=None\n",
    "        word2index_target_skipgram=None\n",
    "        index2word_target_skipgram=None\n",
    "        \n",
    "        print ('filter words for context....')\n",
    "        model_skipgram=rm_stopw_context(model_skipgram)\n",
    "        \n",
    "    elif model_type=='a la carte':\n",
    "        model_w2v=read_w2v(model_param_file)\n",
    "        w=xp.array(deepcopy(model_w2v.wv.vectors))\n",
    "        #vector normalize for target w embedding, consistent with context2vec w and convenient for cosine computation among substitutes\n",
    "        s = xp.sqrt((w * w).sum(1))\n",
    "        s[s==0.] = 1.\n",
    "        w /= s.reshape((s.shape[0], 1))\n",
    "        \n",
    "        index2word=model_w2v.wv.index2word\n",
    "        word2index={key: model_w2v.wv.vocab[key].index for key in model_w2v.wv.vocab}\n",
    "        w_target=None\n",
    "        word2index_target=None\n",
    "        index2word_target=None\n",
    "        print ('filter words for context....')\n",
    "        model={word:model_w2v.wv.__getitem__(word) for word in model_w2v.wv.vocab}\n",
    "    \n",
    "    \n",
    "    if matrix_f!=None:\n",
    "        transform=load_transform(matrix_f,{word:model_w2v.wv.__getitem__(word) for word in model_w2v.wv.vocab})\n",
    "    else:\n",
    "        transform=None\n",
    "#     remove stop words in target word space and asarray for computing CI\n",
    "    print ('filter words for target....')\n",
    "    w,word2index,index2word=filter_w(w,word2index,index2word)\n",
    "    if  index2word_target!=None:\n",
    "        w_target,word2index_target,index2word_target=filter_w(w_target,word2index_target,index2word_target)\n",
    "    if model_type=='context2vec-skipgram?skipgram':\n",
    "        w_skipgram,word2index_skipgram,index2word_skipgram=filter_w(w_skipgram,word2index_skipgram,index2word_skipgram)\n",
    "    \n",
    "    #### 4. per word weight\n",
    "    \n",
    "    w2salience=None\n",
    "    for wt in weights:\n",
    "        if wt==LDA:\n",
    "            print ('load vectors and entropy')\n",
    "            w2salience=pickle.load(open(w2salience_f))\n",
    "        elif wt in [INVERSE_W_FREQ, INVERSE_S_FREQ]:\n",
    "            print ('load w2freq')\n",
    "            w2salience=load_w2salience(model_w2v,w2salience_f,wt)\n",
    "    \n",
    "\n",
    "\n",
    "    ##### 5. combine parameters for skipgram?context2vec-skipgram\n",
    "    if model_type=='context2vec-skipgram?skipgram':\n",
    "        model=(model,model_skipgram)\n",
    "        w=(w,w_skipgram)\n",
    "        index2word=(index2word,index2word_skipgram)\n",
    "        word2index=(word2index,word2index_skipgram)\n",
    "        w2salience=(w2salience,w2salience)\n",
    "        w_target=(w_target,w_target_skipgram)\n",
    "        word2index_target=(word2index_target,word2index_target_skipgram)\n",
    "        index2word_target=(index2word_target,index2word_target_skipgram)\n",
    "    \n",
    "    print (model_param_file,model_type,weights,data,w2salience_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    ##### 6. read in data and perform evaluation\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    print (os.path.basename(os.path.split(data)[0]))\n",
    "    trials=1\n",
    "#     data='./eval_data/data-chimeras/dataset_alacarte.l6.fixed.test.txt.punct'\n",
    "#     data='./eval_data/data-nonces/n2v.definitional.dataset.test.txt'\n",
    "\n",
    "\n",
    "    if os.path.basename(os.path.split(data)[0])== 'data-chimeras':\n",
    "\n",
    "            eval_chimera(data,model,model_type,n_result,w,index2word,word2index,weights,w2salience,w_target,word2index_target,index2word_target,M=transform)\n",
    "\n",
    "    elif os.path.basename(os.path.split(data)[0])== 'data-nonces':\n",
    "        if context_flag==True:\n",
    "            eval_nonce_withcontexts(data,model,model_w2v,model_type,n_result,w,index2word,word2index,weights,w2salience,w_target,word2index_target,index2word_target,trials,M=transform)\n",
    "        else:\n",
    "            ranks=eval_nonce(data,model,model_w2v,model_type,n_result,w,index2word,word2index,weights,w2salience,w_target,word2index_target,index2word_target,M=transform)\n",
    "\n",
    "    elif os.path.basename(os.path.split(data)[0])=='CRW':\n",
    "        \n",
    "            model_predicts,sps=eval_crw_stf(data,model_param_file,model,model_type,n_result,w,index2word,word2index,weights,w2salience,w_target,word2index_target,index2word_target,trials,M=transform)\n",
    "    \n",
    "    elif os.path.basename(os.path.split(data)[0])=='card-660':\n",
    "#             import pdb; pdb.set_trace()\n",
    "\n",
    "\n",
    "            eval_card(data,model_param_file,model,model_type,n_result,w,index2word,word2index,weights,w2salience,w_target,word2index_target,index2word_target,trials,contexts=context_flag)\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
