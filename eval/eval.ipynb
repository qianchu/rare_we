{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import six\n",
    "import sys\n",
    "import os\n",
    "import traceback\n",
    "import re\n",
    "import pickle\n",
    "from copy import deepcopy\n",
    "\n",
    "from chainer import cuda\n",
    "from context2vec.common.context_models import Toks\n",
    "from context2vec.common.model_reader import ModelReader\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import logging\n",
    "from scipy.stats import spearmanr\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import gensim\n",
    "import math\n",
    "import collections\n",
    "import argparse\n",
    "import h5py\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vectors(vectorfile, dim=300, skip=False):\n",
    "  '''loads word vectors from a text file\n",
    "  Args:\n",
    "    vectorfile: string; vector file name\n",
    "    dim: int; dimensions of the vectors\n",
    "    skip: boolean; whether or not to skip the first line (for word2vec)\n",
    "  Returns:\n",
    "    generator of (string, numpy array); word and its embedding\n",
    "  '''\n",
    "  with open(vectorfile, 'r') as f:\n",
    "    for line in f:\n",
    "      if skip:\n",
    "        skip = False\n",
    "      else:\n",
    "        index = line.index(' ')\n",
    "        word = line[:index]\n",
    "        yield word, np.array([FLOAT(entry) for entry in line[index+1:].split()[:dim]])\n",
    "\n",
    "def produce_top_n_simwords(w_filter,context_embed,n_result,index2word,debug=False):\n",
    "        #assume that w_filter is already normalized\n",
    "        context_embed = context_embed / xp.sqrt((context_embed * context_embed).sum())\n",
    "        similarity_scores=[]\n",
    "        print('producing top {0} simwords'.format(n_result))\n",
    "        similarity = (w_filter.dot(context_embed)+1.0)/2\n",
    "        top_words_i=[]\n",
    "        top_words=[]\n",
    "        count = 0\n",
    "        for i in (-similarity).argsort():\n",
    "                    if xp.isnan(similarity[i]):\n",
    "                        continue\n",
    "                    if debug==True:\n",
    "                        print('{0}: {1}'.format(str(index2word[int(i)]), str(similarity[int(i)])))\n",
    "                    count += 1\n",
    "                    top_words_i.append(int(i))\n",
    "                    top_words.append(index2word[int(i)])\n",
    "                    similarity_scores.append(float(similarity[int(i)]))\n",
    "                    if count == n_result:\n",
    "                        break\n",
    "\n",
    "        top_vec=w_filter[top_words_i,:]\n",
    "        return top_vec,xp.array(similarity_scores),top_words\n",
    "\n",
    "def top_cluster_density(top_vec,similarity_scores):\n",
    "    #normalize the top_vec\n",
    "    s = xp.sqrt((top_vec * top_vec).sum(1))\n",
    "    s[s==0.] = 1.\n",
    "    top_vec = top_vec/ s.reshape((s.shape[0], 1))\n",
    "    \n",
    "    #perform the centroid\n",
    "    max_score=similarity_scores[0]\n",
    "    similarity_scores=xp.array(similarity_scores).reshape(len(similarity_scores),1)/sum(similarity_scores)\n",
    "    centroid_vector=sum(top_vec*similarity_scores)\n",
    "    # average of cosine distance to the centroid,weighted by max scores\n",
    "    inf_score=float(sum(top_vec.dot(centroid_vector))/len(top_vec)*max_score)\n",
    "    return inf_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_w2salience(w2salience_f,weight_type):\n",
    "    w2salience={}\n",
    "    with open(w2salience_f) as f:\n",
    "        for line in f:\n",
    "            line=line.strip()\n",
    "            if line=='':\n",
    "                continue\n",
    "            if line.startswith('sentence total'):\n",
    "                sent_total=int(line.split(':')[1])\n",
    "                continue\n",
    "            w,w_count,s_count=line.split('\\t')\n",
    "            if weight_type==INVERSE_W_FREQ:\n",
    "                w2salience[w]=1/float(w_count)\n",
    "            elif weight_type==INVERSE_S_FREQ:\n",
    "                w2salience[w]=math.log(1+sent_total/float(s_count))\n",
    "#                 w2salience[w]=math.log(1+84755431/float(s_count))\n",
    "    return w2salience\n",
    "\n",
    "def skipgram_context(model,words,pos,weight=None,w2entropy=None):\n",
    "    context_wvs=[]\n",
    "    weights=[]\n",
    "    for i,word in enumerate(words):\n",
    "        if i != pos: #surroudn context words\n",
    "            try:\n",
    "                if weight ==LDA:\n",
    "                    if word in w2entropy and word in model:\n",
    "                        print (word,w2entropy[word])\n",
    "                        weights.append(1/(w2entropy[word]+1.0))\n",
    "                        context_wvs.append(model[word])\n",
    "                elif weight in [INVERSE_W_FREQ,INVERSE_S_FREQ]:\n",
    "                    if word in w2entropy and word in model:\n",
    "                        print (word,w2entropy[word])\n",
    "                        weights.append(w2entropy[word])\n",
    "                        context_wvs.append(model[word])\n",
    "                else:\n",
    "                    #equal weights per word\n",
    "                    context_wvs.append(model[word])\n",
    "                    weights.append(1.0)\n",
    "            except KeyError as e:\n",
    "                print ('==warning==: key error in context {0}'.format(e))\n",
    "    print ('per word weights',weights)\n",
    "    context_embed=sum(np.array(context_wvs)*np.array(weights).reshape(len(weights),1))#/sum(weights)\n",
    "    return sum(weights),context_embed #  will be normalized later\n",
    "\n",
    "def lg_model_out_w2v(top_words,w_target,word2index_target):\n",
    "        # lg model substitutes in skipgram embedding\n",
    "        top_vec=[]\n",
    "        index_list=[]\n",
    "        for i,word in enumerate(top_words):\n",
    "            try :\n",
    "                top_vec.append(w_target[word2index_target[word]])\n",
    "                index_list.append(i)\n",
    "            except KeyError as e:\n",
    "                print (e)\n",
    "        if top_vec==[]:\n",
    "            print ('no language model substitutes in w2v space')\n",
    "            return xp.array([]),[]\n",
    "        else:\n",
    "            return xp.stack(top_vec),index_list\n",
    "    \n",
    "def context_inform(test_s,test_w, model,model_type,n_result,w_filter,index2word,weight,w2entropy=None,w_target=None,word2index_target=None,index2word_target=None):\n",
    "    #produce context representation and infromative score for each context\n",
    "    test_s=test_s.replace(test_w, ' '+test_w+' ')\n",
    "    print(test_s)\n",
    "    words=test_s.split()\n",
    "    pos=words.index(test_w)\n",
    "    \n",
    "    score=1.0 #default score\n",
    "    \n",
    "    # Decide on the model\n",
    "    if model_type=='context2vec':\n",
    "        context_embed= model.context2vec(words, pos)\n",
    "        \n",
    "    elif model_type=='skipgram':\n",
    "        score,context_embed=skipgram_context(model,words,pos,weight,w2entropy)\n",
    "        context_embed_out=xp.array(context_embed)\n",
    "        \n",
    "    elif model_type=='context2vec-skipgram':\n",
    "        # context2vec substitutes in skipgram space\n",
    "        context_embed= model.context2vec(words, pos)\n",
    "        top_vec,sim_scores,top_words=produce_top_n_simwords(w_filter,context_embed,n_result,index2word)\n",
    "        top_vec,index_list=lg_model_out_w2v(top_words,w_target,word2index_target) \n",
    "        sim_scores=sim_scores[index_list] #weighted by substitute probability\n",
    "        if weight==SUBSTITUTE_PROB:\n",
    "            context_embed_out=xp.array(sum(top_vec*sim_scores.reshape(len(sim_scores),1)))\n",
    "        else:\n",
    "            context_embed_out=xp.array(sum(top_vec*((sim_scores/sum(sim_scores)).reshape(len(sim_scores),1))))\n",
    "        \n",
    "    else:\n",
    "        print ('model type {0} not recognized'.format(model_type))\n",
    "        sys.exit(1)\n",
    "        \n",
    "#     print('context_embed original', context_embed[:10])\n",
    "#     print ('context_embed_out',context_embed_out[:10])\n",
    "\n",
    "    #decide on weight per sentence\n",
    "    print ('weight mode',weight)\n",
    "    if weight==TOP_MUTUAL_SIM:\n",
    "#         if word2index_target==None: #not context2vec-skipgram\n",
    "#             context2vec word embedding space neighbours\n",
    "        top_vec,sim_scores,top_words=produce_top_n_simwords(w_filter,context_embed,n_result,index2word)\n",
    "        #skipgram word embedding space neighbours when context2vec-skipgram\n",
    "        score=top_mutual_sim(top_vec,sim_scores)\n",
    "    elif weight==TOP_CLUSTER_DENSITY:\n",
    "#         if word2index_target==None: #not context2vec-skipgram\n",
    "#             context2vec word embedding space neighbours\n",
    "        top_vec,sim_scores,top_words=produce_top_n_simwords(w_filter,context_embed,n_result,index2word)\n",
    "        score=top_cluster_density(top_vec,sim_scores)\n",
    "    elif weight==SUBSTITUTE_PROB:\n",
    "        score=sum(sim_scores)\n",
    "        print ('substitute prob score',score)\n",
    "    elif weight=='learned':\n",
    "        print ('learned not implemented')\n",
    "    elif weight=='gaussian':\n",
    "        print ('gaussian not implemented')\n",
    "    elif weight ==False or weight in [LDA,INVERSE_S_FREQ,INVERSE_W_FREQ]:\n",
    "        score=score\n",
    "    else:\n",
    "        print ('weight mode {0} not recognized'.format(weight))\n",
    "\n",
    "    return score,context_embed_out\n",
    "\n",
    "def additive_model(test_ss,test_w, model_type,model,n_result,w_filter,index2word,weight=False,w2entropy=None,w_target=None,word2index_target=None,index2word_target=None,f_w=None):\n",
    "    #produce context representation across contexts using weighted average\n",
    "    context_out=[]\n",
    "    context_weights=[]\n",
    "    for test_s in test_ss:\n",
    "        test_s=test_s.lower().strip()\n",
    "        #produce context representation with scores\n",
    "        score,context_embed=context_inform(test_s,test_w, model,model_type,n_result,w_filter,index2word,weight,w2entropy,w_target,word2index_target,index2word_target)\n",
    "        if score==0 or context_embed.all()==0:\n",
    "            print ('empty context vector')\n",
    "           \n",
    "        else:\n",
    "            context_out.append(context_embed)\n",
    "#             print ('context_embedtype',type(context_embed))\n",
    "            context_weights.append(score)\n",
    "    \n",
    "    \n",
    "    print ('context_weights',context_weights)\n",
    "    #sum representation across contexts\n",
    "    if context_out==[]:\n",
    "        return None\n",
    "    else:\n",
    "        context_out=xp.stack(context_out)\n",
    "    \n",
    "    \n",
    "    if model_type=='skipgram' or weight==SUBSTITUTE_PROB:\n",
    "        # context representation by weighted sum of all context words in all contexts\n",
    "        context_avg=sum(context_out)/sum(context_weights)\n",
    "    else:\n",
    "        norm_weights=xp.array(context_weights).reshape(len(context_weights),1)/float(sum(context_weights))\n",
    "        if f_w!=None:\n",
    "            f_w.write(','.join([str(i[0]) for i in norm_weights])+'\\n')\n",
    "        print ('normalized weight: \\n  {0}'.format(norm_weights))\n",
    "        # context represenatation by weighted sum of contexts\n",
    "        context_avg=sum(norm_weights*context_out)\n",
    "    \n",
    "    \n",
    "    # check new embedding neighbours\n",
    "\n",
    "    print('producing top {0} words for new embedding'.format(n_result))\n",
    "    if index2word_target==None:\n",
    "        top_vec,scores,top_words=produce_top_n_simwords(w_filter,context_avg,n_result,index2word,debug=True)\n",
    "    else:\n",
    "        #print the target space neighbours for context2vec-skipgram\n",
    "        top_vec,scores,top_words=produce_top_n_simwords(w_target,context_avg,n_result,index2word_target,debug=True)\n",
    "    \n",
    "    return context_avg\n",
    "\n",
    "\n",
    "def contexts_per_tgw(sents,model_type,context_model,n_result,w,index2word,weight,w2entropy,w_target,word2index_target,index2word_target):\n",
    "    \n",
    "    if model_type=='context2vec-skipgram?skipgram':\n",
    "#             \n",
    "            #context2vevc                \n",
    "            context_avg_1=additive_model(sents,'___', model_type.split('?')[0],context_model[0],n_result,w[0],index2word[0],weight[0],w2entropy[0],w_target[0],word2index_target[0],index2word_target[0])\n",
    "            context_avg_2=additive_model(sents,'___', model_type.split('?')[1],context_model[1],n_result,w[1],index2word[1],weight[1],w2entropy[1],w_target[1],word2index_target[1],index2word_target[1])\n",
    "                \n",
    "            if type(context_avg_1)!=type(None) and type(context_avg_2)!=type(None):\n",
    "                context_avg=(context_avg_1+context_avg_2)/2\n",
    "                print ('context2vec avg embed',context_avg_1[:10])\n",
    "                print ('skipgram avg embed', context_avg_2[:10])\n",
    "                print ('context2vec avg out', context_avg[:10])\n",
    "\n",
    "            elif type(context_avg_1)!=type(None):\n",
    "                context_avg=context_avg_1\n",
    "                print ('context2vec avg embed',context_avg_1[:10])\n",
    "\n",
    "            elif type(context_avg_2)!=type(None):\n",
    "                context_avg=context_avg_2\n",
    "                print ('skipgram avg embed', context_avg_2[:10])\n",
    "            else:\n",
    "                context_avg=None\n",
    "            \n",
    "    else:\n",
    "\n",
    "            context_avg=additive_model(sents,'___', model_type,context_model,n_result,w,index2word,weight[0],w2entropy,w_target,word2index_target,index2word_target)\n",
    "            if type(context_avg)!=type(None):\n",
    "                print ('context avg out', context_avg[:10])\n",
    "            \n",
    "    \n",
    "    return context_avg\n",
    "  \n",
    "def output_embedding(w,w_target,word2index,word2index_target):\n",
    "    if model_type=='context2vec-skipgram?skipgram':\n",
    "        #compute probe embeddings in skipgram space\n",
    "            w_out=w[1]\n",
    "            w_target_out=w_target[1]\n",
    "            word2index_out=word2index[1]\n",
    "            word2index_target_out=word2index_target[1]\n",
    "    else:\n",
    "            w_out=w\n",
    "            w_target_out=w_target\n",
    "            word2index_out=word2index\n",
    "            word2index_target_out=word2index_target\n",
    "    if word2index_target_out==None:\n",
    "        return w_out,word2index_out\n",
    "    else:\n",
    "        return w_target_out,word2index_target_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_w(w,word2index,index2word):\n",
    "    #filter out words with no letters in, and stopwords\n",
    "    stopw=stopwords.words('english')\n",
    "    stopw=[word.encode('utf-8') for word in stopw]\n",
    "    index2word_filter={}\n",
    "    word2index_filter={}\n",
    "    index_filter2index=[]\n",
    "    counter=0\n",
    "    for word in word2index:\n",
    "            if word not in stopw:\n",
    "                    index_filter2index.append(word2index[word])\n",
    "                    word2index_filter[word]=counter\n",
    "                    index2word_filter[counter]=word\n",
    "                    counter+=1\n",
    "    w_filter= w[index_filter2index,:]\n",
    "    return w_filter,word2index_filter,index2word_filter\n",
    "\n",
    "def rm_stopw_context(model):\n",
    "    stopw=stopwords.words('english')\n",
    "    stopw=[word.encode('utf-8') for word in stopw]\n",
    "    \n",
    "    model2={word:model.wv.__getitem__(word) for word in model.wv.vocab if word not in stopw}\n",
    "    return model2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_nonce(sent,contexts):\n",
    "    \n",
    "    sents_out=[]\n",
    "    \n",
    "    sent=sent.lower()\n",
    "    results=re.finditer('___ ',sent)\n",
    "    matches=[m for m in results]\n",
    "    for i in range(len(matches)):\n",
    "        sent_masked=sent\n",
    "        matches_mask=[(m2.start(0),m2.end(0)) for i2,m2 in enumerate(matches) if i2!=i]\n",
    "        matches_mask=sorted(matches_mask, key=lambda x:x[0],reverse=True)\n",
    "        for m in matches_mask:\n",
    "            sent_masked=sent_masked[:m[0]]+sent_masked[m[1]:]\n",
    "        sents_out.append(sent_masked+' .')\n",
    "    return sents_out\n",
    "\n",
    "def eval_nonce(nonce_data_f,context_model,model_w2v,model_type,n_result,w,index2word,word2index,weight=False,w2entropy=None,w_target=None,word2index_target=None,index2word_target=None,contexts=None):\n",
    "        ranks = []\n",
    "        mrr = 0.0\n",
    "        data=pd.read_csv(os.path.join(nonce_data_f),delimiter='\\t',header=None,comment='#')\n",
    "        c = 0\n",
    "        for index, row in data.iterrows():\n",
    "            if index>100 and index%100==0:\n",
    "                print (index)\n",
    "            sents=preprocess_nonce(row[1],contexts)\n",
    "            nonce=row[0]\n",
    "            if nonce not in model_w2v:\n",
    "                print ('{0} not known'.format(nonce))\n",
    "                continue\n",
    "            context_avg=contexts_per_tgw(sents,model_type,context_model,n_result,w,index2word,weight,w2entropy,w_target,word2index_target,index2word_target)\n",
    "           \n",
    "            if xp==cuda.cupy:\n",
    "                context_avg=xp.asnumpy(context_avg)\n",
    "                \n",
    "            # MRR Rank calculation\n",
    "            nns=model_w2v.similar_by_vector(context_avg,topn=len(model_w2v.wv.vocab))\n",
    "\n",
    "            rr = 0\n",
    "            n = 1\n",
    "            for nn in nns:\n",
    "                word = nn[0]\n",
    "                if word == nonce:\n",
    "                    print (word)\n",
    "                    rr = n\n",
    "                    ranks.append(rr)\n",
    "                else:\n",
    "                    n+=1\n",
    "\n",
    "            if rr != 0:\n",
    "                mrr+=float(1)/float(rr)\t\n",
    "            print rr,mrr\n",
    "            c+=1\n",
    "        print (\"Final MRR: \",mrr,c,float(mrr)/float(c))\n",
    "\n",
    "        print ('mediam : {0}'.format(np.median(ranks)))\n",
    "        return ranks\n",
    "            \n",
    "\n",
    "\n",
    "def eval_chimera(chimeras_data_f,context_model,model_type,n_result,w,index2word,word2index,weight=False,w2entropy=None,w_target=None,word2index_target=None,index2word_target=None):\n",
    "    chimeras_data_dir=os.path.dirname(chimeras_data_f)\n",
    "    num_sent=os.path.basename(chimeras_data_f).split('.')[1][1]\n",
    "    print (chimeras_data_dir)\n",
    "    print (num_sent)\n",
    "    with open(chimeras_data_dir+'/weights_{0}_{1}_{2}'.format(num_sent,model_type,str(weight)),'w') as f_w:\n",
    "        spearmans=[]\n",
    "        data=pd.read_csv(os.path.join(chimeras_data_f),delimiter='\\t',header=None)\n",
    "        w_target_out,word2index_target_out=output_embedding(w,w_target,word2index,word2index_target)\n",
    "        for index, row in data.iterrows():\n",
    "            if index>100 and index%100==0:\n",
    "                print (index)\n",
    "            golds=[]\n",
    "            model_predict=[]\n",
    "            probes=[]\n",
    "            sents=row[1].lower().split('@@')\n",
    "            #compute context representation\n",
    "            context_avg=contexts_per_tgw(sents,model_type,context_model,n_result,w,index2word,weight,w2entropy,w_target,word2index_target,index2word_target)\n",
    "            context_avg = context_avg / xp.sqrt((context_avg * context_avg).sum())\n",
    "\n",
    "            #cosine similarity with probe embedding\n",
    "            for gold,probe in zip(row[3].split(','),row[2].split(',')):\n",
    "                try:\n",
    "                    probe_w_vec=w_target_out[word2index_target_out[probe]]\n",
    "                    probe_w_vec=probe_w_vec/xp.sqrt((probe_w_vec*probe_w_vec).sum())\n",
    "                    cos=float(probe_w_vec.dot(context_avg))\n",
    "                    if np.isnan(cos):\n",
    "                        continue\n",
    "                    else:\n",
    "                        model_predict.append(cos)\n",
    "                        golds.append(gold)\n",
    "                        probes.append(probe)\n",
    "                except KeyError as e:\n",
    "                    print (\"====warning key error for probe=====: {0}\".format(e))\n",
    "            print ('probes',probes)\n",
    "            print ('gold',golds)\n",
    "            print ('model_predict',model_predict)\n",
    "            sp=spearmanr(golds,model_predict)[0]\n",
    "            print ('spearman correlation is {0}'.format(sp))\n",
    "            if not math.isnan(sp):\n",
    "                spearmans.append(sp)\n",
    "        print (\"AVERAGE RHO:\",float(sum(spearmans))/float(len(spearmans)))\n",
    "        \n",
    "\n",
    "def eval_crw_stf(crw_stf_f,model_param_f,context_model,model_type,n_result,w,index2word,word2index,weight=False,w2entropy=None,w_target=None,word2index_target=None,index2word_target=None):\n",
    "    data=pd.read_csv(os.path.join(crw_stf_f),delimiter='\\t',header=None,comment='#')\n",
    "#     contexts_fs=[]\n",
    "    model_predict=[]\n",
    "    \n",
    "    #load model outputs and contexts\n",
    "#     for model_f in model_param_f.split(\"?\"):\n",
    "#         try:\n",
    "#             #check if there are exsiting model output files\n",
    "#             contexts_fname=os.path.join(os.path.dirname(crw_stf_f),'context_'+os.path.basename(model_f)+'.vec.h5')\n",
    "#             contexts_f = h5py.File(contexts_fname, 'r')\n",
    "#             contexts_fs.append(contexts_f)\n",
    "\n",
    "#             #read in attribute dict\n",
    "#             rw2contexts=defaultdict(lambda: defaultdict(list))\n",
    "#             with open(os.path.join(os.path.dirname(crw_stf_f),'context_'+os.path.basename(model_f)+'.index')) as attribute_f:\n",
    "#                 for line in attribute_f:\n",
    "#                     for sent_len_index in line.split(':::')[1].split('\\t'):\n",
    "#                         rw2contexts[line.split(':::')[0]][sent_len_index.split(',')[0]].append(sent_len_index.split(',')[1])\n",
    "            \n",
    "#         except IOError as e:\n",
    "            \n",
    "#             contexts_fs.append(None)\n",
    "#             print ('model file {0} output not produced yet'.format(model_f))\n",
    "\n",
    "    w_target_out,word2index_target_out=output_embedding(w,w_target,word2index,word2index_target)\n",
    "    contexts_f=os.path.join(os.path.dirname(crw_stf_f),'context')\n",
    "    rw2embed={}\n",
    "    for index, row in data.iterrows():\n",
    "        if index>100 and index%100==0:\n",
    "            print (index)\n",
    "        probe_w=row[0]\n",
    "        rw=row[1]\n",
    "        gold=row[2]\n",
    "        print ('processing rareword {0}'.format(rw))\n",
    "\n",
    "        if rw in rw2embed:\n",
    "            context_avg=rw2embed[rw]\n",
    "        else:\n",
    "            sents=[]\n",
    "            with open (os.path.join(contexts_f,rw+'.txt')) as f:\n",
    "                for line in f:\n",
    "                    line=line.replace(rw,'___').strip().lower()\n",
    "                    if not line.endswith('.'):\n",
    "                        line=line+' .'\n",
    "                    sents.append(line+' .')\n",
    "            context_avg=contexts_per_tgw(sents,model_type,context_model,n_result,w,index2word,weight,w2entropy,w_target,word2index_target,index2word_target)\n",
    "            rw2embed[rw]=context_avg\n",
    "        \n",
    "        context_avg = context_avg / xp.sqrt((context_avg * context_avg).sum())\n",
    "        probe_w_vec=w_target_out[word2index_target_out[probe_w]]\n",
    "        probe_w_vec=probe_w_vec/xp.sqrt((probe_w_vec*probe_w_vec).sum())\n",
    "        cos=float(probe_w_vec.dot(context_avg))\n",
    "        model_predict.append(cos)\n",
    "        if np.isnan(cos):\n",
    "            continue\n",
    "        else:\n",
    "            model_predict.append(cos)\n",
    "            golds.append(gold)\n",
    "    \n",
    "    sp=spearmanr(golds,model_predict)[0]\n",
    "\n",
    "        #load contexts\n",
    "#         for contexts_f in contexts_fs:\n",
    "#             if contexts_f==os.path.join(os.path.dirname(crw_stf_f),'context'):\n",
    "                \n",
    "#             else:\n",
    "#                 #load from model output\n",
    "#                 sents=[]\n",
    "#                 for sent_len in rw2contexts[rw]:\n",
    "#                      sents+=list(contexts_f.get(str(sent_len))[sorted(rw2contexts[rw][sent_len]),:,:])\n",
    "#                 sents_per_model.append(sents)\n",
    "#                 print len(sents)\n",
    "    \n",
    "    \n",
    "        #context representation\n",
    "        \n",
    "#     for contexts_f in contexts_fs:\n",
    "#         if type(contexts_f)==file or type(contexts_f)==h5py._hl.files.File:\n",
    "#             print (type(contexts_f))\n",
    "#             contexts_f.close()\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if __name__==\"__main__\":\n",
    "    \n",
    "    TOP_MUTUAL_SIM='top_mutual_sim'\n",
    "    TOP_CLUSTER_DENSITY='top_cluster_density'\n",
    "    LDA='lda'\n",
    "    INVERSE_S_FREQ='inverse_s_freq'\n",
    "    INVERSE_W_FREQ='inverse_w_q'\n",
    "    SUBSTITUTE_PROB='substitute_prob'\n",
    "    WEIGHT_DICT={0:False,1:TOP_MUTUAL_SIM,2:LDA,3:INVERSE_S_FREQ,4:INVERSE_W_FREQ,5:TOP_CLUSTER_DENSITY, 6:SUBSTITUTE_PROB}\n",
    "\n",
    "    ##### 1. params read in\n",
    "    if sys.argv[0]=='/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py':\n",
    "        \n",
    "        data='./eval_data/data-chimeras/dataset_alacarte.l2.fixed.test.txt.punct'\n",
    "#         data='./eval_data/data-nonces/n2v.definitional.dataset.train.txt'\n",
    "#         data='./eval_data/CRW/CRW-562.txt'\n",
    "        weights=[WEIGHT_DICT[5],WEIGHT_DICT[3]]\n",
    "        gpu=1\n",
    "        model_type='context2vec-skipgram?skipgram'\n",
    "        w2salience_f=None\n",
    "        n_result=20\n",
    "\n",
    "        if model_type=='skipgram':\n",
    "            model_param_file='../models/wiki_all.model/wiki_all.sent.split.model'\n",
    "            model_type='skipgram'\n",
    "    #         weight='inverse_w_freq'\n",
    "            w2salience_f='../corpora/corpora/wiki.all.utf8.sent.split.tokenized.vocab'\n",
    "    #         w2salience_f='../models/lda/w2entropy'\n",
    "            \n",
    "        elif model_type=='context2vec-skipgram':\n",
    "            model_param_file='../models/context2vec/model_dir/MODEL-wiki.params.14?../models/wiki_all.model/wiki_all.sent.split.model'\n",
    "    #         model_param_file='../models/context2vec/model_dir/context2vec.ukwac.model.params?../models/wiki_all.model/wiki_all.sent.split.model'\n",
    "        elif model_type=='context2vec-skipgram?skipgram':\n",
    "            model_param_file='../models/context2vec/model_dir/MODEL-wiki.params.14?../models/wiki_all.model/wiki_all.sent.split.model'\n",
    "    #         weight='inverse_w_freq'\n",
    "            w2salience_f='../corpora/corpora/wiki.all.utf8.sent.split.tokenized.vocab'\n",
    "    #         w2salience_f='../models/lda/w2entropy'\n",
    "    \n",
    "    else:\n",
    "        \n",
    "\n",
    "        parser = argparse.ArgumentParser(description='Process some integers.')\n",
    "        parser.add_argument('--f',  type=str,\n",
    "                            help='model_param_file',dest='model_param_file')\n",
    "        parser.add_argument('--m', dest='model_type', type=str,\n",
    "                            help='<model_type: context2vec; context2vec-skipgram (context2vec substitutes in skipgram space); context2vec-skipgram?skipgram (context2vec substitutes in skipgram space plus skipgram context words)>')\n",
    "        parser.add_argument('--w', dest='weights', type=int, nargs='+',help='<weight:{0}>'.format (sys.argv[0],WEIGHT_DICT.items()))       \n",
    "        parser.add_argument('--d', dest='data', type=str, help='data file')\n",
    "        parser.add_argument('--g', dest='gpu',type=int, default=-1,help='gpu, default is -1')\n",
    "        parser.add_argument('--ws', dest='w2salience_f',type=str, default=None,help='word2salience file, optional')\n",
    "        parser.add_argument('--n_result',default=20,dest='n_result',type=int,help='top n result for language model substitutes')\n",
    "        args = parser.parse_args()\n",
    "\n",
    "        model_param_file = args.model_param_file\n",
    "        model_type=args.model_type\n",
    "        n_result=args.n_result\n",
    "        weights=[WEIGHT_DICT[w_i] for w_i in args.weights]\n",
    "        \n",
    "#         if '-' in args.weight:\n",
    "#             weight,n_result=args.weight.split('-')\n",
    "#             weight=WEIGHT_DICT[int(weight)]\n",
    "#             n_result=int(n_result)\n",
    "#         else:\n",
    "#             weight=WEIGHT_DICT[int(args.weight)]\n",
    "#             n_result=20 #default is 20 top\n",
    "            \n",
    "        data =args.data\n",
    "        gpu=args.gpu\n",
    "        w2salience_f=args.w2salience_f\n",
    "           \n",
    "    \n",
    "    #### 2. gpu setup \n",
    "   \n",
    "    if gpu >= 0:\n",
    "        cuda.check_cuda_available()\n",
    "        cuda.get_device(gpu).use()    \n",
    "    xp = cuda.cupy if gpu >= 0 else np\n",
    "    \n",
    "    \n",
    "    #### 3. initialize according to model types\n",
    "    print ('read model....')\n",
    "    if model_type=='context2vec':\n",
    "        #read in model\n",
    "        model_reader = ModelReader(model_param_file,gpu)\n",
    "        w = xp.array(model_reader.w)\n",
    "        index2word = model_reader.index2word\n",
    "        word2index=model_reader.word2index\n",
    "        model = model_reader.model\n",
    "        w_target=None\n",
    "        word2index_target=None\n",
    "        index2word_target=None\n",
    "        \n",
    "    elif model_type=='skipgram':\n",
    "        model_w2v = gensim.models.Word2Vec.load(model_param_file)\n",
    "        w=xp.array(deepcopy(model_w2v.wv.vectors))\n",
    "        #vector normalize for target w embedding, consistent with context2vec w and convenient for cosine computation among substitutes\n",
    "        s = xp.sqrt((w * w).sum(1))\n",
    "        s[s==0.] = 1.\n",
    "        w /= s.reshape((s.shape[0], 1))\n",
    "        \n",
    "        index2word=model_w2v.wv.index2word\n",
    "        word2index={key: model_w2v.wv.vocab[key].index for key in model_w2v.wv.vocab}\n",
    "        w_target=None\n",
    "        word2index_target=None\n",
    "        index2word_target=None\n",
    "        print ('filter words for context....')\n",
    "        model=rm_stopw_context(model_w2v)\n",
    "        \n",
    "    elif model_type=='context2vec-skipgram':\n",
    "        model_param_context,model_param_w2v=model_param_file.split('?')\n",
    "        model_reader = ModelReader(model_param_context,gpu)\n",
    "        w = xp.array(model_reader.w)\n",
    "        index2word = model_reader.index2word\n",
    "        word2index=model_reader.word2index\n",
    "        model = model_reader.model\n",
    "        \n",
    "        model_w2v = gensim.models.Word2Vec.load(model_param_w2v)\n",
    "        w_target=xp.array(model_w2v.wv.vectors)\n",
    "        index2word_target=model_w2v.wv.index2word\n",
    "        word2index_target={key: model_w2v.wv.vocab[key].index for key in model_w2v.wv.vocab}\n",
    "    \n",
    "    elif model_type=='context2vec-skipgram?skipgram':\n",
    "        model_param_context,model_param_w2v=model_param_file.split('?')\n",
    "        #context2vec-skipgram\n",
    "        model_reader = ModelReader(model_param_context,gpu)\n",
    "        w = xp.array(model_reader.w)\n",
    "        index2word = model_reader.index2word\n",
    "        word2index =model_reader.word2index\n",
    "        model = model_reader.model\n",
    "        \n",
    "        model_w2v = gensim.models.Word2Vec.load(model_param_w2v)\n",
    "        w_target=xp.array(model_w2v.wv.vectors)\n",
    "        index2word_target=model_w2v.wv.index2word\n",
    "        word2index_target={key: model_w2v.wv.vocab[key].index for key in model_w2v.wv.vocab}\n",
    "    \n",
    "        # skigpram\n",
    "        model_skipgram = model_w2v\n",
    "        w_skipgram=xp.array(deepcopy(model_skipgram.wv.vectors))\n",
    "        #vector normalize for probe w embedding for calculating top vector similarity\n",
    "        s = xp.sqrt((w_skipgram * w_skipgram).sum(1))\n",
    "        s[s==0.] = 1.\n",
    "        w_skipgram /= s.reshape((s.shape[0], 1))\n",
    "        \n",
    "        index2word_skipgram=model_skipgram.wv.index2word\n",
    "        word2index_skipgram={key: model_skipgram.wv.vocab[key].index for key in model_skipgram.wv.vocab}\n",
    "        w_target_skipgram=None\n",
    "        word2index_target_skipgram=None\n",
    "        index2word_target_skipgram=None\n",
    "        \n",
    "        print ('filter words for context....')\n",
    "        model_skipgram=rm_stopw_context(model_skipgram)\n",
    "        \n",
    "    \n",
    "    #remove stop words in target word space and asarray\n",
    "    print ('filter words for target....')\n",
    "    w,word2index,index2word=filter_w(w,word2index,index2word)\n",
    "    if  index2word_target!=None:\n",
    "        w_target,word2index_target,index2word_target=filter_w(w_target,word2index_target,index2word_target)\n",
    "    if model_type=='context2vec-skipgram?skipgram':\n",
    "        w_skipgram,word2index_skipgram,index2word_skipgram=filter_w(w_skipgram,word2index_skipgram,index2word_skipgram)\n",
    "    \n",
    "    #### 4. per word weight\n",
    "    \n",
    "    w2salience=None\n",
    "    for wt in weights:\n",
    "        if wt==LDA:\n",
    "            print ('load vectors and entropy')\n",
    "            w2salience=pickle.load(open(w2salience_f))\n",
    "        elif wt in [INVERSE_W_FREQ, INVERSE_S_FREQ]:\n",
    "            print ('load w2freq')\n",
    "            w2salience=load_w2salience(w2salience_f,wt)\n",
    "    \n",
    "\n",
    "\n",
    "    ##### 5. combine parameters for skipgram?context2vec-skipgram\n",
    "    if model_type=='context2vec-skipgram?skipgram':\n",
    "        model=(model,model_skipgram)\n",
    "        w=(w,w_skipgram)\n",
    "        index2word=(index2word,index2word_skipgram)\n",
    "        word2index=(word2index,word2index_skipgram)\n",
    "        w2salience=(w2salience,w2salience)\n",
    "        w_target=(w_target,w_target_skipgram)\n",
    "        word2index_target=(word2index_target,word2index_target_skipgram)\n",
    "        index2word_target=(index2word_target,index2word_target_skipgram)\n",
    "    \n",
    "    print (model_param_file,model_type,weights,data,w2salience_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    ##### 6. read in data and perform evaluation\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    print (os.path.basename(os.path.split(data)[0]))\n",
    "    if os.path.basename(os.path.split(data)[0])== 'data-chimeras':\n",
    "\n",
    "            eval_chimera(data,model,model_type,n_result,w,index2word,word2index,weights,w2salience,w_target,word2index_target,index2word_target)\n",
    "\n",
    "    elif os.path.basename(os.path.split(data)[0])== 'data-nonces':\n",
    "            ranks=eval_nonce(data,model,model_w2v,model_type,n_result,w,index2word,word2index,weights,w2salience,w_target,word2index_target,index2word_target)\n",
    "    \n",
    "    elif os.path.basename(os.path.split(data)[0])=='CRW':\n",
    "        \n",
    "            eval_crw_stf(data,model_param_file,model,model_type,n_result,w,index2word,word2index,weights,w2salience,w_target,word2index_target,index2word_target)\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
