{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ql261/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import six\n",
    "import sys\n",
    "import os\n",
    "import traceback\n",
    "import re\n",
    "import pickle\n",
    "from copy import deepcopy\n",
    "\n",
    "from chainer import cuda\n",
    "from context2vec.common.context_models import Toks\n",
    "from context2vec.common.model_reader import ModelReader\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import logging\n",
    "from scipy.stats import spearmanr\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import gensim\n",
    "import math\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_top_n_simwords(w_filter,context_embed,n_result,index2word):\n",
    "        #assume that w_filter is already normalized\n",
    "        context_embed = context_embed / xp.sqrt((context_embed * context_embed).sum())\n",
    "        similarity_scores=[]\n",
    "        print('producing top {0} simwords'.format(n_result))\n",
    "        similarity = (w_filter.dot(context_embed)+1.0)/2\n",
    "        top_words_i=[]\n",
    "        top_words=[]\n",
    "        count = 0\n",
    "        for i in (-similarity).argsort():\n",
    "                    if xp.isnan(similarity[i]):\n",
    "                        continue\n",
    "                    print('{0}: {1}'.format(str(index2word[i]), str(similarity[i])))\n",
    "                    count += 1\n",
    "                    top_words_i.append(i)\n",
    "                    top_words.append(index2word[i])\n",
    "                    similarity_scores.append(similarity[i])\n",
    "                    if count == n_result:\n",
    "                        break\n",
    "\n",
    "        top_vec=w_filter[top_words_i,:]\n",
    "        \n",
    "        return top_vec,np.array(similarity_scores),top_words\n",
    "    \n",
    "def top_mutual_sim(top_vec,similarity_scores):\n",
    "\n",
    "    #normalize the top_vec\n",
    "    s = np.sqrt((top_vec * top_vec).sum(1))\n",
    "    s[s==0.] = 1.\n",
    "    top_vec /= s.reshape((s.shape[0], 1))\n",
    "    \n",
    "    # substitutes' similarity to sentence (similarity_scores) as weight matrix to mutual similarity\n",
    "    max_score=similarity_scores[0]\n",
    "    similarity_scores=np.array(similarity_scores)\n",
    "    sim_weights=(similarity_scores+similarity_scores.reshape(len(similarity_scores),1))/2.0\n",
    "    #weighted by the maximum score in the substitutes (highre max score means the context is more certain about the substitutes)\n",
    "    sim_weights=(sim_weights/float(sum(sum(sim_weights))))*max_score\n",
    "    # dot product weighted by substitute probability (sim_weights)\n",
    "    inf_score=sum(sum(top_vec.dot(top_vec.T)*sim_weights))\n",
    "    return inf_score\n",
    "\n",
    "def top_cluster_density(top_vec,similarity_scores):\n",
    "    #normalize the top_vec\n",
    "    s = np.sqrt((top_vec * top_vec).sum(1))\n",
    "    s[s==0.] = 1.\n",
    "    top_vec = top_vec/ s.reshape((s.shape[0], 1))\n",
    "    \n",
    "    #perform the centroid\n",
    "    max_score=similarity_scores[0]\n",
    "    similarity_scores=np.array(similarity_scores).reshape(len(similarity_scores),1)/sum(similarity_scores)\n",
    "    centroid_vector=sum(top_vec*similarity_scores)\n",
    "    # average of cosine distance to the centroid,weighted by max scores\n",
    "    inf_score=sum(top_vec.dot(centroid_vector))/len(top_vec)*max_score\n",
    "    return inf_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_w2salience(w2salience_f,weight_type):\n",
    "    w2salience={}\n",
    "    with open(w2salience_f) as f:\n",
    "        for line in f:\n",
    "            if line.strip()=='':\n",
    "                continue\n",
    "            w,w_count,s_count=line.strip().split('\\t')\n",
    "            if weight_type==INVERSE_W_FREQ:\n",
    "                w2salience[w]=1/float(w_count)\n",
    "            elif weight_type==INVERSE_S_FREQ:\n",
    "                w2salience[w]=math.log(1+84755431/float(s_count))\n",
    "    return w2salience\n",
    "\n",
    "def skipgram_context(model,words,pos,weight=None,w2entropy=None):\n",
    "    context_wvs=[]\n",
    "    weights=[]\n",
    "    for i,word in enumerate(words):\n",
    "        if i != pos: #surroudn context words\n",
    "            try:\n",
    "                if weight ==LDA:\n",
    "                    if word in w2entropy and word in model:\n",
    "                        print (word,w2entropy[word])\n",
    "                        weights.append(1/(w2entropy[word]+1.0))\n",
    "                        context_wvs.append(model[word])\n",
    "                elif weight in [INVERSE_W_FREQ,INVERSE_S_FREQ]:\n",
    "                    if word in w2entropy and word in model:\n",
    "                        print (word,w2entropy[word])\n",
    "                        weights.append(w2entropy[word])\n",
    "                        context_wvs.append(model[word])\n",
    "                else:\n",
    "                    #equal weights per word\n",
    "                    context_wvs.append(model[word])\n",
    "                    weights.append(1.0)\n",
    "            except KeyError as e:\n",
    "                print ('==warning==: key error in context {0}'.format(e))\n",
    "    print ('per word weights',weights)\n",
    "    context_embed=sum(np.array(context_wvs)*np.array(weights).reshape(len(weights),1))#/sum(weights)\n",
    "    return sum(weights),context_embed #  will be normalized later\n",
    "\n",
    "def lg_model_out_w2v(top_words,w_target,word2index_target):\n",
    "        # lg model substitutes in skipgram embedding\n",
    "        top_vec=[]\n",
    "        index_list=[]\n",
    "        for i,word in enumerate(top_words):\n",
    "            try :\n",
    "                top_vec.append(w_target[word2index_target[word]])\n",
    "                print ('target word substitute',w_target[word2index_target[word]][:10])\n",
    "                index_list.append(i)\n",
    "            except KeyError as e:\n",
    "                print (e)\n",
    "        return np.array(top_vec),index_list\n",
    "    \n",
    "def context_inform(test_s,test_w, model,model_type,n_result,w_filter,index2word,weight,w2entropy=None,w_target=None,word2index_target=None,index2word_target=None):\n",
    "    #produce context representation and infromative score for each context\n",
    "    test_s=test_s.replace(test_w, ' '+test_w+' ')\n",
    "    print(test_s)\n",
    "    words=test_s.split()\n",
    "    pos=words.index(test_w)\n",
    "    \n",
    "    score=1.0 #default score\n",
    "    \n",
    "    # Decide on the model\n",
    "    if model_type=='context2vec':\n",
    "        context_embed= model.context2vec(words, pos)\n",
    "        context_embed_out=context_embed\n",
    "    \n",
    "    elif model_type=='skipgram':\n",
    "        score,context_embed=skipgram_context(model,words,pos,weight,w2entropy)\n",
    "        context_embed_out=context_embed\n",
    "        \n",
    "    elif model_type=='context2vec-skipgram':\n",
    "        # context2vec substitutes in skipgram space\n",
    "        context_embed= model.context2vec(words, pos)\n",
    "        top_vec,sim_scores,top_words=produce_top_n_simwords(w_filter,context_embed,n_result,index2word)\n",
    "        top_vec,index_list=lg_model_out_w2v(top_words,w_target,word2index_target) \n",
    "        sim_scores=sim_scores[index_list] #weighted by substitute probability\n",
    "        if weight==SUBSTITUTE_PROB:\n",
    "            context_embed_out=sum(top_vec*sim_scores.reshape(len(sim_scores),1))\n",
    "        else:\n",
    "            context_embed_out=sum(top_vec*((sim_scores/sum(sim_scores)).reshape(len(sim_scores),1)))\n",
    "    else:\n",
    "        print ('model type {0} not recognized'.format(model_type))\n",
    "        sys.exit(1)\n",
    "        \n",
    "    print('context_embed original', context_embed[:10])\n",
    "    print ('context_embed_out',context_embed_out[:10])\n",
    "    #decide on weight per sentence\n",
    "    print ('weight mode',weight)\n",
    "    if weight==TOP_MUTUAL_SIM:\n",
    "#         if word2index_target==None: #not context2vec-skipgram\n",
    "#             context2vec word embedding space neighbours\n",
    "        top_vec,sim_scores,top_words=produce_top_n_simwords(w_filter,context_embed,n_result,index2word)\n",
    "        #skipgram word embedding space neighbours when context2vec-skipgram\n",
    "        score=top_mutual_sim(top_vec,sim_scores)\n",
    "    elif weight==TOP_CLUSTER_DENSITY:\n",
    "#         if word2index_target==None: #not context2vec-skipgram\n",
    "#             context2vec word embedding space neighbours\n",
    "        top_vec,sim_scores,top_words=produce_top_n_simwords(w_filter,context_embed,n_result,index2word)\n",
    "        score=top_cluster_density(top_vec,sim_scores)\n",
    "    elif weight==SUBSTITUTE_PROB:\n",
    "        score=sum(sim_scores)\n",
    "        print ('substitute prob score',score)\n",
    "    elif weight=='learned':\n",
    "        print ('learned not implemented')\n",
    "    elif weight=='gaussian':\n",
    "        print ('gaussian not implemented')\n",
    "    elif weight ==False or weight in [LDA,INVERSE_S_FREQ,INVERSE_W_FREQ]:\n",
    "        score=score\n",
    "    else:\n",
    "        print ('weight mode {0} not recognized'.format(weight))\n",
    "    return score,context_embed_out\n",
    "\n",
    "def additive_model(f_w,test_ss,test_w, model_type,model,n_result,w_filter,index2word,weight=False,w2entropy=None,w_target=None,word2index_target=None,index2word_target=None):\n",
    "    #produce context representation across contexts using weighted average\n",
    "    context_out=[]\n",
    "    context_weights=[]\n",
    "    for test_s in test_ss.split('@@'):\n",
    "        test_s=test_s.strip()\n",
    "        #produce context representation with scores\n",
    "        score,context_embed=context_inform(test_s,test_w, model,model_type,n_result,w_filter,index2word,weight,w2entropy,w_target,word2index_target,index2word_target)\n",
    "        print ('weight is {0}'.format(score))\n",
    "        print ('context_embed is ', context_embed[:10])\n",
    "        context_out.append(context_embed)\n",
    "        context_weights.append(score)\n",
    "    \n",
    "    \n",
    "    print ('context_weights',context_weights)\n",
    "    #sum representation across contexts\n",
    "    context_out=np.array(context_out)\n",
    "    \n",
    "    \n",
    "    if model_type=='skipgram' or weight==SUBSTITUTE_PROB:\n",
    "        # context representation by weighted sum of all context words in all contexts\n",
    "        context_avg=sum(context_out)/sum(context_weights)\n",
    "    else:\n",
    "        norm_weights=np.array(context_weights).reshape(len(context_weights),1)/float(sum(context_weights))\n",
    "        f_w.write(','.join([str(i[0]) for i in norm_weights])+'\\n')\n",
    "        print ('normalized weight: \\n  {0}'.format(norm_weights))\n",
    "        # context represenatation by weighted sum of contexts\n",
    "        context_avg=sum(norm_weights*context_out)\n",
    "    \n",
    "    \n",
    "    # check new embedding neighbours\n",
    "\n",
    "    print('producing top {0} words for new embedding'.format(n_result))\n",
    "    if index2word_target==None:\n",
    "        top_vec,scores,top_words=produce_top_n_simwords(w_filter,context_avg,n_result,index2word)\n",
    "    else:\n",
    "        #print the target space neighbours for context2vec-skipgram\n",
    "        print (w_target.shape)\n",
    "        top_vec,scores,top_words=produce_top_n_simwords(w_target,context_avg,n_result,index2word_target)\n",
    "    \n",
    "    return context_avg\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_w(w,word2index,index2word):\n",
    "    #filter out words with no letters in, and stopwords\n",
    "    stopw=stopwords.words('english')\n",
    "    stopw=[word.encode('utf-8') for word in stopw]\n",
    "    index2word_filter={}\n",
    "    word2index_filter={}\n",
    "    index_filter2index=[]\n",
    "    counter=0\n",
    "    for word in word2index:\n",
    "            if word not in stopw:\n",
    "                    index_filter2index.append(word2index[word])\n",
    "                    word2index_filter[word]=counter\n",
    "                    index2word_filter[counter]=word\n",
    "                    counter+=1\n",
    "    w_filter= w[index_filter2index,:]\n",
    "    return w_filter,word2index_filter,index2word_filter\n",
    "\n",
    "def rm_stopw_context(model):\n",
    "    stopw=stopwords.words('english')\n",
    "    stopw=[word.encode('utf-8') for word in stopw]\n",
    "    \n",
    "    model={word:model.wv.__getitem__(word) for word in model.wv.vocab if word not in stopw}\n",
    "    return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_chimera(chimeras_data_f,context_model,model_type,n_result,w,index2word,word2index,weight=False,w2entropy=None,w_target=None,word2index_target=None,index2word_target=None):\n",
    "    chimeras_data_dir='/'.join(chimeras_data_f.split('/')[:-1])\n",
    "    num_sent=chimeras_data_f.split('/')[-1].split('.')[1][1]\n",
    "    print (chimeras_data_dir)\n",
    "    print (num_sent)\n",
    "    with open(chimeras_data_dir+'/weights_{0}_{1}_{2}'.format(num_sent,model_type,str(weight)),'w') as f_w:\n",
    "        spearmans=[]\n",
    "        data=pd.read_csv(os.path.join(chimeras_data_f),delimiter='\\t',header=None)\n",
    "\n",
    "        for index, row in data.iterrows():\n",
    "            golds=[]\n",
    "            model_predict=[]\n",
    "            probes=[]\n",
    "            #compute context representation\n",
    "            if model_type=='context2vec-skipgram?skipgram':\n",
    "                    #context2vevc\n",
    "                    \n",
    "                    context_avg_1=additive_model(f_w,row[1].lower(),'___', model_type.split('?')[0],context_model[0],n_result,w[0],index2word[0],weight[0],w2entropy[0],w_target[0],word2index_target[0],index2word_target[0])\n",
    "                    print ('context2vec avg embed',context_avg_1[:10])\n",
    "                    context_avg_2=additive_model(f_w,row[1].lower(),'___', model_type.split('?')[1],context_model[1],n_result,w[1],index2word[1],weight[1],w2entropy[1],w_target[1],word2index_target[1],index2word_target[1])\n",
    "                    print ('skipgram avg embed', context_avg_2[:10])\n",
    "                    context_avg=(context_avg_1+context_avg_2)/2\n",
    "                    print ('context2vec avg skipgram', context_avg[:10])\n",
    "                    #compute probe embeddings in skipgram space\n",
    "                    w_out=w[1]\n",
    "                    w_target_out=w_target[1]\n",
    "                    word2index_out=word2index[1]\n",
    "                    word2index_target_out=word2index_target[1]\n",
    "                    \n",
    "            else:\n",
    "                    \n",
    "                    context_avg=additive_model(f_w,row[1].lower(),'___', model_type,context_model,n_result,w,index2word,weight,w2entropy,w_target,word2index_target,index2word_target)\n",
    "                    print ('context avg out', context_avg[:10])\n",
    "                    w_out=w\n",
    "                    w_target_out=w_target\n",
    "                    word2index_out=word2index\n",
    "                    word2index_target_out=word2index_target\n",
    "            \n",
    "            context_avg = context_avg / xp.sqrt((context_avg * context_avg).sum())\n",
    "\n",
    "           \n",
    "            \n",
    "            #cosine similarity with probe embedding\n",
    "            for gold,probe in zip(row[3].split(','),row[2].split(',')):\n",
    "                try:\n",
    "                    if word2index_target_out==None:\n",
    "                        probe_w_vec=xp.array(w_out[word2index_out[probe]])\n",
    "                    else:\n",
    "                        probe_w_vec=xp.array(w_target_out[word2index_target_out[probe]])\n",
    "                    probe_w_vec=probe_w_vec/xp.sqrt((probe_w_vec*probe_w_vec).sum())\n",
    "                    cos=probe_w_vec.dot(context_avg)\n",
    "                    if xp.isnan(cos):\n",
    "                        continue\n",
    "                    else:\n",
    "                        model_predict.append(cos)\n",
    "                        golds.append(gold)\n",
    "                        probes.append(probe)\n",
    "                except KeyError as e:\n",
    "                    print (\"====warning key error for probe=====: {0}\".format(e))\n",
    "            print ('probes',probes)\n",
    "            print ('gold',golds)\n",
    "            print ('model_predict',model_predict)\n",
    "            sp=spearmanr(golds,model_predict)[0]\n",
    "            print ('spearman correlation is {0}'.format(sp))\n",
    "            if not math.isnan(sp):\n",
    "                spearmans.append(sp)\n",
    "        print (\"AVERAGE RHO:\",float(sum(spearmans))/float(len(spearmans)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read model....\n",
      "Reading config file: ../models/context2vec/model_dir/MODEL-wiki.params.14\n",
      "Config:  {'config_path': '../models/context2vec/model_dir/', 'model_file': 'MODEL-wiki.14', 'deep': 'yes', 'drop_ratio': '0.0', 'words_file': 'WORDS-wiki.targets.14', 'unit': '400'}\n",
      "filter words for context....\n",
      "filter words for target....\n",
      "('../models/context2vec/model_dir/MODEL-wiki.params.14?../models/wiki_all.model/wiki_all.sent.split.model', 'context2vec-skipgram?skipgram', ('top_cluster_density', False), './eval_data/data-chimeras/dataset.l2.fixed.test.txt.punct', None)\n"
     ]
    }
   ],
   "source": [
    "TOP_MUTUAL_SIM='top_mutual_sim'\n",
    "TOP_CLUSTER_DENSITY='top_cluster_density'\n",
    "LDA='lda'\n",
    "INVERSE_S_FREQ='inverse_s_freq'\n",
    "INVERSE_W_FREQ='inverse_w_q'\n",
    "SUBSTITUTE_PROB='substitute_prob'\n",
    "WEIGHT_DICT={0:False,1:TOP_MUTUAL_SIM,2:LDA,3:INVERSE_S_FREQ,4:INVERSE_W_FREQ,5:TOP_CLUSTER_DENSITY, 6:SUBSTITUTE_PROB}\n",
    "\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    \n",
    "    #params read in\n",
    "    if sys.argv[0]=='/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py':\n",
    "        \n",
    "        data='./eval_data/data-chimeras/dataset.l2.fixed.test.txt.punct'\n",
    "\n",
    "        weight=WEIGHT_DICT[5]\n",
    "        \n",
    "#         ##context2vec\n",
    "##         model_param_file='../models/context2vec/model_dir/context2vec.ukwac.model.params'\n",
    "#         model_param_file='../models/context2vec/model_dir/MODEL-wiki.params.14'\n",
    "        \n",
    "#         model_type='context2vec'\n",
    "\n",
    "####skipgram\n",
    "#         model_param_file='../models/wiki_all.model/wiki_all.sent.split.model'\n",
    "#         model_type='skipgram'\n",
    "#         weight='inverse_w_freq'\n",
    "#         w2salience_f='../corpora/corpora/wiki.all.utf8.sent.split.tokenized.vocab'\n",
    "#         w2salience_f='../models/lda/w2entropy'\n",
    "#         n_result=20\n",
    "\n",
    "\n",
    "####context2vec-skipgram\n",
    "        model_param_file='../models/context2vec/model_dir/MODEL-wiki.params.14?../models/wiki_all.model/wiki_all.sent.split.model'\n",
    "#         model_param_file='../models/context2vec/model_dir/context2vec.ukwac.model.params?../models/wiki_all.model/wiki_all.sent.split.model'\n",
    "        model_type='context2vec-skipgram?skipgram'\n",
    "        n_result=20\n",
    "        w2salience_f=None\n",
    "\n",
    "# #####skipgram?context2vec-skipgram\n",
    "#         model_param_file='../models/context2vec/model_dir/MODEL-wiki.params.14?../models/wiki_all.model/wiki_all.sent.split.model'\n",
    "#         model_type='context2vec-skipgram?skipgram'\n",
    "# #         weight='inverse_w_freq'\n",
    "# #         w2salience_f='../corpora/corpora/wiki.all.utf8.sent.split.tokenized.vocab'\n",
    "# #         w2salience_f='../models/lda/w2entropy'\n",
    "#         n_result=20\n",
    "    \n",
    "    else:\n",
    "        if len(sys.argv) < 5:\n",
    "            print >> sys.stderr, \"Usage: {0} <model_param_file> <model_type: context2vec; context2vec-skipgram (context2vec substitutes in skipgram space); context2vec-skipgram?skipgram (context2vec substitutes in skipgram space plus skipgram context words)> <weight:{1}> <eval_data> <w2salience>\"  .format (sys.argv[0],WEIGHT_DICT.items())\n",
    "            sys.exit(1)\n",
    "        \n",
    "        model_param_file = sys.argv[1]\n",
    "        model_type=sys.argv[2]\n",
    "        \n",
    "        if '-' in sys.argv[3]:\n",
    "            weight,n_result=sys.argv[3].split('-')\n",
    "            weight=WEIGHT_DICT[int(weight)]\n",
    "            n_result=int(n_result)\n",
    "        else:\n",
    "            weight=WEIGHT_DICT[int(sys.argv[3])]\n",
    "            n_result=20 #default is 20 top\n",
    "            \n",
    "#         context_rm_stopw=int(sys.argv[4])\n",
    "        data =sys.argv[4]\n",
    "        \n",
    "        if len(sys.argv)>5:\n",
    "            w2salience_f=argv[5]\n",
    "        else:\n",
    "            w2salience_f=None\n",
    "    \n",
    "    #gpu setup \n",
    "    gpu = -1 # todo: make this work with gpu\n",
    "\n",
    "    if gpu >= 0:\n",
    "        cuda.check_cuda_available()\n",
    "        cuda.get_device(gpu).use()    \n",
    "    xp = cuda.cupy if gpu >= 0 else np\n",
    "    \n",
    "    # logging\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "\n",
    "    \n",
    "    #choose model type\n",
    "    print ('read model....')\n",
    "    if model_type=='context2vec':\n",
    "        #read in model\n",
    "        \n",
    "        model_reader = ModelReader(model_param_file)\n",
    "        w = model_reader.w\n",
    "        index2word = model_reader.index2word\n",
    "        word2index=model_reader.word2index\n",
    "        model = model_reader.model\n",
    "        w_target=None\n",
    "        word2index_target=None\n",
    "        index2word_target=None\n",
    "        \n",
    "    elif model_type=='skipgram':\n",
    "        model = gensim.models.Word2Vec.load(model_param_file)\n",
    "        w=deepcopy(model.wv.vectors)\n",
    "        #vector normalize for target w embedding, consistent with context2vec w and convenient for cosine computation among substitutes\n",
    "        s = np.sqrt((w * w).sum(1))\n",
    "        s[s==0.] = 1.\n",
    "        w /= s.reshape((s.shape[0], 1))\n",
    "        \n",
    "        index2word=model.wv.index2word\n",
    "        word2index={key: model.wv.vocab[key].index for key in model.wv.vocab}\n",
    "        w_target=None\n",
    "        word2index_target=None\n",
    "        index2word_target=None\n",
    "        \n",
    "        print ('filter words for context....')\n",
    "\n",
    "        model=rm_stopw_context(model)\n",
    "        \n",
    "    elif model_type=='context2vec-skipgram':\n",
    "        model_param_context,model_param_w2v=model_param_file.split('?')\n",
    "        model_reader = ModelReader(model_param_context)\n",
    "        w = model_reader.w\n",
    "        index2word = model_reader.index2word\n",
    "        word2index=model_reader.word2index\n",
    "        model = model_reader.model\n",
    "        \n",
    "        model_w2v = gensim.models.Word2Vec.load(model_param_w2v)\n",
    "        w_target=model_w2v.wv.vectors\n",
    "        index2word_target=model_w2v.wv.index2word\n",
    "        word2index_target={key: model_w2v.wv.vocab[key].index for key in model_w2v.wv.vocab}\n",
    "    \n",
    "    elif model_type=='context2vec-skipgram?skipgram':\n",
    "        model_param_context,model_param_w2v=model_param_file.split('?')\n",
    "        #context2vec-skipgram\n",
    "        model_reader = ModelReader(model_param_context)\n",
    "        w = model_reader.w\n",
    "        index2word = model_reader.index2word\n",
    "        word2index =model_reader.word2index\n",
    "        model = model_reader.model\n",
    "        \n",
    "        model_w2v = gensim.models.Word2Vec.load(model_param_w2v)\n",
    "        w_target=model_w2v.wv.vectors\n",
    "        index2word_target=model_w2v.wv.index2word\n",
    "        word2index_target={key: model_w2v.wv.vocab[key].index for key in model_w2v.wv.vocab}\n",
    "    \n",
    "        # skigpram\n",
    "        model_skipgram = model_w2v\n",
    "        w_skipgram=deepcopy(model_skipgram.wv.vectors)\n",
    "        #vector normalize for probe w embedding\n",
    "        s = np.sqrt((w_skipgram * w_skipgram).sum(1))\n",
    "        s[s==0.] = 1.\n",
    "        w_skipgram /= s.reshape((s.shape[0], 1))\n",
    "        \n",
    "        index2word_skipgram=model_skipgram.wv.index2word\n",
    "        word2index_skipgram={key: model_skipgram.wv.vocab[key].index for key in model_skipgram.wv.vocab}\n",
    "        w_target_skipgram=None\n",
    "        word2index_target_skipgram=None\n",
    "        index2word_target_skipgram=None\n",
    "        \n",
    "        print ('filter words for context....')\n",
    "\n",
    "        model_skipgram=rm_stopw_context(model_skipgram)\n",
    "        \n",
    "                \n",
    "        \n",
    "    \n",
    "    #remove stop words in target word space\n",
    "    print ('filter words for target....')\n",
    "    w,word2index,index2word=filter_w(w,word2index,index2word)\n",
    "    if  index2word_target!=None:\n",
    "        w_target,word2index_target,index2word_target=filter_w(w_target,word2index_target,index2word_target)\n",
    "    if model_type=='context2vec-skipgram?skipgram':\n",
    "        w_skipgram,word2index_skipgram,index2word_skipgram=filter_w(w_skipgram,word2index_skipgram,index2word_skipgram)\n",
    "    \n",
    "    #per word weight\n",
    "    \n",
    "    w2salience=None\n",
    "    if weight==LDA:\n",
    "        print ('load vectors and entropy')\n",
    "        w2salience=pickle.load(open(w2salience_f))\n",
    "    elif weight==INVERSE_W_FREQ:\n",
    "        print ('load w2freq')\n",
    "        w2salience=load_w2salience(w2salience_f,weight)\n",
    "    elif weight==INVERSE_S_FREQ:\n",
    "        print ('load w2freq')\n",
    "        w2salience=load_w2salience(w2salience_f,weight)\n",
    "\n",
    "\n",
    "    #combine parameters for skipgram?context2vec-skipgram\n",
    "    if model_type=='context2vec-skipgram?skipgram':\n",
    "        model=(model,model_skipgram)\n",
    "        w=(w,w_skipgram)\n",
    "        index2word=(index2word,index2word_skipgram)\n",
    "        word2index=(word2index,word2index_skipgram)\n",
    "        weight=(weight,WEIGHT_DICT[0])#assume that skipgram has no weight\n",
    "        w2salience=(w2salience,w2salience)\n",
    "        w_target=(w_target,w_target_skipgram)\n",
    "        word2index_target=(word2index_target,word2index_target_skipgram)\n",
    "        index2word_target=(index2word_target,index2word_target_skipgram)\n",
    "    \n",
    "    print (model_param_file,model_type,weight,data,w2salience_f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./eval_data/data-chimeras\n",
      "2\n",
      "('top_cluster_density', False)\n",
      "canned sardines and  ___  between two slices of wholemeal bread and thinly spread flora original .\n",
      "producing top 2 simwords\n",
      "meatloaf: 0.5400038\n",
      "sandwiches: 0.5396633\n",
      "('target word substitute', array([-0.03469653, -0.525551  ,  0.28010306, -0.1321948 ,  0.10325604,\n",
      "        0.13125885,  0.05481365, -0.14237191,  0.14524433,  0.30375952],\n",
      "      dtype=float32))\n",
      "('target word substitute', array([ 0.02203639,  0.29302943,  0.08690894, -0.3772151 ,  0.2763922 ,\n",
      "       -0.22779332,  0.07307623, -0.43730363,  0.20441696,  0.3818804 ],\n",
      "      dtype=float32))\n",
      "('context_embed original', array([-2.0292683 , -0.12115667, -0.58547366, -0.2508922 , -1.0164319 ,\n",
      "       -0.46230623, -0.41658485,  0.76794076,  0.27725974,  0.72865707],\n",
      "      dtype=float32))\n",
      "('context_embed_out', array([-0.00633901, -0.11638986,  0.18353644, -0.2546663 ,  0.1897968 ,\n",
      "       -0.04821063,  0.06394206, -0.28979126,  0.17482132,  0.34280765],\n",
      "      dtype=float32))\n",
      "('weight mode', 'top_cluster_density')\n",
      "producing top 2 simwords\n",
      "meatloaf: 0.5400038\n",
      "sandwiches: 0.5396633\n",
      "weight is 0.372764696921\n",
      "('context_embed is ', array([-0.00633901, -0.11638986,  0.18353644, -0.2546663 ,  0.1897968 ,\n",
      "       -0.04821063,  0.06394206, -0.28979126,  0.17482132,  0.34280765],\n",
      "      dtype=float32))\n",
      "erm ,  ___  , low fat dairy products , incidents of heart disease for those who have an olive oil rich diet .\n",
      "producing top 2 simwords\n",
      "dairies: 0.54627275\n",
      "pharmaceuticals: 0.53947484\n",
      "('target word substitute', array([ 0.07633998,  0.13791087,  0.1387819 , -0.5291779 ,  0.51710486,\n",
      "       -0.01692714, -0.00197337, -0.14920644, -0.18324421, -0.21469943],\n",
      "      dtype=float32))\n",
      "('target word substitute', array([-0.2124599 ,  0.07261022,  0.63129336, -0.42908496, -0.06258088,\n",
      "       -0.09496085, -0.21298398, -0.20257291,  0.68739295, -0.17937297],\n",
      "      dtype=float32))\n",
      "('context_embed original', array([ 0.051423  ,  0.5232299 , -0.6043477 ,  0.7778011 ,  0.01178985,\n",
      "        0.4086424 , -0.45285642,  0.46220404, -1.2985977 ,  0.09733371],\n",
      "      dtype=float32))\n",
      "('context_embed_out', array([-0.06715587,  0.10546497,  0.3834958 , -0.47944474,  0.22907671,\n",
      "       -0.05569971, -0.1068181 , -0.17572261,  0.24934882, -0.19714679],\n",
      "      dtype=float32))\n",
      "('weight mode', 'top_cluster_density')\n",
      "producing top 2 simwords\n",
      "dairies: 0.54627275\n",
      "pharmaceuticals: 0.53947484\n",
      "weight is 0.382891244763\n",
      "('context_embed is ', array([-0.06715587,  0.10546497,  0.3834958 , -0.47944474,  0.22907671,\n",
      "       -0.05569971, -0.1068181 , -0.17572261,  0.24934882, -0.19714679],\n",
      "      dtype=float32))\n",
      "('context_weights', [0.3727646969210241, 0.38289124476276015])\n",
      "normalized weight: \n",
      "  [[0.4932995]\n",
      " [0.5067005]]\n",
      "producing top 2 words for new embedding\n",
      "(259235, 400)\n",
      "producing top 2 simwords\n",
      "salads: 2.358779330223971\n",
      "sandwiches: 2.303981410464689\n",
      "('context2vec avg embed', array([-0.03715494, -0.00397591,  0.28485595, -0.36856165,  0.20969995,\n",
      "       -0.05200535, -0.0225822 , -0.23199262,  0.21258444,  0.06921247]))\n",
      "canned sardines and  ___  between two slices of wholemeal bread and thinly spread flora original .\n",
      "==warning==: key error in context 'and'\n",
      "==warning==: key error in context 'between'\n",
      "==warning==: key error in context 'of'\n",
      "==warning==: key error in context 'and'\n",
      "==warning==: key error in context '.'\n",
      "('per word weights', [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0])\n",
      "('context_embed original', array([-0.25425556, -0.25280828,  0.73290821, -1.27273212,  1.43104529,\n",
      "       -1.29429817,  0.80987991,  0.81756981, -0.2416677 ,  2.68517341]))\n",
      "('context_embed_out', array([-0.25425556, -0.25280828,  0.73290821, -1.27273212,  1.43104529,\n",
      "       -1.29429817,  0.80987991,  0.81756981, -0.2416677 ,  2.68517341]))\n",
      "('weight mode', False)\n",
      "weight is 10.0\n",
      "('context_embed is ', array([-0.25425556, -0.25280828,  0.73290821, -1.27273212,  1.43104529,\n",
      "       -1.29429817,  0.80987991,  0.81756981, -0.2416677 ,  2.68517341]))\n",
      "erm ,  ___  , low fat dairy products , incidents of heart disease for those who have an olive oil rich diet .\n",
      "==warning==: key error in context ','\n",
      "==warning==: key error in context ','\n",
      "==warning==: key error in context ','\n",
      "==warning==: key error in context 'of'\n",
      "==warning==: key error in context 'for'\n",
      "==warning==: key error in context 'those'\n",
      "==warning==: key error in context 'who'\n",
      "==warning==: key error in context 'have'\n",
      "==warning==: key error in context 'an'\n",
      "==warning==: key error in context '.'\n",
      "('per word weights', [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0])\n",
      "('context_embed original', array([-0.84230892,  0.7778287 ,  1.46854832, -3.10901148, -0.24001103,\n",
      "        1.83680855, -0.76301876, -0.39193594,  2.68265816,  0.80789384]))\n",
      "('context_embed_out', array([-0.84230892,  0.7778287 ,  1.46854832, -3.10901148, -0.24001103,\n",
      "        1.83680855, -0.76301876, -0.39193594,  2.68265816,  0.80789384]))\n",
      "('weight mode', False)\n",
      "weight is 12.0\n",
      "('context_embed is ', array([-0.84230892,  0.7778287 ,  1.46854832, -3.10901148, -0.24001103,\n",
      "        1.83680855, -0.76301876, -0.39193594,  2.68265816,  0.80789384]))\n",
      "('context_weights', [10.0, 12.0])\n",
      "producing top 2 words for new embedding\n",
      "producing top 2 simwords\n",
      "wholemeal: 0.8537659242290676\n",
      "pureed: 0.8476144073059836\n",
      "('skipgram avg embed', array([-0.04984384,  0.02386456,  0.10006621, -0.19917016,  0.05413792,\n",
      "        0.02465956,  0.00213005,  0.01934699,  0.11095411,  0.15877578]))\n",
      "('context2vec avg skipgram', array([-0.04349939,  0.00994433,  0.19246108, -0.28386591,  0.13191894,\n",
      "       -0.01367289, -0.01022607, -0.10632281,  0.16176928,  0.11399412]))\n",
      "('probes', ['rhubarb', 'onion', 'pear', 'strawberry', 'limousine', 'cushion'])\n",
      "('gold', ['3', '2.86', '1.43', '2.14', '1.29', '1.71'])\n",
      "('model_predict', [0.4286175169140154, 0.4621758321289938, 0.3950618418415166, 0.40604369660836714, 0.22453733507153864, 0.2044109998330913])\n",
      "spearman correlation is 0.771428571429\n",
      "('AVERAGE RHO:', 0.7714285714285715)\n"
     ]
    }
   ],
   "source": [
    "#read in data\n",
    "# data='./eval_data/data-chimeras/dataset.l2.fixed.test.txt.punct'\n",
    "if data.split('/')[-2]== 'data-chimeras':\n",
    "#         n_result=2\n",
    "#         print (weight)\n",
    "        eval_chimera(data,model,model_type,n_result,w,index2word,word2index,weight,w2salience,w_target,word2index_target,index2word_target)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
