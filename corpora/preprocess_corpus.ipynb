{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ql261/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "anarchism does not offer a fixed body of doctrine from a single particular world view , instead fluxing and flowing as a philosophy .\n",
      "many types and traditions of anarchism exist , not all of which are mutually exclusive .\n",
      "anarchist schools of thought can differ fundamentally , supporting anything from extreme individualism to complete collectivism .\n",
      "strains of anarchism have often been divided into the categories of social and individualist anarchism or similar dual classifications .\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import codecs\n",
    "from collections import Counter,defaultdict\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk import sent_tokenize\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ql261/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: '/home/ql261/.local/share/jupyter/runtime/kernel-b5b8a4d2-ff78-432c-8e85-a3ff0983564e.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e7801622a2cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mword_context_matrix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     \u001b[0mcontext_max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m     \u001b[0mtarget_max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0mword2id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: '/home/ql261/.local/share/jupyter/runtime/kernel-b5b8a4d2-ff78-432c-8e85-a3ff0983564e.json'"
     ]
    }
   ],
   "source": [
    "def tokenize_vocab(corpus_out):\n",
    "    sent_total=0\n",
    "    word_counts=defaultdict(lambda: [0, 0])\n",
    "    with codecs.open (corpus_out, 'w',encoding='utf-8') as f_out:\n",
    "        #first pass\n",
    "        counter=0\n",
    "        for line in f:\n",
    "            \n",
    "            line=line.strip().lower()\n",
    "            for sent in sent_tokenize(line):\n",
    "                if sent=='':\n",
    "                        continue\n",
    "                ws=nltk.word_tokenize(sent)\n",
    "                sent=' '.join(ws)\n",
    "                bgw_per_sent={}\n",
    "                for w in ws:\n",
    "                    word_counts[w][0]+=1\n",
    "                    if w not in bgw_per_sent:\n",
    "                        bgw_per_sent[w]=1\n",
    "                        word_counts[w][1]+=1\n",
    "\n",
    "                f_out.write(sent+'\\n')\n",
    "                sent_total+=1\n",
    "                if counter%10000==0 and counter>=10000:\n",
    "                    print ('{0} '.format(counter)),\n",
    "                counter+=1\n",
    "    return word_counts,sent_total\n",
    "       \n",
    "def write_to_vocab(vocab_fn,word_count,sent_total):\n",
    "    with codecs.open(vocab_fn,encoding='utf-8',mode='w') as vocab_f:\n",
    "            vocab_f.write('sentence total:{0}\\n'.format(str(sent_total)))\n",
    "            w_id=0\n",
    "            word_counts_most_common_w=sorted(word_count.items(), key=lambda x:x[1][0],reverse=True)\n",
    "            for w, counts in word_counts_most_common_w:\n",
    "                w_count,s_count=counts\n",
    "                vocab_f.write(w+'\\t'+str(w_count)+'\\t'+str(s_count)+'\\n')\n",
    "                word2id[w]=w_id\n",
    "                id2word[w_id]=w\n",
    "                w_id+=1\n",
    "            \n",
    "           \n",
    "    return word2id,id2word\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    word_context_matrix=defaultdict(lambda: Counter())\n",
    "    context_max=int(sys.argv[2])\n",
    "    target_max=int(sys.argv[3])\n",
    "    word2id={}\n",
    "    id2word={}\n",
    "    \n",
    "    \n",
    "    corpus_dir=sys.argv[1]\n",
    "#     corpus_dir='./corpora/wiki.all.utf8.sent.split.mini'\n",
    "    corpus_out=corpus_dir+'.tokenized'\n",
    "    vocab_fn=corpus_dir+'.tokenized.vocab'\n",
    "    w_c_fn=corpus_dir+'.tokenized.context'\n",
    "    with codecs.open (corpus_dir,encoding='utf-8') as f:\n",
    "        print ('===first pass====')\n",
    "        word_count,sent_total=tokenize_vocab(corpus_out)\n",
    "        word2id,id2word=write_to_vocab(vocab_fn,word_count,sent_total)\n",
    "#         word_count=''\n",
    "        \n",
    "                \n",
    "        #second pass\n",
    "    if context_max!=0 and target_max!=0:\n",
    "        with codecs.open (corpus_dir,encoding='utf-8') as f:\n",
    "            print ('\\n===second pass=====')\n",
    "            counter=0\n",
    "            for line in f:\n",
    "\n",
    "                line=line.strip().lower()\n",
    "                if line=='':\n",
    "                        continue\n",
    "\n",
    "                if counter%10000==0 and counter>=10000:\n",
    "                    print ('{0} '.format(counter)),\n",
    "                ws=line.split()\n",
    "                counter+=1\n",
    "                for w in ws:\n",
    "                    if word2id[w] <target_max:\n",
    "                        for c_w in ws:\n",
    "                            if word2id[c_w]<context_max and c_w !=w:\n",
    "                                word_context_matrix[w][c_w]+=1\n",
    "\n",
    "\n",
    "        with codecs.open(w_c_fn,encoding='utf-8',mode='w') as w_c_f:\n",
    "            for i in range(min(target_max,len(id2word))):\n",
    "                w=id2word[i]\n",
    "                w_c_pairs= [str(word2id[c_w])+':'+str(word_context_matrix[w][c_w]) for c_w in word_context_matrix[w] if word2id[c_w]<context_max]\n",
    "                w_c_f.write(str(len(w_c_pairs))+' '+' '.join(w_c_pairs)+'\\n')\n",
    "\n",
    "\n",
    "            \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
