{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===first pass====\n",
      "10000  20000  30000  40000  50000  60000  70000  80000  \n",
      "===second pass=====\n",
      "10000  20000  30000  40000  50000  60000  70000  80000 \n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import nltk\n",
    "# from nltk.tokenize import sent_tokenize\n",
    "import codecs\n",
    "from collections import Counter,defaultdict\n",
    "\n",
    "def tokenize_vocab(corpus_out):\n",
    "    word_counts=Counter()\n",
    "    with codecs.open (corpus_out, 'w',encoding='utf-8') as f_out:\n",
    "        #first pass\n",
    "        counter=0\n",
    "        for line in f:\n",
    "\n",
    "            line=line.strip().lower()\n",
    "\n",
    "            ws=line.split()\n",
    "            for w in ws:\n",
    "                word_counts[w]+=1\n",
    "            if line=='':\n",
    "                    continue\n",
    "            f_out.write(line+'\\n')\n",
    "                \n",
    "            if counter%10000==0 and counter>=10000:\n",
    "                print ('{0} '.format(counter)),\n",
    "            counter+=1\n",
    "    return word_counts\n",
    "       \n",
    "def write_to_vocab(vocab_fn,word_count):\n",
    "    with codecs.open(vocab_fn,encoding='utf-8',mode='w') as vocab_f:\n",
    "            w_id=0\n",
    "            for w, count in word_count.most_common()[:20000]:\n",
    "                vocab_f.write(w+'\\t'+str(count)+'\\n')\n",
    "                word2id[w]=w_id\n",
    "                id2word[w_id]=w\n",
    "                w_id+=1\n",
    "           \n",
    "    return word2id,id2word\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "#     nltk.download('punkt')\n",
    "    word_context_matrix=defaultdict(lambda: Counter())\n",
    "    word2id={}\n",
    "    id2word={}\n",
    "    \n",
    "    \n",
    "    corpus_dir=sys.argv[1]\n",
    "#     corpus_dir='./corpora/wiki.all.utf8.sent.split.mini'\n",
    "    corpus_out=corpus_dir+'.tokenized'\n",
    "    vocab_fn=corpus_dir+'.tokenized.vocab'\n",
    "    w_c_fn=corpus_dir+'.tokenized.context'\n",
    "    with codecs.open (corpus_dir,encoding='utf-8') as f:\n",
    "        print ('===first pass====')\n",
    "        word_count=tokenize_vocab(corpus_out)\n",
    "        word2id,id2word=write_to_vocab(vocab_fn,word_count)\n",
    "        \n",
    "#                     for c_w in ws:\n",
    "#                         if c_w !=w:\n",
    "#                             word_context_matrix[w][c_w]+=1\n",
    "                \n",
    "                                \n",
    "        #filter target and contexg words\n",
    "#         target_w_freq=word_counts.most_common()[:20000]\n",
    "#         target_w=zip(*target_w_freq)[0]\n",
    "#         context_w=zip(*target_w_freq[:5000])[0]\n",
    "                \n",
    "        #second pass\n",
    "    with codecs.open (corpus_dir,encoding='utf-8') as f:\n",
    "        print ('\\n===second pass=====')\n",
    "        counter=0\n",
    "        for line in f:\n",
    "            \n",
    "            line=line.strip().lower()\n",
    "            if line=='':\n",
    "                    continue\n",
    "                    \n",
    "            if counter%10000==0 and counter>=10000:\n",
    "                print ('{0} '.format(counter)),\n",
    "            ws=line.split()\n",
    "            counter+=1\n",
    "            for w in ws:\n",
    "                if word2id[w] <20000:\n",
    "                    for c_w in ws:\n",
    "                        if word2id[c_w]<5000 and c_w !=w:\n",
    "                            word_context_matrix[w][c_w]+=1\n",
    "                \n",
    "    \n",
    "    \n",
    "                \n",
    "    with codecs.open(w_c_fn,encoding='utf-8',mode='w') as w_c_f:\n",
    "        for i in range(20000):\n",
    "            w=id2word[i]\n",
    "            w_c_pairs= [str(word2id[c_w])+':'+str(word_context_matrix[w][c_w]) for c_w in word_context_matrix[w] if word2id[c_w]<5000]\n",
    "            w_c_f.write(str(len(w_c_pairs))+' '+' '.join(w_c_pairs)+'\\n')\n",
    "\n",
    "            \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 'b'), (1, 1)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from collections import Counter\n",
    "# a=Counter()\n",
    "# a['a']+=1\n",
    "# a['b']+=1\n",
    "# zip(*a.most_common())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
