{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install chainer\n",
    "!pip install fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading config file: ./context2vec/model/MODEL-1B-300dim.params.10\n",
      "Config:  {'config_path': './context2vec/model/', 'model_file': 'MODEL-1B-300dim.10', 'words_file': 'WORDS-1B-300dim.targets.10', 'unit': '300', 'deep': 'yes', 'drop_ratio': '0.0'}\n"
     ]
    }
   ],
   "source": [
    "from chainer import cuda\n",
    "from context2vec.common.context_models import Toks\n",
    "from context2vec.common.model_reader import ModelReader\n",
    "import re\n",
    "\n",
    "usim=open('./usim_en.txt').readlines()\n",
    "model_reader = ModelReader('./context2vec/model/MODEL-1B-300dim.params.10')\n",
    "W = model_reader.w\n",
    "word2index = model_reader.word2index\n",
    "index2word = model_reader.index2word\n",
    "model = model_reader.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy \n",
    "\n",
    "target_exp = re.compile('\\[.*\\]')\n",
    "n_result = 10 \n",
    "gpu = -1 # todo: make this work with gpu\n",
    "\n",
    "if gpu >= 0:\n",
    "    cuda.check_cuda_available()\n",
    "    cuda.get_device(gpu).use()    \n",
    "xp = cuda.cupy if gpu >= 0 else numpy\n",
    "\n",
    "def mult_sim(w, target_v, context_v):\n",
    "    target_similarity = w.dot(target_v)\n",
    "    target_similarity[target_similarity<0] = 0.0\n",
    "    context_similarity = w.dot(context_v)\n",
    "    context_similarity[context_similarity<0] = 0.0\n",
    "    return (target_similarity * context_similarity)\n",
    "\n",
    "def parse_input(line):\n",
    "    sent = line.strip().split()\n",
    "    target_pos = None\n",
    "    for i, word in enumerate(sent):\n",
    "        if target_exp.match(word) != None:\n",
    "            target_pos = i\n",
    "            if word == '[]':\n",
    "                word = None\n",
    "            else:\n",
    "                word = word[1:-1]\n",
    "            sent[i] = word\n",
    "    return sent, target_pos\n",
    "\n",
    "def produce_candidates(line,index2word,w):\n",
    "    sent, target_pos = parse_input(line)\n",
    "    if target_pos == None:\n",
    "        raise ParseException(\"Can't find the target position.\") \n",
    "\n",
    "    if sent[target_pos] == None:\n",
    "        target_v = None\n",
    "    elif sent[target_pos] not in word2index:\n",
    "        raise ParseException(\"Target word is out of vocabulary.\")\n",
    "    else:\n",
    "        target_v = w[word2index[sent[target_pos]]]\n",
    "    if len(sent) > 1:\n",
    "        context_v = model.context2vec(sent, target_pos) \n",
    "        context_v = context_v / xp.sqrt((context_v * context_v).sum())\n",
    "    else:\n",
    "        context_v = None\n",
    "\n",
    "    if target_v is not None and context_v is not None:\n",
    "        similarity = mult_sim(w, target_v, context_v)\n",
    "    else:\n",
    "        if target_v is not None:\n",
    "            v = target_v\n",
    "        elif context_v is not None:\n",
    "            v = context_v                \n",
    "        else:\n",
    "            raise ParseException(\"Can't find a target nor context.\")   \n",
    "        similarity = (w.dot(v)+1.0)/2 # Cosine similarity can be negative, mapping similarity to [0,1]\n",
    "\n",
    "    count = 0\n",
    "    results=[]\n",
    "    for i in (-similarity).argsort():\n",
    "        if numpy.isnan(similarity[i]):\n",
    "                continue\n",
    "#         print('{0}: {1}'.format(index2word[i], similarity[i]))\n",
    "        results.append((index2word[i],similarity[i]))\n",
    "        count += 1\n",
    "        if count == n_result:\n",
    "            break\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "import fasttext\n",
    "ft_model = fasttext.load_model(\"wiki.en.300.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=numpy.array([[1,2],[4,5],[6,7]])\n",
    "d=numpy.array([3,5,8]).reshape(3,1)\n",
    "a*d\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d648067d1bf4f48adbeed2a94218d75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=2266.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from collections import defaultdict\n",
    "import math\n",
    "\n",
    "left_embeds_firstorder=[]\n",
    "left_embeds_secondorder=[]\n",
    "right_embeds_firstorder=[]\n",
    "right_embeds_secondorder=[]\n",
    "w2docfreqs=defaultdict(int)\n",
    "doc_len=len(usim)\n",
    "for line in usim:\n",
    "    for  word in set(line.strip().split()):\n",
    "        w2docfreqs[word]+=1\n",
    "w2idf={word:math.log(doc_len/w2docfreqs[word]) for word in w2docfreqs}\n",
    "        \n",
    "print (w)\n",
    "for linei in tqdm(range(0,len(usim))):\n",
    "    \n",
    "#     print (linei)\n",
    "    \n",
    "    origsentence,wi,_,_=usim[linei].split('\\t')\n",
    "    sentence=origsentence.split()\n",
    "    # first order\n",
    "    contexts=[ft_model[w] for w in sentence if w not in stop_words]\n",
    "    idfs=numpy.array([w2idf[w] for w in sentence if w not in stop_words])\n",
    "    idfs=idfs.reshape(len(idfs),1)\n",
    "#     print (idfs)\n",
    "#     break\n",
    "    first_order_embeds=numpy.vstack(contexts)*idfs\n",
    "    first_order_embeds=first_order_embeds.mean(0)\n",
    "    # second  order\n",
    "    sentence[int(wi)]='[]'\n",
    "    sentence=' '.join(sentence)\n",
    "#     print (sentence)\n",
    "    candidates=produce_candidates(sentence,index2word,W)\n",
    "    words,simscores=list(zip(*candidates))\n",
    "    simscores=numpy.array([simscore/sum(simscores) for simscore in simscores]).reshape(len(simscores),1)\n",
    "    ft_embeds=numpy.vstack([ft_model[w] for w in words])\n",
    "    second_order_embeds=ft_embeds*simscores\n",
    "    second_order_embeds=second_order_embeds.mean(0)\n",
    "    if linei%2==0:\n",
    "        left_embeds_secondorder.append(second_order_embeds)\n",
    "        left_embeds_firstorder.append(first_order_embeds)\n",
    "    else:\n",
    "        right_embeds_secondorder.append(second_order_embeds)\n",
    "        right_embeds_firstorder.append(first_order_embeds)\n",
    "\n",
    "  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(right_embeds_firstorder)==len(left_embeds_firstorder)==len(right_embeds_secondorder)==len(left_embeds_secondorder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "firstorder_scores=cosine_similarity(right_embeds_firstorder,left_embeds_firstorder)\n",
    "firstorder_scores=[firstorder_scores[i][j] for i in range(len(firstorder_scores)) for j in range(len(firstorder_scores)) if i==j]\n",
    "secondorder_scores=cosine_similarity(right_embeds_secondorder,left_embeds_secondorder)\n",
    "secondorder_scores=[secondorder_scores[i][j] for i in range(len(secondorder_scores)) for j in range(len(secondorder_scores)) if i==j]\n",
    "scores=[float(usim[linei].strip().split('\\t')[-2]) for linei in range(0,len(usim),2)]\n",
    "\n",
    "assert len(firstorder_scores)==len(secondorder_scores)==len(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpearmanrResult(correlation=0.16028445440323638, pvalue=5.817021845429074e-08)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import spearmanr\n",
    "spearmanr(scores,firstorder_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "bertmodel=BertModel.from_pretrained('bert-base-uncased')\n",
    "berttokenizer=BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "bertmodel=bertmodel.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def delete_tokenmark_input(input_ids,tokenizer):\n",
    "    input_id_new=[]\n",
    "    del_num=0\n",
    "    token_pos_start_id=[tokenizer.encode('[',add_special_tokens=False)[0],tokenizer.encode(' [',add_special_tokens=False)[0]]\n",
    "    token_pos_end_id=[tokenizer.encode(']',add_special_tokens=False)[0],tokenizer.encode(' ]',add_special_tokens=False)[0]]\n",
    "    token_pos_start_end_id=set(token_pos_start_id+token_pos_end_id)\n",
    "    for i,input_i in enumerate(input_ids):\n",
    "        if input_i not in token_pos_start_end_id:\n",
    "            input_id_new.append(input_i)\n",
    "        else:\n",
    "            del_num+=1\n",
    "    input_id_new+=del_num*[tokenizer.pad_token_id]\n",
    "    return input_id_new\n",
    "\n",
    "def delete_tokenmarker_am(input_ids,tokenizer):\n",
    "    am_new=[]\n",
    "    for i in input_ids:\n",
    "        if i==tokenizer.pad_token_id:\n",
    "            am_new.append(0)\n",
    "        else:\n",
    "            am_new.append(1)\n",
    "    return am_new\n",
    "\n",
    "def find_token_id(input_id,tokenizer):\n",
    "    token_pos_start_id=set([tokenizer.encode('[',add_special_tokens=False)[0],tokenizer.encode(' [',add_special_tokens=False)[0]])    \n",
    "    token_pos_end_id=set([tokenizer.encode(']',add_special_tokens=False)[0],tokenizer.encode(' ]',add_special_tokens=False)[0]])    \n",
    "    \n",
    "    token_ids=[]\n",
    "    for i,input_i in enumerate(input_id):\n",
    "        input_i=int(input_i)\n",
    "        if i==len(input_id)-1: # the last token\n",
    "            continue\n",
    "        if input_i in [tokenizer.mask_token_id,tokenizer.cls_token_id,tokenizer.pad_token_id]:\n",
    "            continue\n",
    "        if input_i in token_pos_start_id:\n",
    "            token_ids.append(i+1)\n",
    "            # logger.info(\"first word\",token_ids)\n",
    "        elif input_i in token_pos_end_id:\n",
    "            token_ids.append(i)\n",
    "    try:\n",
    "        assert len(token_ids)==2\n",
    "    except AssertionError as e:\n",
    "        print ('Warning, token id alter is not length 2')\n",
    "        print (input_id)\n",
    "        print (tokenizer.convert_ids_to_tokens(input_id))\n",
    "        print (token_pos_start_id)\n",
    "        print (token_pos_end_id)\n",
    "        print (token_ids)\n",
    "        sys.exit(1)\n",
    "   \n",
    "    try:\n",
    "        assert token_ids[1]!=token_ids[0]\n",
    "    except AssertionError as e:\n",
    "        print ('token marker star == end')\n",
    "        print (input_id)\n",
    "        print (token_ids)\n",
    "        sys.exit(1)\n",
    "    token_ids[1]=token_ids[1]-1\n",
    "    token_ids[0]=token_ids[0]-1\n",
    "    return token_ids\n",
    "    \n",
    "def delete_tokenmaker_tokentypeids(input_ids,tokenizer):\n",
    "    tokentype_ids=[]\n",
    "    item=0\n",
    "    for i in input_ids:\n",
    "    \n",
    "        if i==tokenizer.pad_token_id:\n",
    "            tokentype_ids.append(0)\n",
    "        \n",
    "        elif i==tokenizer.sep_token_id:\n",
    "            tokentype_ids.append(item)\n",
    "            item=1\n",
    "        else:\n",
    "            tokentype_ids.append(item)  \n",
    "    return tokentype_ids\n",
    "\n",
    "def get_embed(sentences,tokenizer,model,flag='cls',layer_start=None,layer_end=None,maxlen=64):\n",
    "    if flag=='cls':\n",
    "        sentences=[sentence.replace('[','').replace(']','') for sentence in sentences]\n",
    "        toks = tokenizer.batch_encode_plus(sentences, max_length = maxlen,truncation = True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            outputs_ = model(input_ids=toks['input_ids'].cuda(),attention_mask=toks['attention_mask'].cuda(), output_hidden_states=True)\n",
    "        last_hidden_state = outputs_.last_hidden_state\n",
    "        output = last_hidden_state.detach().cpu().numpy()[:,0]\n",
    "    elif flag=='cls_with_token':\n",
    "        toks = tokenizer.batch_encode_plus(sentences, max_length = maxlen,truncation = True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            outputs_ = model(input_ids=toks['input_ids'].cuda(),attention_mask=toks['attention_mask'].cuda(), output_hidden_states=True)\n",
    "        last_hidden_state = outputs_.last_hidden_state\n",
    "        output = last_hidden_state.detach().cpu().numpy()[:,0]\n",
    "    elif flag=='mean':\n",
    "        sentences=[sentence.replace('[','').replace(']','') for sentence in sentences]\n",
    "        toks = tokenizer.batch_encode_plus(sentences, max_length = maxlen,truncation = True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            outputs_ = model(input_ids=toks['input_ids'].cuda(),attention_mask=toks['attention_mask'].cuda(), output_hidden_states=True)\n",
    "        hidden_states = outputs_.hidden_states\n",
    "        average_layer_batch = sum(hidden_states[layer_start:layer_end]) / (layer_end-layer_start)\n",
    "        \n",
    "        output = average_layer_batch.detach().cpu().numpy().mean(1)\n",
    "\n",
    "    elif flag=='preappend':\n",
    "        sentences=[sentence.split()[sentence.split().index('[')+1]+' $ '+ sentence for sentence in sentences]\n",
    "        # print (sentences)\n",
    "        toks = tokenizer.batch_encode_plus(sentences, max_length = maxlen,truncation = True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            outputs_ = model(input_ids=toks['input_ids'].cuda(),attention_mask=toks['attention_mask'].cuda(), output_hidden_states=True)\n",
    "        last_hidden_state = outputs_.last_hidden_state\n",
    "        output = last_hidden_state.detach().cpu().numpy()[:,0,:]\n",
    "    elif flag=='alltoken':\n",
    "        toks = tokenizer.batch_encode_plus(sentences, max_length = maxlen,truncation = True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "\n",
    "       \n",
    "        # for num in range(average_layer_batch.size()[0]):\n",
    "        #     embeds_per_sent=average_layer_batch[num]\n",
    "        #     token_ids_per_sent=all_token_ids[num]\n",
    "            \n",
    "        #     embed_token=torch.mean(embeds_per_sent[int(token_ids_per_sent[0]):int(token_ids_per_sent[1])],dim=0,keepdim=True)\n",
    "        #     # assert int(token_ids_per_sent[0])!=int(token_ids_per_sent[1])\n",
    "        #     assert not torch.isnan(embed_token).any()\n",
    "        #     if num == 0:\n",
    "        #         output = embed_token\n",
    "        #     else:\n",
    "        #         output = torch.cat((output, embed_token),0)\n",
    "        output = output.detach().cpu().numpy()\n",
    "    elif flag.startswith('token'):\n",
    "        toks = tokenizer.batch_encode_plus(sentences, max_length = maxlen,truncation = True, padding=\"max_length\")\n",
    "        all_token_ids=torch.tensor([find_token_id(tok,tokenizer) for tok in toks['input_ids']], dtype=torch.long).cuda()\n",
    "        all_input_ids=torch.tensor([delete_tokenmark_input(tok,tokenizer) for tok in toks['input_ids']], dtype=torch.long).cuda()\n",
    "        all_attention_mask=torch.tensor([delete_tokenmarker_am(input_ids,tokenizer) for input_ids in all_input_ids], dtype=torch.long).cuda()\n",
    "        all_token_type_ids=torch.tensor([delete_tokenmaker_tokentypeids(input_ids,tokenizer) for input_ids in all_input_ids], dtype=torch.long).cuda()\n",
    "        inputs = {\"input_ids\": all_input_ids, \"attention_mask\": all_attention_mask}\n",
    "        with torch.no_grad():\n",
    "            outputs_ = model(**inputs, output_hidden_states=True)\n",
    "        hidden_states = outputs_.hidden_states\n",
    "        average_layer_batch = sum(hidden_states[layer_start:layer_end]) / (layer_end-layer_start)\n",
    "        \n",
    "        for num in range(average_layer_batch.size()[0]):\n",
    "            embeds_per_sent=average_layer_batch[num]\n",
    "            token_ids_per_sent=all_token_ids[num]\n",
    "            \n",
    "            embed_token=torch.mean(embeds_per_sent[int(token_ids_per_sent[0]):int(token_ids_per_sent[1])],dim=0,keepdim=True)\n",
    "            # assert int(token_ids_per_sent[0])!=int(token_ids_per_sent[1])\n",
    "            assert not torch.isnan(embed_token).any()\n",
    "            if num == 0:\n",
    "                output = embed_token\n",
    "            else:\n",
    "                output = torch.cat((output, embed_token),0)\n",
    "        output = output.detach().cpu().numpy()\n",
    "        if flag=='token+cls':\n",
    "            last_hidden_state = outputs_.last_hidden_state\n",
    "            output=np.concatenate([output, last_hidden_state.detach().cpu().numpy()[:,0]],axis=1)\n",
    "            # print (output.shape)\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_input_left=[]\n",
    "bert_input_right=[]\n",
    "\n",
    "for linei,line in enumerate(usim):\n",
    "    sentence,wi,_,_=line.strip().split('\\t')\n",
    "    wi=int(wi)\n",
    "    sentence=sentence.split()\n",
    "    prev_sentence=' '.join(sentence[:int(wi)]).replace('[','').replace(']','').split()\n",
    "    after_sentence=' '.join(sentence[int(wi)+1:]).replace('[','').replace(']','').split()\n",
    "    sentence=prev_sentence+['[',sentence[wi],']']+after_sentence\n",
    "    if linei%2==0:\n",
    "        bert_input_left.append(' '.join(sentence))\n",
    "    else:\n",
    "        bert_input_right.append(' '.join(sentence))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(bert_input_left)==len(bert_input_right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_input_left[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_embeds=get_embed(bert_input_left,berttokenizer,bertmodel,'token',9,13)\n",
    "right_embeds=get_embed(bert_input_right,berttokenizer,bertmodel,'token',9,13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "NORMALIZE='normalize'\n",
    "CENTER='center'\n",
    "def normalize_embeddings(emb, types, mean=None):\n",
    "    \"\"\"\n",
    "    Normalize embeddings by their norms / recenter them.\n",
    "    \"\"\"\n",
    "    for t in types.split(','):\n",
    "        if t == '':\n",
    "            continue\n",
    "        if t == CENTER:\n",
    "            if mean is None:\n",
    "                mean = emb.mean(0, keepdim=True)\n",
    "            emb.sub_(mean.expand_as(emb))\n",
    "        elif t == NORMALIZE:\n",
    "            matrix_norm(emb)\n",
    "        else:\n",
    "            raise Exception('Unknown normalization type: \"%s\"' % t)\n",
    "    return mean if mean is not None else None\n",
    "\n",
    "def produce_cosine_list(test_src,test_tgt):\n",
    "    cos_matrix=produce_cos_matrix(test_src, test_tgt)\n",
    "    scores_pred = [float(cos_matrix[i][i]) for i in range(len(cos_matrix))]\n",
    "    return scores_pred\n",
    "\n",
    "def produce_cos_matrix(test_src,test_tgt):\n",
    "    normalize_embeddings(test_src, NORMALIZE, None)\n",
    "    normalize_embeddings(test_tgt, NORMALIZE, None)\n",
    "    cos_matrix = torch.mm(test_src, test_tgt.transpose(0, 1))\n",
    "    return cos_matrix\n",
    "\n",
    "def matrix_norm(emb):\n",
    "    emb.div_(emb.norm(2, 1, keepdim=True).expand_as(emb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_bert=produce_cosine_list(torch.from_numpy(left_embeds),torch.from_numpy(right_embeds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpearmanrResult(correlation=0.08867805493761785, pvalue=0.002812678748437777)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spearmanr(firstorder_scores,secondorder_scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
